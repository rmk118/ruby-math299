[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 299",
    "section": "",
    "text": "Ruby Krasnow\nThis website will house my assignments as I work through the course material for QERM 514: Analysis of Ecological and Environmental Data, one of the core requirements for graduate students in the Quantitative Ecology & Resource Management (QERM) program at the University of Washington. The QERM 514 course materials developed and made publicly available online by Dr. Mark Scheuerell will serve as a foundation for a directed study I will complete during my final semester at Clark, supervised by Michael Satz (Clark University, Department of Mathematics)."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Math 299",
    "section": "",
    "text": "My name is Ruby Krasnow (she/her) and I am an aspiring quantitative marine ecologist interested in using modeling and data science to support sustainable fisheries management and marine aquaculture."
  },
  {
    "objectID": "HW/hw_01_intro_to_markdown.html",
    "href": "HW/hw_01_intro_to_markdown.html",
    "title": "HW1: Introduction",
    "section": "",
    "text": "Link to original assignment PDF\n\n1) Who am I & what do I study?\nI am an aspiring marine ecologist with a strong interest in sustainable fisheries and aquaculture. I am especially passionate about using mathematical and statistical models to understand aquatic ecosystems, support sustainable fisheries management, and advance the kelp and shellfish aquaculture industries. I am currently a senior at Clark University, where I am majoring in Biology with a minor in Mathematics. After graduating with my B.A. in December 2024, I will start my PhD in Marine Biology at the University of Maine. I am one of the team captains for the Clark cross-country team and am excited to continue my athletic as well as academic career by competing on the UMaine track and cross-country teams as a graduate student.\nA major focus of my research is enhancing the accuracy of kelp (Saccharina latissima) growth models by incorporating factors such as blade erosion, biofouling, genetic variation, and phenotypic plasticity. Additionally, I am collaborating with the NOAA Northeast Fisheries Science Center to model spatial variation in crustacean size-at-maturity, supporting the sustainable management of the emerging Jonah crab fishery in New England.\nTo learn more about me, you can visit my personal website: https://rmk118.github.io/\n\nKeywords\n\nquantitative marine ecology\nfisheries & aquaculture\n\nkelp aquaculture\nmodeling crustacean size-at-maturity\npopulation dynamics & stock assessment\n\nsynthesis research/meta-analyses\nopen & reproducible data science\n\n\n\n\n2) What do I want from this course?\n\nDevelop a more solid theoretical understanding of the concepts underlying the modeling techniques I frequently use in my research\nLearn new techniques for model selection and inference (e.g., practice cross-validation)\nBecome more familiar with distributions/models that can address common issues in ecological analyses, such as zero-inflated and hurdle models\n\n\n\n3) Plot air quality data\n\nggplot(air)+\n  geom_point(aes(x=wind, y=temp))+\n  theme_light()+ #define custom theme for ggplots\n  theme(\n    axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)),\n    axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0)),\n    text=element_text(size=13))+labs(x=\"Wind speed (mph)\", y=\"Temperature (°F)\")+\n  xlim(0,NA)\n\n\n\n\nFigure 1: Avg. wind speed (mph) and max. daily temperature (°F) at LaGuardia Airport, NY, from May-Sept. 1973\n\n\n\n\n\n\n4) Say it with an equation\n\\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon  \\tag{1}\\] \\[ \\varepsilon \\sim N(0, \\sigma^2)\\]\nIn Equation 1, the two different predictor variables are represented by \\(X_1\\) and \\(X_2\\).\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#math-299-directed-study-environmental-data-analysis",
    "href": "index.html#math-299-directed-study-environmental-data-analysis",
    "title": "Math 299",
    "section": "",
    "text": "Ruby Krasnow\nThis website will house my assignments as I work through the course material for QERM 514: Analysis of Ecological and Environmental Data, one of the core requirements for graduate students in the Quantitative Ecology & Resource Management (QERM) program at the University of Washington. The QERM 514 course materials developed and made publicly available online by Dr. Mark Scheuerell will serve as a foundation for a directed study I will complete during my final semester at Clark, supervised by Michael Satz (Clark University, Department of Mathematics)."
  },
  {
    "objectID": "HW/hw_02_fitting_lm.html",
    "href": "HW/hw_02_fitting_lm.html",
    "title": "HW2: Linear Models",
    "section": "",
    "text": "Link to original assignment PDF\n\nBackground\nThe goal of this assignment is to familiarize yourself with fitting linear models in R. We will be working some data from nearby Lake Washington that is part of a long-term monitoring program begun in the 1960s by the late, and rather famous, Dr. W.T. Edmondson and since continued by Dr. Daniel Schindler. The accompanying data file L_Washington_plankton.csv contains information on the following four variables:\n\nDaphnia: index of the density of the cladoceran Daphnia (unitless)\nGreens: index of the density of green algae (unitless)\nCyclops: index of the density of the copepod Cyclops (unitless)\nTemp: water temperature (°C)\n\nDaphnia are an effective grazer on phytoplankton and green algae make up a large proportion of their diet. Cyclops are an inferior grazer compared to Daphnia, but a competitor nonetheless. Daphnia growth rates are also affected by water temperature.\n\n\nQuestion 1\n\nWrite out the equation for a linear regression model that expresses Daphnia abundance as a function of its preferred prey, green algae, and describe the terms in your model.\n\n\\[y_i =\\beta_0 + \\beta_1 x_i+e_i \\] Each observation of Daphnia abundance \\((y_i)\\) is a function of an intercept \\((\\beta_0)\\) and the effect \\((\\beta_1)\\) of green algae density \\((x_i)\\). The model residuals are assumed to be independent and normally distributed with mean 0 and variance \\(\\sigma^2\\), such that \\(e_i \\sim \\mathrm{N}(0, \\sigma^2)\\).\n\nProduce a scatterplot that shows the relationship between Daphnia and Greens. Make sure to label your plot accordingly and give it an informative caption. Describe the relationship between Daphnia and Greens. Does a linear model seem reasonable here?\n\n\n\n\n\n\nFigure 1: Relationship between the density of Daphnia and its preferred prey, green algae\n\n\n\n\nThere seems to be a weak positive relationship between green algae and Daphnia density that could be approximated with a linear model.\n\nProduce the step-by-step R code required to fit your model via linear algebra to generate estimates the model parameters and the data. Be sure to show the construction of the design matrix \\((\\mathbf{X})\\), the calculation of the parameter estimates \\((\\hat{\\beta_i})\\), the calculation of the hat matrix \\((\\mathbf{H})\\), and the calculation of the model predictions \\((\\hat{y_i})\\).\n\nFirst, we construct the design matrix:\n\nnn &lt;- nrow(daph)\nyy &lt;- matrix(data=daph$Daphnia, nrow = nn, ncol = 1)\nhead(yy)\n\n      [,1]\n[1,] -1.15\n[2,] -1.73\n[3,] -1.89\n[4,] -0.94\n[5,] -0.05\n[6,]  0.99\n\nintercept &lt;- rep(1, nn)\ngreens &lt;- daph$Greens\nXX &lt;- cbind(intercept, greens)\nhead(XX)\n\n     intercept greens\n[1,]         1  -1.32\n[2,]         1  -1.51\n[3,]         1  -2.48\n[4,]         1  -0.69\n[5,]         1   0.02\n[6,]         1   0.82\n\n\nUsing the design matrix, we can find the parameter estimates \\((\\hat{\\beta_i})\\) using the following formula: \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{y}.\n\\]\n\nbeta_hat &lt;- solve(t(XX) %*% XX) %*% t(XX) %*% yy # or solve(crossprod(XX,XX),crossprod(XX,yy))\nbeta_hat\n\n                 [,1]\nintercept -0.04914691\ngreens     0.42112528\n\n#check answer\ndaph_mod &lt;- lm(data=daph, Daphnia~Greens)\ntidy(daph_mod) %&gt;% select(c(term, estimate))\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n-0.0491469\n\n\nGreens\n0.4211253\n\n\n\n\n\n\nNow we find the hat matrix \\((\\mathbf{H})\\):\n\n## hat matrix\nHH &lt;- XX %*% solve(t(XX) %*% XX) %*% t(XX)\ndim(HH)\n\n[1] 36 36\n\n# Check answer\nhat_auto &lt;- optR::hatMatrix(XX)\nall.equal(HH, hat_auto)\n\n[1] TRUE\n\n\nFinally, we find the model predictions \\((\\hat{y_i})\\):\n\ny_hat &lt;- HH %*% yy\n#or\ny_hat &lt;- XX %*% beta_hat\n\n# alternative built-in methods\npredict(daph_mod)\n\n           1            2            3            4            5            6 \n-0.605032288 -0.685046093 -1.093537619 -0.339723359 -0.040724407  0.296175821 \n           7            8            9           10           11           12 \n 0.969976277 -0.095470694 -0.318667095  0.098246938  0.409879648  0.009810628 \n          13           14           15           16           17           18 \n-0.082836935  0.216162017  0.506738464  0.536217234  0.056134409 -0.002823131 \n          19           20           21           22           23           24 \n 0.658343567  0.047711903 -0.015456889 -0.019668142 -0.074414429 -0.466060944 \n          25           26           27           28           29           30 \n 0.152993225 -0.592398530 -0.238653290 -0.440793427  0.430935913  0.161415730 \n          31           32           33           34           35           36 \n 0.334077097  0.110880696  0.169838236 -0.099681946  0.334077097 -0.078625682 \n\nhead(broom::augment(daph_mod)) # see fitted column\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDaphnia\nGreens\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n-1.15\n-1.32\n-0.6050323\n-0.5449677\n0.0916530\n0.8569811\n0.0228327\n-0.6727384\n\n\n-1.73\n-1.51\n-0.6850461\n-1.0449539\n0.1094821\n0.8409326\n0.1043338\n-1.3027983\n\n\n-1.89\n-2.48\n-1.0935376\n-0.7964624\n0.2346629\n0.8480611\n0.1758904\n-1.0711252\n\n\n-0.94\n-0.69\n-0.3397234\n-0.6002766\n0.0482177\n0.8560678\n0.0132742\n-0.7239090\n\n\n-0.05\n0.02\n-0.0407244\n-0.0092756\n0.0281488\n0.8627408\n0.0000018\n-0.0110699\n\n\n0.99\n0.82\n0.2961758\n0.6938242\n0.0422076\n0.8538700\n0.0153292\n0.8340942\n\n\n\n\n\n\n\nCalculate and report your estimate of the residual variance \\((\\sigma^2)\\).\n\nWe can use the following formula: \\[\\sigma^2 = \\frac{\\mathrm{RSS}}{n-p}, \\quad\\text{where}\\] \\[\\mathrm{RSS}= \\mathbf{e^\\top}\\mathbf{e}=\\left(\\mathbf{y}-X\\hat{\\beta}\\right)^\\top\\left(\\mathbf{y}-X\\hat{\\beta}\\right)\\]\n\nrss &lt;- as.numeric(crossprod(yy-XX %*% beta_hat) )\nrss\n\n[1] 24.56271\n\n# check answer\nall.equal(rss, deviance(daph_mod))\n\n[1] TRUE\n\nsigma2 &lt;- rss/(nn-2)\nsigma2\n\n[1] 0.7224325\n\n# check answer\nall.equal(sigma2, sigma(daph_mod)^2)\n\n[1] TRUE\n\n\n\nGive a prediction of what you might expect the specific abundance of Daphnia to be on the next sampling occasion if the abundance of green algae is 1.5 units. Also provide an estimate of the interval around your estimate that conveys 95% confidence in your prediction. Again, do so via direct calculations rather than relying on R’s built-in functions.\n\nFirst, we use the \\(\\beta_0\\) and \\(\\beta_1\\) values we found above to estimate \\(y\\) when \\(x=1.5\\).\n\n# y = b_0 + b_1 * x\ny_star &lt;- unname(beta_hat[1,1])+unname(beta_hat[2,1])*1.5\ny_star\n\n[1] 0.582541\n\n#OR\n\n#vector of new data\nX_star &lt;- c(intercept = 1, greens = 1.5)\nsum(X_star * beta_hat)\n\n[1] 0.582541\n\n\nNow we need to find the standard error of our estimate\n\n## inside sqrt\ninner_X &lt;- t(X_star) %*% solve(t(XX) %*% XX) %*% X_star\n\n## critical t-value\nt_crit &lt;- qt(0.975, df = nn-2)\n\n## estimated SD\nsigma &lt;- sqrt(sigma2)\n\n## 95% CI\ny_star + c(-1,1) * t_crit * sigma * c(sqrt(inner_X))\n\n[1] 0.07980506 1.08527698\n\n# for prediction interval, \ninner_X_pred &lt;- 1+ t(X_star) %*% solve(t(XX) %*% XX) %*% X_star\ny_star + c(-1,1) * t_crit * sigma * c(sqrt(inner_X_pred))\n\n[1] -1.216459  2.381541\n\n\nNow let’s check our answer using the built-in functions in R\n\npredict(daph_mod, new = data.frame(Greens=1.5),\n        level = 0.95, interval = \"confidence\")\n\n       fit        lwr      upr\n1 0.582541 0.07980506 1.085277\n\npredict(daph_mod, new = data.frame(Greens=1.5),\n        level = 0.95, interval = \"pred\")\n\n       fit       lwr      upr\n1 0.582541 -1.216459 2.381541\n\n\n\n\nQuestion 2\n\nExpand upon your model from Question 1 to include the additional effects of Cyclops and water temperature on Daphnia. Write out your equation and describe the terms in the model.\n\n\\[y_i =\\beta_0 + \\beta_1 x_{1,i}+\\beta_2 x_{2,i}+\\beta_3 x_{3,i}+e_i \\] Each observation of Daphnia abundance \\((y_i)\\) is a function of an intercept \\((\\beta_0)\\), the effect \\((\\beta_1)\\) of green algae density \\((x_{1,i})\\), the effect \\((\\beta_2)\\) of Cyclops density \\((x_{2,i})\\), and the effect \\((\\beta_3)\\) of water temperature \\((x_{3,i})\\). The model residuals are again assumed to be independent and normally distributed with mean 0 and variance \\(\\sigma^2\\), such that \\(e_i \\sim \\mathrm{N}(0, \\sigma^2)\\).\n\nUsing R’s built-in functions, fit the model from (a) and show the resulting table of results. For each of the \\(p\\)-values shown in the table, describe the null hypothesis being tested.\n\n\ndaphmod_full &lt;- lm(Daphnia ~ Greens + Cyclops + Temp, data=daph)\nsumary(daphmod_full)\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept) -4.143339   0.650218 -6.3722 3.711e-07\nGreens       0.046216   0.099249  0.4657 0.6446140\nCyclops      0.278715   0.075458  3.6936 0.0008212\nTemp         0.291675   0.045371  6.4286 3.157e-07\n\nn = 36, p = 4, Residual SE = 0.50245, R-Squared = 0.73\n\n\nThe first \\(p\\)-value corresponds to a t-test that the intercept is 0, \\(H_0: \\beta_0=0\\). The remaining three \\(p\\)-values shown in the table correspond to null hypothesis tests that the given predictor can be dropped from the model.\nFor example, can Cyclops be dropped from this model? We fit a reduced model and compare to it to the full model via an \\(F\\)-test with \\(H_0: \\beta_\\text{cyclops} = 0\\)\n\\[\n\\begin{aligned}\n\\Theta: \\text{Daphnia}_i &= \\beta_0 + \\beta_1 \\text{Greens}_i + \\beta_2 \\text{Cyclops}_i + \\beta_3 \\text{Temp}_i + e_i \\\\\n~ \\\\\n\\theta: \\text{Daphnia}_i &= \\beta_0 + \\beta_1 \\text{Greens}_i + \\beta_2 \\text{Temp}_i + e_i\n\\end{aligned}\n\\]\n\n## reduced model without `Cyclops`\nreduced_mod &lt;- lm(Daphnia ~ Greens + Temp, data=daph)\n## use `anova('reduced', 'full')` to get the F-test results\nanova(reduced_mod, daphmod_full)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n33\n11.522716\nNA\nNA\nNA\nNA\n\n\n32\n8.078507\n1\n3.444209\n13.64295\n0.0008212\n\n\n\n\n\n\nWe see that the resulting p-value is the same as the one displayed for the Cyclops estimate in the summary table for the full model.\n\nTest the hypothesis that \\(\\beta_{Greens} = \\beta_{Cyclops} = \\beta_{Temp} = 0\\). What is the \\(F\\)-statistic, the associated \\(df\\), and the \\(p\\)-value? What can you conclude from this test?\n\nWe write the null hypothesis as \\[H_0: \\beta_1 = \\beta_2 = \\beta_3 = 0\\] which corresponds to a model where we simply estimate the data based on their mean.\n\\[\\begin{align*}\n\\Theta: \\mathbf{y} &= \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{e} \\\\\n\\theta: \\mathbf{y} &= \\boldsymbol{\\mu} + \\mathbf{e} \\\\\n\\end{align*}\\]\nWe will base this test on an \\(F\\)-distribution, such that\n\\[\\begin{align*}\nSSE_{\\Theta} &= \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right)^{\\top} \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) = \\mathbf{e}^{\\top} \\mathbf{e} = SSE \\\\\nSSE_{\\theta} &= \\left( \\mathbf{y} - \\bar{y} \\right)^{\\top} \\left( \\mathbf{y} - \\bar{y} \\right) =  SSTO \\\\\n&\\Downarrow \\\\\nF &= \\frac{ \\left( SSTO - SSE \\right)  / (k - 1) } { SSE  / (n - k)}\n\\end{align*}\\]\n\n\\(F\\)-test by hand\n\n## get matrix of predictors\nXX &lt;- daph %&gt;% select(-Daphnia) %&gt;% add_column(Intercept=1, .before = \"Temp\") %&gt;% as.matrix()\n## estimate beta\nbeta_hat &lt;- solve(t(XX) %*% XX) %*% t(XX) %*% yy\n## total sum of squares\nSSE &lt;- t(yy - XX %*% beta_hat) %*% (yy - XX %*% beta_hat)\n## error sum of squares\nSSTO &lt;- t(yy - mean(yy)) %*% (yy - mean(yy))\n## F statistic\n(F_stat &lt;- ((SSTO - SSE) / (4 - 1)) / (SSE / (nn - 4)))\n\n         [,1]\n[1,] 29.47894\n\n## F test\npf(F_stat, 4-1, nn-4, lower.tail = F)\n\n             [,1]\n[1,] 2.467335e-09\n\n\nThis \\(F\\)-statistic is quite large and the \\(p\\)-value is very small, so we would reject the null hypothesis that we would be justified in dropping the 3 predictors from this model in favor of a mean-only model.\nChecking using the built-in summary function, we see that the F-statistic and p-value reported at the bottom of the output are the same as our manually calculated values.\n\nsummary(daphmod_full)\n\n\nCall:\nlm(formula = Daphnia ~ Greens + Cyclops + Temp, data = daph)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.74029 -0.41712 -0.04736  0.27781  1.16894 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.14334    0.65022  -6.372 3.71e-07 ***\nGreens       0.04622    0.09925   0.466 0.644614    \nCyclops      0.27871    0.07546   3.694 0.000821 ***\nTemp         0.29168    0.04537   6.429 3.16e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5024 on 32 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7094 \nF-statistic: 29.48 on 3 and 32 DF,  p-value: 2.467e-09\n\n\n\nIt has come to your attention that someone has done lab experiments suggesting the effect of temperature on Daphnia abundance is 0.4 per degree Celsius after controlling for the effects of prey (green algae) and competitors (Cyclops). Create a null hypothesis test to evaluate the evidence for this finding from the data collected in the field. Specify \\(H_0\\) and report the results of your test. What do you conclude?\n\n\\[\n\\text{Daphnia}_i = \\beta_0 + \\beta_1 \\text{Greens}_i + \\beta_2 \\text{Cyclops}_i + 0.4* \\text{Temp}_i + e_i\n\\]\nWe write the null hypothesis as \\[H_0: \\beta_{\\text{temp}} = 0.4\\]\n\n## model with effect of `elevation` = 1\nfixed_mod &lt;- lm(Daphnia ~ Greens + Cyclops + offset(0.4 * Temp), data=daph)\n## use `anova('comb', 'full')` to get the F-test results\nanova(fixed_mod, daphmod_full)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n33\n9.517552\nNA\nNA\nNA\nNA\n\n\n32\n8.078507\n1\n1.439045\n5.700241\n0.0230387\n\n\n\n\n\n\nWe have found significant evidence to reject \\(H_0\\), suggesting that it may be worthwhile to re-run the lab experiments demonstrating the effect of temperature on Daphnia abundance is 0.4/°C after controlling for green algae and Cyclops density and determine if the findings can be replicated.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Labs/lab1.html",
    "href": "Labs/lab1.html",
    "title": "Lecture & Lab 1: Fitting Linear Models",
    "section": "",
    "text": "Faraway, Ch. 1 & 2\nMatrix math cheat sheet\n\ndata(\"gala\")\nhead(gala) # or glimpse(gala)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nEndemics\nArea\nElevation\nNearest\nScruz\nAdjacent\n\n\n\n\nBaltra\n58\n23\n25.09\n346\n0.6\n0.6\n1.84\n\n\nBartolome\n31\n21\n1.24\n109\n0.6\n26.3\n572.33\n\n\nCaldwell\n3\n3\n0.21\n114\n2.8\n58.7\n0.78\n\n\nChampion\n25\n9\n0.10\n46\n1.9\n47.4\n0.18\n\n\nCoamano\n2\n1\n0.05\n77\n1.9\n1.9\n903.82\n\n\nDaphne.Major\n18\n11\n0.34\n119\n8.0\n8.0\n1.84\n\n\n\n\n\ngala &lt;- gala %&gt;% select(-Endemics) %&gt;% clean_names()\n\nlmod &lt;- lm(species ~ area + elevation + nearest + scruz + adjacent, data = gala)\n\nsummary(lmod) # or faraway::sumary(lmod) or broom::tidy(lmod)\n\n\nCall:\nlm(formula = species ~ area + elevation + nearest + scruz + adjacent, \n    data = gala)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-111.679  -34.898   -7.862   33.460  182.584 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.068221  19.154198   0.369 0.715351    \narea        -0.023938   0.022422  -1.068 0.296318    \nelevation    0.319465   0.053663   5.953 3.82e-06 ***\nnearest      0.009144   1.054136   0.009 0.993151    \nscruz       -0.240524   0.215402  -1.117 0.275208    \nadjacent    -0.074805   0.017700  -4.226 0.000297 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.98 on 24 degrees of freedom\nMultiple R-squared:  0.7658,    Adjusted R-squared:  0.7171 \nF-statistic:  15.7 on 5 and 24 DF,  p-value: 6.838e-07\n\nx &lt;- model.matrix(~ area + elevation + nearest + scruz + adjacent, data = gala)\ny &lt;- gala$species\n  \nxtxi &lt;- solve(t(x) %*% x)\nxtxi %*% t(x) %*% y\n\n                    [,1]\n(Intercept)  7.068220709\narea        -0.023938338\nelevation    0.319464761\nnearest      0.009143961\nscruz       -0.240524230\nadjacent    -0.074804832\n\nsolve(crossprod(x,x),crossprod(x,y))\n\n                    [,1]\n(Intercept)  7.068220709\narea        -0.023938338\nelevation    0.319464761\nnearest      0.009143961\nscruz       -0.240524230\nadjacent    -0.074804832\n\ndeviance(lmod) #RSS\n\n[1] 89231.37\n\nsqrt(deviance(lmod)/df.residual(lmod)) #sigma\n\n[1] 60.97519\n\nsigma &lt;- summary(lmod)$sigma\n\nxtxi &lt;- summary(lmod)$cov.unscaled\n\n#standard errors of the coefficients\nsqrt(diag(xtxi))*sigma #OR\n\n(Intercept)        area   elevation     nearest       scruz    adjacent \n19.15419782  0.02242235  0.05366280  1.05413595  0.21540225  0.01770019 \n\nsummary(lmod)$coef[,2]\n\n(Intercept)        area   elevation     nearest       scruz    adjacent \n19.15419782  0.02242235  0.05366280  1.05413595  0.21540225  0.01770019 \n\n\n\n\n\n\n\ndata(\"teengamb\")\n\ngambmod &lt;- lm(gamble ~ sex + status + income + verbal, data = teengamb)\nmodsum&lt;- summary(gambmod)\nmodsum\n\n\nCall:\nlm(formula = gamble ~ sex + status + income + verbal, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.082 -11.320  -1.451   9.452  94.252 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22.55565   17.19680   1.312   0.1968    \nsex         -22.11833    8.21111  -2.694   0.0101 *  \nstatus        0.05223    0.28111   0.186   0.8535    \nincome        4.96198    1.02539   4.839 1.79e-05 ***\nverbal       -2.95949    2.17215  -1.362   0.1803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.69 on 42 degrees of freedom\nMultiple R-squared:  0.5267,    Adjusted R-squared:  0.4816 \nF-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06\n\n#A\nmodsum$r.squared\n\n[1] 0.5267234\n\n#B\nunname(which.max(modsum$residuals))\n\n[1] 24\n\n#C\nmean(modsum$residuals)\n\n[1] -2.26769e-16\n\nmedian(modsum$residuals)\n\n[1] -1.451392\n\n#D\ncor(gambmod$fitted.values, modsum$residuals)\n\n[1] -5.79346e-17\n\nplot(gambmod, which = 1) # or DHARMa::plotConventionalResiduals(gambmod)\n\n\n\nDHARMa::plotQQunif(gambmod)\n\n\n\n#E\ncor(modsum$residuals, teengamb$gamble)\n\n[1] 0.687951\n\n#F \nabs(modsum$coefficients[\"sex\", \"Estimate\"])\n\n[1] 22.11833\n\nemmeans::emmeans(gambmod, \"sex\")\n\n sex emmean   SE df lower.CL upper.CL\n   0  28.24 4.69 42     18.8     37.7\n   1   6.12 5.91 42     -5.8     18.0\n\nConfidence level used: 0.95 \n\nemmplot &lt;- plot(emmeans(gambmod, \"sex\"), colors=c(\"#003087\")) # or marginal_effects::plot_predictions(gambmod, condition = \"sex\")\nemmplot+\n  mytheme+\n  labs(y=\"Sex\", x=\"Estimated marginal mean\")\n\n\n\n\n\n\n\n\ndata(\"uswages\")\n\nwagemod &lt;- lm(wage ~ educ + exper, data=uswages)\ntidy(wagemod)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-242.799412\n50.6815927\n-4.790682\n1.8e-06\n\n\neduc\n51.175268\n3.3419257\n15.313108\n0.0e+00\n\n\nexper\n9.774767\n0.7505955\n13.022683\n0.0e+00\n\n\n\n\n\nwagemod$coefficients[\"educ\"]\n\n    educ \n51.17527 \n\n# Each additional year of education increases the predicted weekly wage by around $51\n\nlog_wagemod &lt;- lm(log(wage) ~ educ + exper, data=uswages)\nsummary(log_wagemod)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper, data = uswages)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7533 -0.3495  0.1068  0.4381  3.5699 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.650319   0.078354   59.35   &lt;2e-16 ***\neduc        0.090506   0.005167   17.52   &lt;2e-16 ***\nexper       0.018079   0.001160   15.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6615 on 1997 degrees of freedom\nMultiple R-squared:  0.1749,    Adjusted R-squared:  0.174 \nF-statistic: 211.6 on 2 and 1997 DF,  p-value: &lt; 2.2e-16\n\ncompare_performance(wagemod, log_wagemod)\n\nSome of the nested models seem to be identical\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC\nAIC_wt\nAICc\nAICc_wt\nBIC\nBIC_wt\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\nwagemod\nlm\n29915.88\n0\n29915.9\n0\n29938.28\n0\n0.1351186\n0.1342524\n427.532832\n427.8538424\n\n\nlog_wagemod\nlm\n28706.68\n1\n28706.7\n1\n28729.08\n1\n0.1748605\n0.1740342\n0.660967\n0.6614633\n\n\n\n\n\ncheck_model(wagemod)\n\n\n\ncheck_model(log_wagemod)\n\n\n\n\nAlthough interpretation of the coefficients is less straightforward for the log-transformed model, it has much lower AIC/AICc and RMSE values and a higher R-squared, indicating it is a better fit for the data. The posterior predictive checks also look much better for the log model."
  },
  {
    "objectID": "Labs/lab1.html#exercises",
    "href": "Labs/lab1.html#exercises",
    "title": "Lecture & Lab 1: Fitting Linear Models",
    "section": "",
    "text": "data(\"teengamb\")\n\ngambmod &lt;- lm(gamble ~ sex + status + income + verbal, data = teengamb)\nmodsum&lt;- summary(gambmod)\nmodsum\n\n\nCall:\nlm(formula = gamble ~ sex + status + income + verbal, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.082 -11.320  -1.451   9.452  94.252 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22.55565   17.19680   1.312   0.1968    \nsex         -22.11833    8.21111  -2.694   0.0101 *  \nstatus        0.05223    0.28111   0.186   0.8535    \nincome        4.96198    1.02539   4.839 1.79e-05 ***\nverbal       -2.95949    2.17215  -1.362   0.1803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.69 on 42 degrees of freedom\nMultiple R-squared:  0.5267,    Adjusted R-squared:  0.4816 \nF-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06\n\n#A\nmodsum$r.squared\n\n[1] 0.5267234\n\n#B\nunname(which.max(modsum$residuals))\n\n[1] 24\n\n#C\nmean(modsum$residuals)\n\n[1] -2.26769e-16\n\nmedian(modsum$residuals)\n\n[1] -1.451392\n\n#D\ncor(gambmod$fitted.values, modsum$residuals)\n\n[1] -5.79346e-17\n\nplot(gambmod, which = 1) # or DHARMa::plotConventionalResiduals(gambmod)\n\n\n\nDHARMa::plotQQunif(gambmod)\n\n\n\n#E\ncor(modsum$residuals, teengamb$gamble)\n\n[1] 0.687951\n\n#F \nabs(modsum$coefficients[\"sex\", \"Estimate\"])\n\n[1] 22.11833\n\nemmeans::emmeans(gambmod, \"sex\")\n\n sex emmean   SE df lower.CL upper.CL\n   0  28.24 4.69 42     18.8     37.7\n   1   6.12 5.91 42     -5.8     18.0\n\nConfidence level used: 0.95 \n\nemmplot &lt;- plot(emmeans(gambmod, \"sex\"), colors=c(\"#003087\")) # or marginal_effects::plot_predictions(gambmod, condition = \"sex\")\nemmplot+\n  mytheme+\n  labs(y=\"Sex\", x=\"Estimated marginal mean\")\n\n\n\n\n\n\n\n\ndata(\"uswages\")\n\nwagemod &lt;- lm(wage ~ educ + exper, data=uswages)\ntidy(wagemod)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-242.799412\n50.6815927\n-4.790682\n1.8e-06\n\n\neduc\n51.175268\n3.3419257\n15.313108\n0.0e+00\n\n\nexper\n9.774767\n0.7505955\n13.022683\n0.0e+00\n\n\n\n\n\nwagemod$coefficients[\"educ\"]\n\n    educ \n51.17527 \n\n# Each additional year of education increases the predicted weekly wage by around $51\n\nlog_wagemod &lt;- lm(log(wage) ~ educ + exper, data=uswages)\nsummary(log_wagemod)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper, data = uswages)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7533 -0.3495  0.1068  0.4381  3.5699 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.650319   0.078354   59.35   &lt;2e-16 ***\neduc        0.090506   0.005167   17.52   &lt;2e-16 ***\nexper       0.018079   0.001160   15.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6615 on 1997 degrees of freedom\nMultiple R-squared:  0.1749,    Adjusted R-squared:  0.174 \nF-statistic: 211.6 on 2 and 1997 DF,  p-value: &lt; 2.2e-16\n\ncompare_performance(wagemod, log_wagemod)\n\nSome of the nested models seem to be identical\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC\nAIC_wt\nAICc\nAICc_wt\nBIC\nBIC_wt\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\nwagemod\nlm\n29915.88\n0\n29915.9\n0\n29938.28\n0\n0.1351186\n0.1342524\n427.532832\n427.8538424\n\n\nlog_wagemod\nlm\n28706.68\n1\n28706.7\n1\n28729.08\n1\n0.1748605\n0.1740342\n0.660967\n0.6614633\n\n\n\n\n\ncheck_model(wagemod)\n\n\n\ncheck_model(log_wagemod)\n\n\n\n\nAlthough interpretation of the coefficients is less straightforward for the log-transformed model, it has much lower AIC/AICc and RMSE values and a higher R-squared, indicating it is a better fit for the data. The posterior predictive checks also look much better for the log model."
  },
  {
    "objectID": "Labs/lab1.html#simple-model",
    "href": "Labs/lab1.html#simple-model",
    "title": "Lecture & Lab 1: Fitting Linear Models",
    "section": "2.1 Simple model",
    "text": "2.1 Simple model\n\n2.1.1 Estimating beta-hat and y-hat\n\nnn &lt;- nrow(gala)\nyy &lt;- matrix(data=gala$species, nrow = nn, ncol = 1)\nhead(yy)\n\n     [,1]\n[1,]   58\n[2,]   31\n[3,]    3\n[4,]   25\n[5,]    2\n[6,]   18\n\nintercept &lt;- rep(1, nn)\narea &lt;- gala$area\nXX &lt;- cbind(intercept, area)\nhead(XX)\n\n     intercept  area\n[1,]         1 25.09\n[2,]         1  1.24\n[3,]         1  0.21\n[4,]         1  0.10\n[5,]         1  0.05\n[6,]         1  0.34\n\nbeta_hat &lt;- solve(t(XX) %*% XX) %*% t(XX) %*% yy\nbeta_hat\n\n                 [,1]\nintercept 63.78286147\narea       0.08196317\n\nHH &lt;- XX %*% solve(t(XX) %*% XX) %*% t(XX)\ny_hat &lt;- HH %*% yy\nhead(y_hat)\n\n         [,1]\n[1,] 65.83932\n[2,] 63.88450\n[3,] 63.80007\n[4,] 63.79106\n[5,] 63.78696\n[6,] 63.81073\n\nhead(XX %*% beta_hat)\n\n         [,1]\n[1,] 65.83932\n[2,] 63.88450\n[3,] 63.80007\n[4,] 63.79106\n[5,] 63.78696\n[6,] 63.81073\n\n\n\nggplot(gala, aes(x=area, y=species))+\n  geom_point()+\n  mytheme+\n  labs(x=expression(\"Area of island (\"*km^2*\")\"), y=\"Number of species\")+\n  geom_smooth(method=\"lm\", se=FALSE, formula = y~x, color=\"black\", linewidth=0.5)\n\n\n\n\n\n\n2.1.2 Automated model fitting\n\nsimple_model &lt;- lm(species ~ area, gala)\nsimple_model\n\n\nCall:\nlm(formula = species ~ area, data = gala)\n\nCoefficients:\n(Intercept)         area  \n   63.78286      0.08196  \n\n## via fitted\ny_hat_f &lt;- fitted(simple_model)\n## via predict\ny_hat_p &lt;- predict(simple_model, type = \"response\")\n## compare these to each other\nall.equal(y_hat_f, y_hat_p)\n\n[1] TRUE\n\n\n\n\n2.1.3 Goodness-of-fit\n\n## SSE\nresids &lt;- yy - y_hat\nSSE &lt;- sum(resids^2) # = t(resids) %*% resids\n\n## SSTO\nSSTO &lt;- sum((yy - mean(yy))^2)\n\n## R^2\n1 - SSE / SSTO\n\n[1] 0.3817301\n\nsummary(simple_model)\n\n\nCall:\nlm(formula = species ~ area, data = gala)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-99.495 -53.431 -29.045   3.423 306.137 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 63.78286   17.52442   3.640 0.001094 ** \narea         0.08196    0.01971   4.158 0.000275 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 91.73 on 28 degrees of freedom\nMultiple R-squared:  0.3817,    Adjusted R-squared:  0.3596 \nF-statistic: 17.29 on 1 and 28 DF,  p-value: 0.0002748\n\n\n\n\n2.1.4 Adjusted R-squared\n\nset.seed(514)\n## generate a vector of Gaussian white noise\nWN &lt;- rnorm(nn)\n## add this to our Galapagos data frame\ngala$WN &lt;- WN \n## fit a model with Area & WN\nsummary(lm(species ~ area + WN, gala))\n\n\nCall:\nlm(formula = species ~ area + WN, data = gala)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-93.86 -44.67 -21.73  15.93 308.84 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  70.68115   17.82480   3.965 0.000485 ***\narea          0.07892    0.01944   4.059 0.000378 ***\nWN          -20.27151   13.91996  -1.456 0.156843    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89.95 on 27 degrees of freedom\nMultiple R-squared:  0.4268,    Adjusted R-squared:  0.3843 \nF-statistic: 10.05 on 2 and 27 DF,  p-value: 0.0005465\n\n1 - (SSE / (nn - 2)) / (SSTO / (nn - 1)) #adj r2\n\n[1] 0.359649"
  },
  {
    "objectID": "Labs/lab1.html#better-model-lab-example",
    "href": "Labs/lab1.html#better-model-lab-example",
    "title": "Lecture & Lab 1: Fitting Linear Models",
    "section": "2.2 Better model (lab example)",
    "text": "2.2 Better model (lab example)\n\n## larger model\nfull_mod &lt;- lm(species ~ area + elevation + nearest, gala)\nfull_mod\n\n\nCall:\nlm(formula = species ~ area + elevation + nearest, data = gala)\n\nCoefficients:\n(Intercept)         area    elevation      nearest  \n   16.46471      0.01908      0.17134      0.07123  \n\n\n\n## get matrix of predictors\nXX &lt;- model.matrix(full_mod)\n## estimate beta\nbeta_hat &lt;- solve(t(XX) %*% XX) %*% t(XX) %*% yy\n## total sum of squares\nSSE &lt;- t(yy - XX %*% beta_hat) %*% (yy - XX %*% beta_hat)\n## error sum of squares\nSSTO &lt;- t(yy - mean(yy)) %*% (yy - mean(yy))\n## F statistic\nF_stat &lt;- ((SSTO - SSE) / (4 - 1)) / (SSE / (nn - 4))\npf(F_stat, 4-1, nn-4, lower.tail = F) #F-test\n\n             [,1]\n[1,] 8.816845e-05\n\n\n\n## null model; the '1' indicates an intercept-only model\nnull_mod &lt;- lm(species ~ 1, gala)\n## use `anova('simple', 'complex')` to get the F-test results\nanova(null_mod, full_mod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n29\n381081.4\nNA\nNA\nNA\nNA\n\n\n26\n169917.6\n3\n211163.8\n10.77043\n8.82e-05\n\n\n\n\n\nfull_mod_sum&lt;-summary(full_mod)\n\npf(full_mod_sum$fstatistic[1], full_mod_sum$fstatistic[2], full_mod_sum$fstatistic[3], lower.tail = F)\n\n       value \n8.816845e-05 \n\n\n\n## reduced model without `nearest`\nreduced_mod &lt;- lm(species ~ area + elevation, gala)\n## use `anova('reduced', 'full')` to get the F-test results\nanova(reduced_mod, full_mod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n27\n169946.8\nNA\nNA\nNA\nNA\n\n\n26\n169917.6\n1\n29.24263\n0.0044746\n0.9471792\n\n\n\n\n\n\n\n2.2.1 Testing a subspace\n\n## full model (with adjacent this time)\nfull_mod2 &lt;- lm(species ~ area + adjacent + elevation + nearest, gala)\n## reduced model without `elevation + nearest`\ncomb_mod &lt;- lm(species ~ I(area + adjacent) + elevation + nearest, gala)\n## use `anova('combined', 'full')` to get the F-test results\nanova(comb_mod, full_mod2)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n26\n116865.40\nNA\nNA\nNA\nNA\n\n\n25\n93867.15\n1\n22998.25\n6.125212\n0.0204613\n\n\n\n\n\n\n\n## model with effect of `elevation` = 1\nfixed_mod &lt;- lm(species ~ area + offset(1 * elevation) + nearest, gala)\n## use `anova('comb', 'full')` to get the F-test results\nanova(fixed_mod, full_mod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n27\n1679730.7\nNA\nNA\nNA\nNA\n\n\n26\n169917.6\n1\n1509813\n231.0246\n0\n\n\n\n\n\n\n\nsumary(full_mod)\n\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 16.464711  23.388841  0.7040 0.487718\narea         0.019085   0.026764  0.7131 0.482158\nelevation    0.171336   0.054519  3.1427 0.004151\nnearest      0.071227   1.064806  0.0669 0.947179\n\nn = 30, p = 4, Residual SE = 80.84116, R-Squared = 0.55\n\n## t statistic\n(t_value &lt;- (0.171336 - 1) / 0.054519)\n\n[1] -15.19955\n\n## p-value = t_alpha * Pr(t_value, df); `pt()` is the pdf for a t-dist\n(p_value &lt;- 1.96 * pt(t_value, 26))\n\n[1] 1.853024e-14\n\n## verify t^2 = F\nall.equal(t_value^2, anova(fixed_mod, full_mod)$F[2], tolerance = 0.0001)\n\n[1] TRUE\n\n\n\n\n2.2.2 CIs for beta-hat\n\n## critical value for the t-dist\n## `qt()` is the quantile function for the t-dist; `p` is the (1-alpha/2) value \nt_crit &lt;- qt(p = 0.975, df = 30-4)\n## 95% CI\nCI95_beta &lt;- 0.019085 + c(-1,1) * t_crit * 0.026764\nround(CI95_beta, 3)\n\n[1] -0.036  0.074\n\n\n\n## all of the 95% CI's\nconfint(full_mod)\n\n                   2.5 %      97.5 %\n(Intercept) -31.61174063 64.54116288\narea         -0.03593020  0.07409948\nelevation     0.05927051  0.28340204\nnearest      -2.11751249  2.25996697\n\n\n\n\n2.2.3 Bootstrap confidence intervals\n\n## residuals from our full model\nresids &lt;- residuals(full_mod)\n\n## number of bootstrap samples\nnb &lt;- 1000\n## empty matrix for beta estimates\nbeta_est &lt;- matrix(NA, nb, 4)\n## fitted values from our full model = X*beta\nXbeta &lt;- fitted(full_mod)\n## sample many times\nfor(i in 1:nb) {\n  ## 3a: sample w/ replacement from e\n  e_star &lt;- sample(resids, rep = TRUE)\n  ## 3b: calculate y_star\n  y_star &lt;- Xbeta + e_star\n  ## 3c: re-estimate beta_star from X & y_star\n  beta_star &lt;- update(full_mod, y_star ~ .)\n  ## save estimated betas\n  beta_est[i,] &lt;- coef(beta_star)\n}\n\n## extract 2.5% and 97.5% values\nCI95 &lt;- apply(beta_est, 2, quantile, c(0.025, 0.975))\ncolnames(CI95) &lt;- c(\"Intercept\", \"area\", \"elevation\" , \"nearest\")\nt(round(CI95, 3))\n\n             2.5%  97.5%\nIntercept -22.402 61.470\narea       -0.027  0.084\nelevation   0.073  0.277\nnearest    -1.918  2.150\n\n\n\n\n2.2.4 Boostrap coefficients and CIs using the rsample package\n\nset.seed(462)\nlibrary(rsample)\n\n# Will be used to fit the models to different bootstrap data sets:\nfit_fun &lt;- function(split, ...) {\n  # We could check for convergence, make new parameters, etc.\n  lm(species ~ area + elevation + nearest, data = analysis(split), ...) %&gt;%\n    tidy()\n}\n\nbt &lt;-\n  bootstraps(gala, times = 1000, apparent = TRUE) %&gt;%\n  mutate(models = map(splits, fit_fun))\n\nint_pctl(bt, models)\n\n\n\n\n\nterm\n.lower\n.estimate\n.upper\n.alpha\n.method\n\n\n\n\n(Intercept)\n-33.7640985\n16.8546860\n57.8416355\n0.05\npercentile\n\n\narea\n-0.0911716\n0.1138601\n0.5206100\n0.05\npercentile\n\n\nelevation\n-0.1372787\n0.1516931\n0.4687342\n0.05\npercentile\n\n\nnearest\n-2.4452623\n-0.1349482\n1.8394215\n0.05\npercentile\n\n\n\n\n\n\n\n\n2.2.5 Confidence interval for new predictions\n\n2.2.5.1 By hand\n\n## matrix of predictors\nXX &lt;- model.matrix(simple_model)\n## new X; vector for now\nX_star &lt;- c(intercept = 1, area = 2000)\n## inside sqrt\ninner_X &lt;- t(X_star) %*% solve(t(XX) %*% XX) %*% X_star\n## critical t-value\nt_crit &lt;- qt(0.975, df = nn-2)\n## estimated SD\nsigma &lt;- summary(simple_model)$sigma\n## predicted y\ny_star &lt;- sum(X_star * coef(simple_model))\n## 95% CI\nc(y_star) + c(-1,1) * c(t_crit) * c(sigma) * c(sqrt(inner_X))\n\n[1] 149.5818 305.8366\n\n\n\n\n2.2.5.2 Using predict\n\npredict(simple_model, new = data.frame(t(X_star)),\n        level = 0.95, interval = \"confidence\")\n\n       fit      lwr      upr\n1 227.7092 149.5818 305.8366\n\n\n\n\n\n2.2.6 Prediction interval for new response\n\n## new X_star\nX_star &lt;- c(intercept = 1, area = 2000)\n## inside sqrt\ninner_X &lt;- 1 + t(X_star) %*% solve(t(XX) %*% XX) %*% X_star\n## 95% CI\ny_star + c(-1,1) * c(t_crit) * c(sigma) * c(sqrt(inner_X))\n\n[1]  24.21065 431.20776\n\npredict(simple_model, new = data.frame(t(X_star)),\n        level = 0.95, interval = \"prediction\")\n\n       fit      lwr      upr\n1 227.7092 24.21065 431.2078"
  },
  {
    "objectID": "Notes/Inference.html",
    "href": "Notes/Inference.html",
    "title": "Inference, Diagnostics, & Errors",
    "section": "",
    "text": "# List of packages required:\npackages &lt;- c(\"tidyverse\", \"PNWColors\", \"janitor\", \"faraway\", \"broom\", \"DHARMa\", \"emmeans\", \"performance\", \"ellipse\")\n\n# Load packages into session\nlapply(packages, require, character.only = TRUE)\nrm(packages)\n\n# Ensure functions with duplicate names are from the correct package\nselect &lt;- dplyr::select\nmap &lt;- purrr::map\nsummarize &lt;- dplyr::summarize\nclean_names &lt;- janitor::clean_names\nmargin &lt;- ggplot2::margin\n\nset.seed(123) #Set seed for pseudo-random number generator, for reproducibility\n\nmytheme &lt;- theme_light()+ #define custom theme for ggplots\n  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, l = 0)),\n        axis.title.x = element_text(margin = margin(t = 10, l = 0)),\n        text=element_text(size=15))\n\ndata(\"gala\")"
  },
  {
    "objectID": "HW/hw_03_diagnostics.html",
    "href": "HW/hw_03_diagnostics.html",
    "title": "HW 3: Model Diagnostics",
    "section": "",
    "text": "Background\nSection 7 of the U.S. Endangered Species Act (ESA) regulates situations in which a federal agency funds, permits, or otherwise has a “federal nexus” on any project that may influence a protected species. Federal agencies must seek a “consultation” on the project with either the U.S. Fish and Wildlife Service (USFWS) or the National Marine Fisheries Service (NMFS), depending on the species, and USFWS or NMFS must assure that any project does not cause “jeopardy” (a relatively high legal standard) for a protected species. A major conservation value of Section 7 consultation is the opportunity for USFWS and NMFS biologists to negotiate changes to projects that could minimize any negative impacts on species (or maximize any positive benefits).\nThe USFWS office in Lacey, Washington wanted to identify the characteristics of projects that would make them worthwhile for focused consultation time, with an emphasis on projects potentially impacting ESA-listed bull trout (Salvelinus confluentus). Experts developed assessments of the potential improvement(s) in a project that could be realized from negotiating changes to projects such as nearshore construction, culvert improvements, and riparian restoration. These assessment generated a unitless score of the potential value for 38 projects.\nAt this point the USFWS would like your assistance in evaluating a statistical model they hope to use for prioritizing project consultations. The accompanying data file usfws_bull_trout.csv contains 9 columns of information. They are\n\nscore: a project’s potential value (numerical score on a scale of 0-15)\nstage: 1 of 3 life history stage(s) occurring in the project area\n\nadults (A)\njuveniles/adults (JA)\neggs/juveniles (EJ)\n\nform: 1 of 2 life history form(s) occurring in the project area\n\nanadromous (An)\nfluvial/anadromous (FlAn)\n\ncond: 1 of 3 habitat conditions in the project area\n\npristine (P)\ndegraded (D)\nhighly degraded (H)\n\nrisk: 1 of 4 levels of extinction risk of the core population occurring in the project area\n\noutside core area (OC)\nlow (L)\nmedium (M)\nhigh (H)\n\nunit: 1 of 4 habitat unit types in the project area\n\ninside a core area (IC)\noutside a core area in freshwater (OF)\nmarine (M)\nother (OT)\n\nprog: whether or not the set of detailed management guidelines for projects of that type have been established\n\nYes\nNo\n\nBMP: whether or not established best management practices will be followed in the project\n\nYes\nNo\n\ndegflex: the degree of flexibility in project design, timing, and location\n\nlow (L)\nmedium (M)\n\n\n\n\nProblems\n\nFit a linear model to the dataset that includes all 8 predictor variables. What is the \\(R^2\\) for the model? Does is seem like a promising model?\n\n\nfws&lt;- read.csv(\"../data/usfws_bull_trout.csv\")\n\nfish &lt;- lm(score ~ stage+form+cond+risk+unit+prog+BMP+degflex, data=fws)\n\nsummary(fish)\n\n\nCall:\nlm(formula = score ~ stage + form + cond + risk + unit + prog + \n    BMP + degflex, data = fws)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8393 -0.4525  0.0465  0.6257  5.0838 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.27852    3.12978   1.048 0.305742    \nstageEJ      6.54996    1.67547   3.909 0.000704 ***\nstageJA      5.66124    1.48850   3.803 0.000916 ***\nformFlAn    -0.63393    2.38959  -0.265 0.793151    \ncondH        0.62407    1.10026   0.567 0.576072    \ncondP       -0.96800    1.55225  -0.624 0.539018    \nriskL        1.48544    2.70158   0.550 0.587728    \nriskM        0.64192    1.79215   0.358 0.723471    \nriskOC      -1.94297    4.11150  -0.473 0.640974    \nunitM        2.29973    3.26551   0.704 0.488348    \nunitOF       1.49315    2.84772   0.524 0.605065    \nunitOT       0.36718    3.17158   0.116 0.908839    \nprogYes     -1.98726    1.05228  -1.889 0.071633 .  \nBMPYes       0.06842    1.34844   0.051 0.959970    \ndegflexM     2.68433    0.91894   2.921 0.007685 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.152 on 23 degrees of freedom\nMultiple R-squared:  0.7549,    Adjusted R-squared:  0.6058 \nF-statistic: 5.061 on 14 and 23 DF,  p-value: 0.000314\n\nanova(fish)\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nstage\n2\n181.5597719\n90.7798860\n19.6083127\n0.0000107\n\n\nform\n1\n11.6307302\n11.6307302\n2.5122194\n0.1266212\n\n\ncond\n2\n3.6778367\n1.8389184\n0.3972035\n0.6767199\n\n\nrisk\n3\n36.6117530\n12.2039177\n2.6360270\n0.0738824\n\n\nunit\n3\n21.5314509\n7.1771503\n1.5502532\n0.2284705\n\n\nprog\n1\n33.4703391\n33.4703391\n7.2295406\n0.0131082\n\n\nBMP\n1\n0.0287882\n0.0287882\n0.0062182\n0.9378296\n\n\ndegflex\n1\n39.5041792\n39.5041792\n8.5328406\n0.0076851\n\n\nResiduals\n23\n106.4822561\n4.6296633\nNA\nNA\n\n\n\n\n\nglance(fish)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.7549299\n0.6057568\n2.151665\n5.060764\n0.000314\n14\n-73.49712\n178.9942\n205.1956\n106.4823\n23\n38\n\n\n\n\n\n\nThe \\(R^2\\) for the model is around 0.755 and the adjusted \\(R^2\\) is around 0.606. While these are relatively high, the anova() table suggests that some of the predictors may not be necessary.\n\nMake a plot of the residuals against the model predictions. Name at least two things you should be looking for in a plot like this. What do you see?\n\n\n# base R\nplot(fish, which=1)\n\n\n\n# ggplot\nres_fit &lt;- data.frame(res = fish$residuals, fit = fish$fitted.values)\n\nggplot(res_fit)+\n  geom_point(aes(x=fit, y=res))+\n  labs(x=\"Model predictions\", y=\"Residuals\")+\n  geom_hline(yintercept = 0, lty=2)+mytheme\n\n\n\n\nThings to look for:\n\nNo obvious nonlinear patterns\nConstant symmetrical variation (homoscedasticity) in the vertical (\\(\\hat{\\varepsilon}\\)) direction (i.e., vertical spread of the points does not increase or decrease with the x-values - no non-constant variance in \\(e\\))\nOutliers and leverage points\n\nAlthough there are no clear nonlinear patterns suggesting nonlinearity in the structural part of the model, the magnitude of the residuals appears to be greater for points with larger predicted values (increasing variance in \\(e\\) with increasing \\(\\hat{y}\\)), suggesting possible heteroscedasticity.\n\nSince the diagnostic plot has indicated potential heteroscedasticity, we can confirm our suspicion with a formal diagnostic test like the Breusch-Pagan test.\nThe check_heteroscedasticity function from the performance package computes the BP-test non-studentized and uses the fitted values for the potential explanatory variables.\n\ncheck_heteroscedasticity(fish)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.002).\n\n\nThis function has the additional benefit of a built-in plot method:\n\ncheck_heteroscedasticity(fish) %&gt;% plot()\n\n\n\n\n\nggplot(res_fit)+\n  geom_point(aes(x=fit, y=sqrt(abs(rstandard(fish)))))+\n  labs(x=\"Model predictions\", y=\"sqrt(abs(stdResiduals))\")+\n  mytheme\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nAs a default, the bptest function from the lmtest package uses the same explanatory variables as in the regression model you specify (and studentizes as well).\n\nlmtest::bptest(fish, studentize = TRUE) #studentized Breusch-Pagan test\n\n\n    studentized Breusch-Pagan test\n\ndata:  fish\nBP = 17.194, df = 14, p-value = 0.246\n\nlmtest::bptest(fish, studentize = FALSE) #Breusch-Pagan test\n\n\n    Breusch-Pagan test\n\ndata:  fish\nBP = 39.311, df = 14, p-value = 0.0003265\n\n\nWe can obtain matching results to the check_heteroscedasticity function by explicitly using the fitted values of the model:\n\nlmtest::bptest(fish, studentize = F, varformula = ~fitted.values(fish))\n\n\n    Breusch-Pagan test\n\ndata:  fish\nBP = 9.1824, df = 1, p-value = 0.002444\n\n\nThe same result can also be obtained using the ncvTest (Non-constant Variance Score Test) function from the package car:\n\ncar::ncvTest(fish)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 9.182395, Df = 1, p = 0.0024435\n\n\nFor additional functionality related to checking for heteroscedasticity, see this vignette from the olsrr package.\n\n\nMake a plot of the residuals against the predictor variable stage. Do you find this plot useful? Why or why not?\n\n\nres_fit &lt;- res_fit %&gt;% mutate(stage = fws$stage)\n\nggplot(res_fit)+\n  geom_point(aes(x=stage, y=res))+\n  labs(x=\"Life history stage\", y=\"Residuals\")+\n  geom_hline(yintercept = 0, lty=2)+mytheme\n\n\n\n\nThis plot is not particularly useful because there are only three life stages present and the design is unbalanced; the overall sample size is small and not all stages are equally represented. For example, the “EJ” stage only has three observations, making it difficult to determine if the possible difference in residual variance between stages is actually meaningful.\n\nProduce a \\(Q\\)-\\(Q\\) plot of the model residuals and include a \\(Q\\)-\\(Q\\) line. Describe what you would hope to see here. Do you?\n\n\n# base R\nqqnorm(residuals(fish),ylab=\"Residuals\",main=\"\")\nqqline(residuals(fish))\n\n\n\n# ggplot\nggplot() + \n  stat_qq(aes(sample = residuals(fish))) + \n  stat_qq_line(aes(sample = residuals(fish))) +\n  mytheme+labs(x=\"Theoretical Quantiles\", y=\"Residuals\")\n\n\n\n# performance\nplot(performance::check_residuals(fish))\n\n\n\n# Shapiro-Wilk test\nshapiro.test(residuals(fish))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(fish)\nW = 0.90503, p-value = 0.003547\n\n\nNormal residuals should approximately follow the Q-Q line. However, this plot shows large deviations from the line at both ends, somewhat like a leptokurtic (heavy-tailed) distribution.\n\n\ndatawizard::kurtosis(residuals(fish), type=\"1\") # \n\n\n\n\n\nKurtosis\nSE\n\n\n\n\n2.572732\n0.654624\n\n\n\n\n\nconfintr::kurtosis(residuals(fish))-3 #Pearson's Measure of Kurtosis\n\n[1] 2.572732\n\nmoments::kurtosis(residuals(fish))-3\n\n[1] 2.572732\n\nmoments::anscombe.test(residuals(fish))\n\n\n    Anscombe-Glynn kurtosis test\n\ndata:  residuals(fish)\nkurt = 5.5727, z = 2.5940, p-value = 0.009486\nalternative hypothesis: kurtosis is not equal to 3\n\nsummary(datawizard::kurtosis(residuals(fish)), test = TRUE)\n\n\n\n\n\nKurtosis\nSE\nz\np\n\n\n\n\n3.122581\n0.654624\n4.770038\n1.8e-06\n\n\n\n\n\n\nIndeed, we see that the distribution of residuals has an (excess) kurtosis value significantly larger than zero, indicating a leptokurtic distribution.\n\n\nWould it make sense to plot \\(e_t\\) against \\(e_{t+1}\\) for this model? Explain why or why not.\n\nIt would not make sense to plot \\(e_t\\) against \\(e_{t+1}\\) because the structure of these data would not result in serial autocorrelation. Plotting lagged residuals and formal diagnostic tests for residual autocorrelation are more relevant for temporal, spatial, and blocked data.\n\nWhich projects have the 3 largest leverages? Briefly explain what this tells us.\n\n\n## leverages of points\nhv &lt;- hatvalues(fish)\n## assign their names\nnames(hv) &lt;- fws$ProjectName\n\nslice_max(data.frame(hat=hv), hat, n=3)\n\n\n\n\n\n\nhat\n\n\n\n\nGail\n1.0000000\n\n\nVernon\n1.0000000\n\n\nTurner\n0.7332097\n\n\nForgotten\n0.7332097\n\n\n\n\n\n\nVernon, Gail, Turner, and Forgotten have the largest leverages. This means that they are extreme in the predictor \\((X)\\) space and may have had a large influence on the model fit.\n\nWhat rule of thumb could you use to assess whether any leverages are particularly large? Under this rule of thumb, do you have any particularly large leverages? If yes, which projects?\n\nA rule of thumb is that a leverage point is large if \\(h_{ii} \\geq 2\\bar{h} = \\frac{2p}{n}\\).\n\n## threshold value for h_i ~= 0.78\np &lt;-length(coefficients(fish))\nn &lt;- length(residuals(fish))\n\nth &lt;- 2 * ( p/n)\nth\n\n[1] 0.7894737\n\n## are any h_i &gt; Eh?\nhv &gt; th\n\n      Bethune         SR522           SPU        Boeing      Wilkeson \n        FALSE         FALSE         FALSE         FALSE         FALSE \n      Turner   Geotechnical  SR542Boulder     SR542Coal         Lummi \n        FALSE         FALSE         FALSE         FALSE         FALSE \n         ONP    SeattlePort       Griffen         Lopex   Bridgehaven \n        FALSE         FALSE         FALSE         FALSE         FALSE \n       Zornes     Bremerton        Sisney    Longfellow       Hartley \n        FALSE         FALSE         FALSE         FALSE         FALSE \n      Glacier          Gail      Heritage Stillaguamish    Bituminous \n        FALSE          TRUE         FALSE         FALSE         FALSE \n         Weed     Recycling        Vernon       Tukwila        Lynden \n        FALSE         FALSE          TRUE         FALSE         FALSE \n    Snohomish       I5Inter       Whatcom          SR20      Thompson \n        FALSE         FALSE         FALSE         FALSE         FALSE \n       Yeager       Century     Forgotten \n        FALSE         FALSE         FALSE \n\nhv[hv&gt;th]\n\n  Gail Vernon \n     1      1 \n\n\nUsing this rule of thumb, Gail and Vernon have particularly large leverages.\nWe can confirm this with a half-normal plot:\n\nhalfnorm(hv,labs=fws$ProjectName)\n\n\n\n\n\nCalculate the studentized residuals to look for outliers. Remember to use a Bonferroni correction, and explain why you should use it. What did you find? Which project has the largest studentized residual?\n\nWe can compute the studentized residuals as \\[t_i = \\frac{y_i-\\hat{y}_{(i)}}{\\hat{\\sigma}_{(i)}\\sqrt{1+X_{i} (X_{(i)}^\\top X_{(i)})^{-1}X_i}} \\]\nwhich are distributed as a \\(t\\)-distribution with \\(n-p-1\\) df. Fortunately, there is an easier way to compute \\(t_i\\) that avoids doing \\(n\\) regressions: \\[t_i=\\frac{\\hat{e_i}}{\\hat{\\sigma}_{(i)} \\sqrt{1-h_i}}= r_i \\left(\\frac{n-p-1}{n-p-r_i^2} \\right)^{1/2} \\]\n\nstd &lt;- rstandard(fish)\nstd_manual &lt;- residuals(fish)/(sigma(fish)*na_if(sqrt(1-hv), 0))\nall.equal(std, std_manual)\n\n[1] TRUE\n\nstu &lt;- rstudent(fish)\nstu_manual &lt;- std*sqrt(((n-p-1)/(n-p-std^2)))\nall.equal(stu, stu_manual)\n\n[1] TRUE\n\n\nWe need to use a Bonferroni correction when performing significance tests to determine if case \\(i\\) is an outlier because otherwise we would identify an excess of outliers (making Type I errors) just because of the number of cases that we are testing. The level of statistical significance \\((\\alpha)\\) determines probability of committing a type I error (rejecting the null hypothesis when it is actually true): if we had \\(n=100\\) and tested all of the cases with \\(\\alpha=0.05\\), we might expect to find around 5 outliers using this procedure even if the null hypothesis was true. Even though we might explicitly test only one or two large \\(t_i\\)’s, we are implicitly testing all cases since we need to consider all the residuals to find out which ones are large.\n\n# Part A\nmax_res &lt;- stu[which.max(abs(stu))]\nmax_res &lt;- unname(max_res)\nmax_res\n\n[1] 3.523815\n\n# Part B\ndf_fish &lt;- df.residual(fish)-1 #df=22\n\n# Part C\nun_p &lt;- 2*(1-pt(max_res, df=df_fish, lower.tail = TRUE)) #unadjusted p-value\n\n# Part D: adjusted p-value. These agree with each other\n2*(1-pt(max_res, df=df_fish, lower.tail = TRUE))*nrow(fws)\n\n[1] 0.07264217\n\np.adjust(un_p, method = \"bonferroni\", n = nrow(fws))\n\n[1] 0.07264217\n\np.adjust(un_p, method = \"bonferroni\", n = nrow(fws)-2)\n\n[1] 0.0688189\n\n2*(1-pt(max_res, df=df_fish, lower.tail = TRUE))*sum(!is.na(stu))\n\n[1] 0.0688189\n\np.adjust(un_p, method = \"bonferroni\", n = sum(!is.na(stu)))\n\n[1] 0.0688189\n\n# Part E\nt &lt;- qt(p=.05/(nrow(fws)*2), df=df_fish) #find adjusted test statistic, -3.678813, which has abs &gt; max_res, indicating that with the correction, the outlier is not significant\n\n# Part F. Here, the rstudet and unadjusted p-val columns agree with the results of Parts A and C, respectively, but the Bonferroni p-value uses the number of non-NA studentized residuals rather than the number of observations in the data set.\n\ncar::outlierTest(fish)\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n   rstudent unadjusted p-value Bonferroni p\n10 3.523815          0.0019116     0.068819\n\n\n\nCalculate Cook’s Distances and produce a halfnormal plot of them. Label the 3 largest \\(D_i\\) in the plot with the project names. Are these the same sites as the top 3 projects you identified in (g)? Briefly explain why or why not.\n\nCook’s D: \\[D_i=\\frac{e_i^2}{p} \\left(\\frac{h_i}{1-h_i} \\right) \\]\nThis metric combines the residual effect and leverage to quantify the influence. A half-normal plot of \\(D_i\\) can be used to identify influential observations.\n\ncook_fish &lt;- cooks.distance(fish)\nhalfnorm(cook_fish,3,labs=fws$ProjectName,ylab=\"Cook’s distances\")\n\n\n\nplot(check_outliers(fish))\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Notes/Inference.html#examples",
    "href": "Notes/Inference.html#examples",
    "title": "Inference, Diagnostics, & Errors",
    "section": "Examples",
    "text": "Examples\n\ndata(\"gala\")\nlmod &lt;- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala)\nnullmod &lt;- lm(Species ~ 1, gala)\nanova(lmod, nullmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n24\n89231.37\nNA\nNA\nNA\nNA\n\n\n29\n381081.37\n-5\n-291850\n15.69941\n7e-07\n\n\n\n\n\n\nWe can compute this by hand:\n\nrss0 &lt;- deviance(nullmod)\nrss &lt;- deviance(lmod)\ndf0 &lt;- df.residual(nullmod)\ndf &lt;- df.residual(lmod)\nfstat &lt;- ((rss-rss0)/(df-df0))/(rss/df)\n\n1-pf(fstat, df0-df, df)\n\n[1] 6.837893e-07\n\n\n\nTesting 1 predictor\n\nlmods &lt;- lm(Species ~ Elevation + Nearest + Scruz + Adjacent, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n25\n93469.08\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n1\n4237.718\n1.139792\n0.296318\n\n\n\n\n\nsumary(lmod)\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)  7.068221  19.154198  0.3690 0.7153508\nArea        -0.023938   0.022422 -1.0676 0.2963180\nElevation    0.319465   0.053663  5.9532 3.823e-06\nNearest      0.009144   1.054136  0.0087 0.9931506\nScruz       -0.240524   0.215402 -1.1166 0.2752082\nAdjacent    -0.074805   0.017700 -4.2262 0.0002971\n\nn = 30, p = 6, Residual SE = 60.97519, R-Squared = 0.77\n\n\n\nsumary(lm(Species ~ Area, gala))\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept) 63.782861  17.524416  3.6397 0.0010943\nArea         0.081963   0.019713  4.1578 0.0002748\n\nn = 30, p = 2, Residual SE = 91.73159, R-Squared = 0.38\n\n\n\n\nTesting a pair of predictors\n\nlmods &lt;- lm(Species ~ Elevation + Nearest + Scruz, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n26\n158291.63\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n2\n69060.26\n9.287352\n0.0010297\n\n\n\n\n\n\n\n\nTesting a subspace\n\\[H_0:\\beta_\\text{Area}= \\beta_\\text{Adjacent}\\]\n\nlmods &lt;- lm(Species ~ I(Area+Adjacent) + Elevation + Nearest + Scruz, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n25\n109591.12\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n1\n20359.75\n5.476035\n0.0279256\n\n\n\n\n\n\n\\[H_0:\\beta_\\text{Elevation}= 0.5\\]\n\nlmods &lt;- lm(Species ~ Area+ offset(0.5*Elevation) + Nearest + Scruz + Adjacent, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n25\n131312.12\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n1\n42080.76\n11.3182\n0.0025738\n\n\n\n\n\n\nA simpler way to test such point hypotheses is to use a t-statistic: \\[t = (\\hat{\\beta}−c)/\\text{se}(\\hat{\\beta})\\] where \\(c\\) is the point hypothesis. In this example, the statistic and corresponding p-value are:\n\nb_hat &lt;- unname(lmod$coefficients[\"Elevation\"])\nse_b_hat &lt;- summary(lmod)$coef[\"Elevation\", \"Std. Error\"]\n(tstat &lt;- (b_hat-0.5)/se_b_hat)\n\n[1] -3.364253\n\n2*pt(tstat, 24)\n\n[1] 0.002573836\n\ntstat^2\n\n[1] 11.3182"
  },
  {
    "objectID": "Notes/Inference.html#permutation-tests",
    "href": "Notes/Inference.html#permutation-tests",
    "title": "Inference, Diagnostics, & Errors",
    "section": "Permutation Tests",
    "text": "Permutation Tests\n\nOverall model\n\nlmod &lt;- lm(Species ~ Nearest + Scruz, gala)\nlms &lt;- summary(lmod)\nlms$fstatistic\n\n     value      numdf      dendf \n 0.6019558  2.0000000 27.0000000 \n\npf(lms$fstatistic[1],lms$fstatistic[2],lms$fstatistic[3],lower.tail=F)\n\n    value \n0.5549255 \n\n\n\nset.seed(123)\n\nnreps &lt;- 4000\nfstats &lt;- numeric(nreps)\n\nfor(i in 1:nreps){\n     lmods &lt;- lm(sample(Species) ~ Nearest+Scruz , gala)\n     fstats[i] &lt;- summary(lmods)$fstat[1]\n}\n\nmean(fstats &gt; lms$fstat[1])\n\n[1] 0.55825\n\n\n\n\nSingle predictor\n\nsummary(lmod)$coef[3,]\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n-0.4406401  0.4025312 -1.0946731  0.2833295 \n\ntstats &lt;- numeric(nreps)\n\nset.seed(123)\n\nfor(i in 1:nreps){\n  lmods &lt;- lm(Species ~ Nearest+sample(Scruz), gala)\n  tstats[i] &lt;- summary(lmods)$coef[3,3]\n}\n\nmean(abs(tstats) &gt; abs(lms$coef[3,3]))\n\n[1] 0.26825"
  },
  {
    "objectID": "Notes/Inference.html#confidence-intervals",
    "href": "Notes/Inference.html#confidence-intervals",
    "title": "Inference, Diagnostics, & Errors",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nSingle parameter\n\\[\\hat{\\beta_i}\\pm t^{(\\alpha/2)}_{n-p}\\text{se}(\\hat{\\beta}) \\]\n\nlmod &lt;- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala)\nsumary(lmod)\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)  7.068221  19.154198  0.3690 0.7153508\nArea        -0.023938   0.022422 -1.0676 0.2963180\nElevation    0.319465   0.053663  5.9532 3.823e-06\nNearest      0.009144   1.054136  0.0087 0.9931506\nScruz       -0.240524   0.215402 -1.1166 0.2752082\nAdjacent    -0.074805   0.017700 -4.2262 0.0002971\n\nn = 30, p = 6, Residual SE = 60.97519, R-Squared = 0.77\n\n\nWe can construct individual 95% CIs for \\(\\beta_\\text{Area}\\) for which we need the 2.5% and 97.5% percentiles of the t-distribution with 30−6 = 24 degrees of freedom.\n\n(t_stat&lt;-qt(0.975, 30-6))\n\n[1] 2.063899\n\nb_hat &lt;- summary(lmod)$coefficients[\"Area\", \"Estimate\"]\nse_b_hat &lt;- summary(lmod)$coefficients[\"Area\", \"Std. Error\"]\n\nb_hat + c(-1, 1)*t_stat*se_b_hat\n\n[1] -0.07021580  0.02233912\n\nconfint(lmod, \"Area\")\n\n          2.5 %     97.5 %\nArea -0.0702158 0.02233912\n\n\n\n\nMultiple parameters\nIf you are interested in more than one parameter, you can construct a \\(100(1−\\alpha)\\)% confidence region for \\(\\beta\\) using: \\[\\left(\\hat{\\beta}-\\beta \\right)^TX^TX\\left(\\hat{\\beta}-\\beta \\right) \\leq p \\hat{\\sigma}^2 F^{(\\alpha)}_{p, n-p} \\] These regions are ellipsoidally shaped. Because these ellipsoids lie in higher dimensions, they cannot easily be visualized except for the two-dimensional case. Let’s see how these compare to the univariate confidence intervals. For example, we can construct the joint 95% confidence region for \\(\\beta_\\text{Area}\\) and \\(\\beta_\\text{Adjacent}\\). We have added the point of the least squares estimates which lies at the center of the ellipse and the univariate confidence intervals for both dimensions as dotted lines:\nUsing base R:\n\nplot(ellipse(lmod,c(2,6)),type=\"l\",ylim=c(-0.13,0), xlim = c(-0.09, 0.04))\npoints(coef(lmod)[2], coef(lmod)[6], pch=19)\nabline(v=confint(lmod)[2,],lty=2)\nabline(h=confint(lmod)[6,],lty=2)\n\n\n\n\nA ggplot version:\n\n# ggplot version\n\ndata.frame(ellipse(lmod,c(2,6))) %&gt;% \n  ggplot(aes(x=Area, y=Adjacent))+\n # geom_point()+\n  geom_density_2d(stat=\"identity\", color=\"black\")+\n  lims(y=c(-0.13,0), x=c(-0.09, 0.04))+\n  geom_vline(xintercept = confint(lmod)[2,],lty=2)+\n  geom_hline(yintercept = confint(lmod)[6,],lty=2)+\n  annotate(geom=\"point\", x=coef(lmod)[2], y=coef(lmod)[6])+\n  mytheme"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html",
    "href": "Notes/Inference_Diagnostics.html",
    "title": "Inference & Diagnostics",
    "section": "",
    "text": "# List of packages required:\npackages &lt;- c(\"tidyverse\", \"PNWColors\", \"janitor\", \"faraway\", \"broom\", \"DHARMa\", \"emmeans\", \"performance\", \"ellipse\")\n\n# Load packages into session\nlapply(packages, require, character.only = TRUE)\nrm(packages)\n\n# Ensure functions with duplicate names are from the correct package\nselect &lt;- dplyr::select\nmap &lt;- purrr::map\nsummarize &lt;- dplyr::summarize\nclean_names &lt;- janitor::clean_names\nmargin &lt;- ggplot2::margin\n\nset.seed(123) #Set seed for pseudo-random number generator, for reproducibility\n\nmytheme &lt;- theme_light()+ #define custom theme for ggplots\n  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, l = 0)),\n        axis.title.x = element_text(margin = margin(t = 10, l = 0)),\n        text=element_text(size=15))\n\ndata(\"gala\")"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#examples",
    "href": "Notes/Inference_Diagnostics.html#examples",
    "title": "Inference & Diagnostics",
    "section": "Examples",
    "text": "Examples\n\ndata(\"gala\")\nlmod &lt;- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala)\nnullmod &lt;- lm(Species ~ 1, gala)\nanova(lmod, nullmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n24\n89231.37\nNA\nNA\nNA\nNA\n\n\n29\n381081.37\n-5\n-291850\n15.69941\n7e-07\n\n\n\n\n\n\nWe can compute this by hand:\n\nrss0 &lt;- deviance(nullmod)\nrss &lt;- deviance(lmod)\ndf0 &lt;- df.residual(nullmod)\ndf &lt;- df.residual(lmod)\nfstat &lt;- ((rss-rss0)/(df-df0))/(rss/df)\n\n1-pf(fstat, df0-df, df)\n\n[1] 6.837893e-07\n\n\n\nTesting 1 predictor\n\nlmods &lt;- lm(Species ~ Elevation + Nearest + Scruz + Adjacent, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n25\n93469.08\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n1\n4237.718\n1.139792\n0.296318\n\n\n\n\n\nsumary(lmod)\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)  7.068221  19.154198  0.3690 0.7153508\nArea        -0.023938   0.022422 -1.0676 0.2963180\nElevation    0.319465   0.053663  5.9532 3.823e-06\nNearest      0.009144   1.054136  0.0087 0.9931506\nScruz       -0.240524   0.215402 -1.1166 0.2752082\nAdjacent    -0.074805   0.017700 -4.2262 0.0002971\n\nn = 30, p = 6, Residual SE = 60.97519, R-Squared = 0.77\n\n\n\nsumary(lm(Species ~ Area, gala))\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept) 63.782861  17.524416  3.6397 0.0010943\nArea         0.081963   0.019713  4.1578 0.0002748\n\nn = 30, p = 2, Residual SE = 91.73159, R-Squared = 0.38\n\n\n\n\nTesting a pair of predictors\n\nlmods &lt;- lm(Species ~ Elevation + Nearest + Scruz, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n26\n158291.63\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n2\n69060.26\n9.287352\n0.0010297\n\n\n\n\n\n\n\n\nTesting a subspace\n\\[H_0:\\beta_\\text{Area}= \\beta_\\text{Adjacent}\\]\n\nlmods &lt;- lm(Species ~ I(Area+Adjacent) + Elevation + Nearest + Scruz, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n25\n109591.12\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n1\n20359.75\n5.476035\n0.0279256\n\n\n\n\n\n\n\\[H_0:\\beta_\\text{Elevation}= 0.5\\]\n\nlmods &lt;- lm(Species ~ Area+ offset(0.5*Elevation) + Nearest + Scruz + Adjacent, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n25\n131312.12\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n1\n42080.76\n11.3182\n0.0025738\n\n\n\n\n\n\nA simpler way to test such point hypotheses is to use a t-statistic: \\[t = (\\hat{\\beta}−c)/\\text{se}(\\hat{\\beta})\\] where \\(c\\) is the point hypothesis. In this example, the statistic and corresponding p-value are:\n\nb_hat &lt;- unname(lmod$coefficients[\"Elevation\"])\nse_b_hat &lt;- summary(lmod)$coef[\"Elevation\", \"Std. Error\"]\n(tstat &lt;- (b_hat-0.5)/se_b_hat)\n\n[1] -3.364253\n\n2*pt(tstat, 24)\n\n[1] 0.002573836\n\ntstat^2\n\n[1] 11.3182"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#permutation-tests",
    "href": "Notes/Inference_Diagnostics.html#permutation-tests",
    "title": "Inference & Diagnostics",
    "section": "Permutation Tests",
    "text": "Permutation Tests\n\nOverall model\n\nlmod &lt;- lm(Species ~ Nearest + Scruz, gala)\nlms &lt;- summary(lmod)\nlms$fstatistic\n\n     value      numdf      dendf \n 0.6019558  2.0000000 27.0000000 \n\npf(lms$fstatistic[1],lms$fstatistic[2],lms$fstatistic[3],lower.tail=F)\n\n    value \n0.5549255 \n\n\n\nset.seed(123)\n\nnreps &lt;- 4000\nfstats &lt;- numeric(nreps)\n\nfor(i in 1:nreps){\n     lmods &lt;- lm(sample(Species) ~ Nearest+Scruz , gala)\n     fstats[i] &lt;- summary(lmods)$fstat[1]\n}\n\nmean(fstats &gt; lms$fstat[1])\n\n[1] 0.55825\n\n\n\n\nSingle predictor\n\nsummary(lmod)$coef[3,]\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n-0.4406401  0.4025312 -1.0946731  0.2833295 \n\ntstats &lt;- numeric(nreps)\n\nset.seed(123)\n\nfor(i in 1:nreps){\n  lmods &lt;- lm(Species ~ Nearest+sample(Scruz), gala)\n  tstats[i] &lt;- summary(lmods)$coef[3,3]\n}\n\nmean(abs(tstats) &gt; abs(lms$coef[3,3]))\n\n[1] 0.26825"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#confidence-intervals",
    "href": "Notes/Inference_Diagnostics.html#confidence-intervals",
    "title": "Inference & Diagnostics",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nSingle parameter\n\\[\\hat{\\beta_i}\\pm t^{(\\alpha/2)}_{n-p}\\text{se}(\\hat{\\beta}) \\]\n\nlmod &lt;- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala)\nsumary(lmod)\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)  7.068221  19.154198  0.3690 0.7153508\nArea        -0.023938   0.022422 -1.0676 0.2963180\nElevation    0.319465   0.053663  5.9532 3.823e-06\nNearest      0.009144   1.054136  0.0087 0.9931506\nScruz       -0.240524   0.215402 -1.1166 0.2752082\nAdjacent    -0.074805   0.017700 -4.2262 0.0002971\n\nn = 30, p = 6, Residual SE = 60.97519, R-Squared = 0.77\n\n\nWe can construct individual 95% CIs for \\(\\beta_\\text{Area}\\) for which we need the 2.5% and 97.5% percentiles of the t-distribution with 30−6 = 24 degrees of freedom.\n\n(t_stat&lt;-qt(0.975, 30-6))\n\n[1] 2.063899\n\nb_hat &lt;- summary(lmod)$coefficients[\"Area\", \"Estimate\"]\nse_b_hat &lt;- summary(lmod)$coefficients[\"Area\", \"Std. Error\"]\n\nb_hat + c(-1, 1)*t_stat*se_b_hat\n\n[1] -0.07021580  0.02233912\n\nconfint(lmod, \"Area\")\n\n          2.5 %     97.5 %\nArea -0.0702158 0.02233912\n\n\n\n\nMultiple parameters\nIf you are interested in more than one parameter, you can construct a \\(100(1−\\alpha)\\)% confidence region for \\(\\beta\\) using: \\[\\left(\\hat{\\beta}-\\beta \\right)^TX^TX\\left(\\hat{\\beta}-\\beta \\right) \\leq p \\hat{\\sigma}^2 F^{(\\alpha)}_{p, n-p} \\] These regions are ellipsoidally shaped. Because these ellipsoids lie in higher dimensions, they cannot easily be visualized except for the two-dimensional case. Let’s see how these compare to the univariate confidence intervals. For example, we can construct the joint 95% confidence region for \\(\\beta_\\text{Area}\\) and \\(\\beta_\\text{Adjacent}\\). We have added the point of the least squares estimates which lies at the center of the ellipse and the univariate confidence intervals for both dimensions as dotted lines:\nUsing base R:\n\nplot(ellipse(lmod,c(2,6)),type=\"l\",ylim=c(-0.13,0), xlim = c(-0.09, 0.04))\npoints(coef(lmod)[2], coef(lmod)[6], pch=19)\nabline(v=confint(lmod)[2,],lty=2)\nabline(h=confint(lmod)[6,],lty=2)\n\n\n\n\nA ggplot version:\n\n# ggplot version\n\ndata.frame(ellipse(lmod,c(2,6))) %&gt;% \n  ggplot(aes(x=Area, y=Adjacent))+\n # geom_point()+\n  geom_density_2d(stat=\"identity\", color=\"black\")+\n  lims(y=c(-0.13,0), x=c(-0.09, 0.04))+\n  geom_vline(xintercept = confint(lmod)[2,],lty=2)+\n  geom_hline(yintercept = confint(lmod)[6,],lty=2)+\n  annotate(geom=\"point\", x=coef(lmod)[2], y=coef(lmod)[6])+\n  mytheme+\n  theme(panel.grid = element_blank())\n\n\n\n\nWe can determine the outcome of various hypotheses from the plot. The joint hypothesis \\(H_0 : \\beta_\\text{Area} = \\beta_\\text{Adjacent} = 0\\) is rejected because the origin does not lie inside the ellipse. The hypothesis \\(H_0 : \\beta_\\text{Area} = 0\\) is not rejected because zero does lie within the vertical dashed lines, whereas the horizontal dashed lines do not encompass zero and so \\(H_0: \\beta_\\text{Adjacent} = 0\\) is rejected."
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#bootstrap-confidence-intervals",
    "href": "Notes/Inference_Diagnostics.html#bootstrap-confidence-intervals",
    "title": "Inference & Diagnostics",
    "section": "Bootstrap Confidence Intervals",
    "text": "Bootstrap Confidence Intervals\n\nset.seed(123)\nnb &lt;- 4000\ncoefmat &lt;- matrix(NA,nb,6)\nresids &lt;- residuals(lmod)\npreds &lt;- fitted(lmod)\n \nfor(i in 1:nb){\nboot &lt;- preds + sample(resids , rep=TRUE)\nbmod &lt;- update(lmod , boot ~ .)\ncoefmat[i,] &lt;- coef(bmod)\n}\n\ncolnames(coefmat) &lt;- c(\"Intercept\",colnames(gala[,3:7]))\ncoefmat &lt;- data.frame(coefmat)\napply(coefmat,2,function(x) quantile(x,c(0.025,0.975)))\n\n      Intercept        Area Elevation   Nearest      Scruz    Adjacent\n2.5%  -25.31406 -0.06236506 0.2310989 -1.716588 -0.6061978 -0.10545278\n97.5%  42.69309  0.01807403 0.4207570  2.122722  0.1677720 -0.03979658"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#cis-for-predictions",
    "href": "Notes/Inference_Diagnostics.html#cis-for-predictions",
    "title": "Inference & Diagnostics",
    "section": "CIs for predictions",
    "text": "CIs for predictions\nThere are two kinds of predictions made from regression models. One is a predicted mean response and the other is a prediction of a future observation. For example, suppose we have built a regression model that predicts the rental price of houses in a given area based on predictors such as the number of bedrooms and closeness to a major highway. There are two kinds of predictions that can be made for a given \\(x_0\\):\n\nSuppose a specific house comes on the market with characteristics \\(x_0\\). Its rental price will be \\(x_0^T \\beta +\\varepsilon\\). Since \\(E(\\varepsilon)=0\\), the predicted price is \\(x_0^T \\hat{\\beta}\\), but in assessing the variance of this prediction, we must include the variance of \\(\\varepsilon\\).\nSuppose we ask the question — “What would a house with characteristics \\(x_0\\) rent for on average?” This selling price is \\(x_0^T \\beta\\) and is again predicted by \\(x_0^T \\hat{\\beta}\\), but now only the variance in \\(\\hat{\\beta}\\) needs to be taken into account.\n\nMost times, we will want the first case, which is called “prediction of a future value,” while the second case, called “prediction of the mean response” is less commonly required. We have:\n\\[\\text{var}(x_0^T \\hat{\\beta})= x_0^T (X^TX)^{-1}x_0\\sigma^2\\]\nWe assume that the future \\(\\varepsilon\\) is independent of \\(\\hat{\\beta}\\). So a \\(100(1-\\alpha)\\)% CI for a single future response is\n\\[\\hat{y_0}\\pm t^{(\\alpha/2)}_{n-p}\\hat{\\sigma}\\sqrt{1+x_0^T(X^TX)^{-1}x_0}\\]\nThere is a conceptual difference here because previous confidence intervals have been for parameters. Parameters are considered to be fixed but unknown — they are not random under the Frequentist approach we are using here. However, a future observation is a random variable. For this reason, it is better to call this a prediction interval. We are saying there is a 95% chance that the future value falls within this interval whereas it would be incorrect to say that for a parameter."
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#example",
    "href": "Notes/Inference_Diagnostics.html#example",
    "title": "Inference & Diagnostics",
    "section": "Example",
    "text": "Example\n\ndata(fat,package=\"faraway\")\nlmod &lt;- lm(brozek ~ age + weight + height + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data=fat)\n\n x &lt;- model.matrix(lmod)\n(x0 &lt;- apply(x,2,median))\n\n(Intercept)         age      weight      height        neck       chest \n       1.00       43.00      176.50       70.00       38.00       99.65 \n      abdom         hip       thigh        knee       ankle      biceps \n      90.95       99.30       59.00       38.50       22.80       32.05 \n    forearm       wrist \n      28.70       18.30 \n\n(y0 &lt;- sum(x0*coef(lmod)))\n\n[1] 17.49322\n\npredict(lmod,new=data.frame(t(x0)))\n\n       1 \n17.49322 \n\npredict(lmod,new=data.frame(t(x0)),interval=\"prediction\")\n\n       fit     lwr      upr\n1 17.49322 9.61783 25.36861\n\npredict(lmod,new=data.frame(t(x0)),interval=\"confidence\")\n\n       fit      lwr      upr\n1 17.49322 16.94426 18.04219\n\n\nSkipping autoregression section b/c would use explicit ARIMA model or other method of time-series analysis where appropriate"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#checking-error-assumptions",
    "href": "Notes/Inference_Diagnostics.html#checking-error-assumptions",
    "title": "Inference & Diagnostics",
    "section": "Checking Error Assumptions",
    "text": "Checking Error Assumptions\nWe wish to check the independence, constant variance and normality of the errors, \\(\\varepsilon\\). The errors are not observable, but we can examine the residuals, \\(\\hat{\\varepsilon}\\). These are not interchangeable with the error, as they have somewhat different properties. Recall that \\(\\hat{y}=X(X^TX)^{-1}Xy=Hy\\), where \\(H\\) is the hat matrix, so that \\[\\hat{\\varepsilon}=y-\\hat{y}=(I-H)y=(I-H)X\\beta+(I-H)\\varepsilon=(I-H)\\varepsilon \\]\nTherefore, \\(\\text{var}\\hat{\\varepsilon} = \\text{var} (I-H)\\varepsilon = (I-H)\\sigma^2\\) assuming that \\(\\text{var}\\varepsilon= \\sigma^2I\\). We see that although the errors may have equal variance and be uncorrelated, the residuals do not."
  },
  {
    "objectID": "HW/hw_07_logistic_regr.html",
    "href": "HW/hw_07_logistic_regr.html",
    "title": "Background",
    "section": "",
    "text": "Background\nThis week’s homework assignment focuses on fitting and evaluating logistic regression models. The data come from a unique tagging program for salmon in the Columbia River basin. Beginning in the mid-1990s, juvenile salmon have been captured in their natal rearing habitats in Idaho, Oregon, and Washington, and implanted with a passive integrated transponder (PIT) tag. These tagged fish can then be detected at numerous locations during their downstream migration to the sea, including most of the hydroelectric dams they pass, which allows researchers to estimate their survival. Those juveniles that then mature and survive their 1-4 years in the ocean can also be detected as they swim upstream towards their spawning grounds.\nMany of these salmon belong to populations that are listed as threatened or endangered under the Endangered Species Act. As such, there is great interest in trying to understand how hydropower operations affect the survival of both juveniles and adults. In particular, the estimated smolt-to-adult returns (SARs) has been a focus due to the perceived delayed effects of the juveniles’ downstream journey on their subsequent survival (so-called “delayed mortality”). Furthermore, previous work by Scheuerell et al. (2009) showed nonlinear relationships between SARs and migration timing within a year, indicating a possible window of opportunity for some fish and a mismatch with the environment for those migrating relatively early or late in the season.\nYour assignment is to investigate how daily estimates of SARs for Chinook salmon from the Snake River basin vary across a portion of their migration season for one year, and explore whether there is any relationship between SARs and water temperature. The accompanying data file srss_chin_sar.csv contains information about tagged salmon detected at Bonneville dam (BON), the last dam juveniles pass as they head to sea and the first dam adults encounter upon their return. Here are descriptions of the fields of information.\n\nday: the day of the month in May (1-31)\nsmolts: the number of tagged juveniles detected at BON on a given day\nadults: the number of surviving adults subsequently detected at BON\ntemp: the water temperature recorded at BON on that day\n\n\n\nQuestions\n\nIdentify the three components of a GLM that you will need to fit a logistic regression model for survival given these data.\nPlot daily estimates of survival against day and temp and describe any patterns you see.\nWould it be reasonable to include both day and temp as predictors in the same model? Why or why not?\nFit a logistic regression model with survival as a function of only an intercept and compute the \\(R^2\\) value. Based upon this model, what is the estimated mean survival for the month of May? Plot the model residuals against day and describe any possible problems with this model.\nFit a logistic regression model with survival as a function of day and day^2 and compute the \\(R^2\\) value. Plot the model residuals against day and describe any possible problems with this model.\nFit a logistic regression model with survival as a function of temp and temp^2 and compute the \\(R^2\\) value. Plot the model residuals against day and describe any possible problems with this model.\nFit a standard linear regression model to temp as a function of day and extract the residuals from this model. These residuals give an indication of whether a particular day of May was warmer or colder than average. Plot these residuals against survival and describe any patterns you see.\nFit a logistic regression model with survival as a function of the residuals from (g) and compute the \\(R^2\\) value. Plot the model residuals against day and describe any possible problems with this model.\nCreate a table showing the \\(\\Delta\\)AIC values and Akaike weights for each of the four logistic regression models you fit above. Which model has the greatest support from the data? How do the other models compare to it?\nBased on the results from (i), compute the model-averaged prediction of survival across all four models. Plot survival versus day and overlay your model-averaged prediction. Does this seem like good model overall?\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "HW/hw_08_count_models.html",
    "href": "HW/hw_08_count_models.html",
    "title": "Background",
    "section": "",
    "text": "Background\nThis week’s homework assignment focuses on fitting and evaluating models for count data. One of your colleagues is interested in the theory of island biogeography and has acquired a data set with which to examine how species richness varies with the area of an island, the island’s elevation, the distance to the nearest island, and the area of the nearest island. In particular, her expectation is that the number of plant species should increase with island area, and a plot of the data suggests this to indeed be the case, but her initial modeling effort has yielded the opposite result. Recognizing that she does not have much experience with this type of data analysis, she has turned to you for assistance.\nHer data are contained in the accompanying file plant_richness.csv, which has the following columns of information:\n\nisland: name of the island\nspecies: number of plant species on the island\narea the area of the island (km\\(^2\\))\nelevation: the highest elevation of the island (m)\ndistance: the distance to the nearest island (km)\nadjacent the area of the nearest island (km\\(^2\\))\n\n\n\nQuestions\n\nPlot the number of species versus island area and describe any patterns you observe. Does your colleague’s assumption of a positive relationship between richness and area seem to hold?\nYour colleague explains that she fit the following model, which yielded the surprising result. Fit the model for yourself and verify if there is indeed a negative effect of area on species. Do the signs of the other coefficients seem to make sense from an ecological perspective? Why or why not?\n\n\\[\n\\text{species}_i = \\beta_0 + \\beta_1 \\text{area}_i + \\beta_2 \\text{elevation}_i + \\beta_3 \\text{nearest}_i + \\beta_4 \\text{adjacent}_i + e_i\n\\]\n\nOffer one explanation for the unexpected effect of area given the apparent relationship in (a). Based on this evaluation, offer a possible suggestion for estimating the effect of area on species.\nDoes it seem reasonable to use species as a response variable in a linear model like the one your colleague fit initially? Why or why not? What would be a more appropriate response variable in a linear model like this?\nBased upon your knowledge of models for count data, offer a simple alternative regression model that models species as a function of area, nearest, and adjacent. What are the important components to this model?\nFit the model you recommended in (e) and examine the summary information. Does the effect of area seem more reasonable in this model? Do you see any problems with this model?\nBased on your assessment of the model in (f), identify three possible alternatives for estimating the model parameters and their associated uncertainty, and show how you would do so in R. How do the these alternative models compare the the estimates in (f).\nFor one of your alternatives in (g), evaluate whether a model that includes only area as a predictor is better than a model with all three predictors. Show the R code necessary to estimate the model and any test(s) or comparison(s) you might use.\nEvaluate the diagnostics for your model from (h) with species as a function of area only. Do you see any problems with this model?\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "HW/hw_04_pvalues_revisited.html",
    "href": "HW/hw_04_pvalues_revisited.html",
    "title": "Background",
    "section": "",
    "text": "Background\nThere have been several recent critiques of the use and misuse of null hypothesis testing in the sciences. Many authors have pointed to problems in setting up the tests (eg, weak or nonsensical null hypotheses), interpretation of the results (eg, correct understanding of a confidence interval), and so-called “\\(p\\)-hacking”, where researchers collect or select data, or conduct many different statistical analyses, until non-significant results become significant, and those are the only results that are ultimately reported. So far in class we have seen a variety of different null hypothesis tests that we can use to evaluate the evidence for or against the inclusion of parameters in a model, for checking the homoscedasticity of residuals, and looking for autocorrelation in data or residuals. As we move forward, however, we will turn our attention to other methods for evaluating evidence, selecting a “best” model from a set, and even averaging the results of multiple models.\n\n\nAssignment\nYour assignment this week is to read the paper by Wasserstein et al. (2019) titled “Moving to a world beyond ‘p &lt; 0.05’” and provide a summary of things we should and should not do with respect to statistical analysis in a frequentist framework. In particular, think about how the topics in the paper fit into your research and experiences you have had so far. Your comment should be 500-700 words.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "HW/hw_06_mixed_models.html",
    "href": "HW/hw_06_mixed_models.html",
    "title": "Background",
    "section": "",
    "text": "This week’s homework assignment focuses on fitting and evaluating linear mixed models. In particular, you will consider different forms for a stock-recruit relationship that describes the density-dependent relationship between fish spawning biomass in “brood year” \\(t\\) \\((S_t)\\) and the biomass of fish arising from that brood year that subsequently “recruit” to the fishery \\((R_t)\\).\n\n\nThe Ricker model (Ricker 1954) is one of the classical forms for describing the stock-recruit relationship. The deterministic form of the model is given by\n\\[\nR_t = S_t \\exp \\left[ r \\left( 1 - \\frac{S_t}{k} \\right) \\right]\n\\]\nwhere \\(r\\) is the intrinsic growth rate and \\(k\\) is the carrying capacity of the environment. In fisheries science, the model is often rewritten as\n\\[\nR_t = a S_t \\exp \\left( -b S_t \\right)\n\\]\nwhere \\(a = \\exp r\\) and \\(b = r / k\\). We can make the model stochastic by including a multiplicative error term \\(\\epsilon_t \\sim \\text{N}(0, \\sigma^2)\\), such that\n\\[\nR_t = a S_t \\exp \\left( -b S_t \\right) \\exp(\\epsilon_t)\n\\]\nThis model is clearly non-linear, but we can use a log-transform to linearize it. Specifically, we have\n\\[\n\\begin{aligned}\n\\log R_t &= \\log a + \\log S_t - b S_t + \\epsilon_t \\\\\n&\\Downarrow \\\\\n\\log R_t - \\log S_t &= \\log a - b S_t + \\epsilon_t \\\\\n&\\Downarrow \\\\\n\\log (R_t / S_t) &= \\log a - b S_t + \\epsilon_t \\\\\n&\\Downarrow \\\\\ny_t &= \\alpha - \\beta S_t + \\epsilon_t\n\\end{aligned}\n\\]\nwhere \\(y_t =\\log (R_t / S_t)\\), \\(\\alpha = \\log a\\), and \\(\\beta = b\\).\n\n\n\nThe data for this assignment come from 21 populations of Chinook salmon (Oncorhynchus tshawytscha) in Puget Sound. The original data come from the NOAA Fisheries Salmon Population Summary (SPS) database, which was subsequently cleaned and summarized for use in a recent paper by Bal et al. (2019). The data are contained in the accompanying file ps_chinook.csv, which contains the following columns:\n\npop: name of the population\npop_n: integer ID for population (1-21)\nyear: year of spawning\nspawners: total number of spawning adults (1000s)\nrecruits: total number of surviving offspring that “recruit” to the fishery (1000s)"
  },
  {
    "objectID": "HW/hw_06_mixed_models.html#ricker-model",
    "href": "HW/hw_06_mixed_models.html#ricker-model",
    "title": "Background",
    "section": "",
    "text": "The Ricker model (Ricker 1954) is one of the classical forms for describing the stock-recruit relationship. The deterministic form of the model is given by\n\\[\nR_t = S_t \\exp \\left[ r \\left( 1 - \\frac{S_t}{k} \\right) \\right]\n\\]\nwhere \\(r\\) is the intrinsic growth rate and \\(k\\) is the carrying capacity of the environment. In fisheries science, the model is often rewritten as\n\\[\nR_t = a S_t \\exp \\left( -b S_t \\right)\n\\]\nwhere \\(a = \\exp r\\) and \\(b = r / k\\). We can make the model stochastic by including a multiplicative error term \\(\\epsilon_t \\sim \\text{N}(0, \\sigma^2)\\), such that\n\\[\nR_t = a S_t \\exp \\left( -b S_t \\right) \\exp(\\epsilon_t)\n\\]\nThis model is clearly non-linear, but we can use a log-transform to linearize it. Specifically, we have\n\\[\n\\begin{aligned}\n\\log R_t &= \\log a + \\log S_t - b S_t + \\epsilon_t \\\\\n&\\Downarrow \\\\\n\\log R_t - \\log S_t &= \\log a - b S_t + \\epsilon_t \\\\\n&\\Downarrow \\\\\n\\log (R_t / S_t) &= \\log a - b S_t + \\epsilon_t \\\\\n&\\Downarrow \\\\\ny_t &= \\alpha - \\beta S_t + \\epsilon_t\n\\end{aligned}\n\\]\nwhere \\(y_t =\\log (R_t / S_t)\\), \\(\\alpha = \\log a\\), and \\(\\beta = b\\)."
  },
  {
    "objectID": "HW/hw_06_mixed_models.html#data",
    "href": "HW/hw_06_mixed_models.html#data",
    "title": "Background",
    "section": "",
    "text": "The data for this assignment come from 21 populations of Chinook salmon (Oncorhynchus tshawytscha) in Puget Sound. The original data come from the NOAA Fisheries Salmon Population Summary (SPS) database, which was subsequently cleaned and summarized for use in a recent paper by Bal et al. (2019). The data are contained in the accompanying file ps_chinook.csv, which contains the following columns:\n\npop: name of the population\npop_n: integer ID for population (1-21)\nyear: year of spawning\nspawners: total number of spawning adults (1000s)\nrecruits: total number of surviving offspring that “recruit” to the fishery (1000s)"
  },
  {
    "objectID": "HW/hw_05_model_selection.html",
    "href": "HW/hw_05_model_selection.html",
    "title": "Background",
    "section": "",
    "text": "Background\nThis week’s home work will require you to use all of the information you have learned so far in class. Your task is to analyze some data on the concentration of nitrogen in the soil at 41 locations on the island of Maui in the Hawaiian Archipelago. Along with the nitrogen measurements, there are 4 possible predictor variables that may help to explain the variation in soil nitrogen concentration. The accompanying data file soil_nitrogen.csv has the following 5 columns of data:\n\nnitrogen: concentration of soil nitrogen (mg nitrogen kg\\(^{-1}\\) soil)\ntemp: average air temperature (\\(^\\circ\\)C)\nprecip: average precipitation (cm)\nslope: slope of the hillside (degrees)\naspect: aspect of the hillside (N, S)\n\nAs you work through the following problems, be sure to show all of the code necessary to produce your answers.\n\n\nProblems\n\nBegin by building a global model that contains all four of the predictors plus an intercept. Show the resulting ANOVA table, and report the multiple and adjusted \\(R^3\\) values. Also report the estimate of the residual variance \\(\\hat{\\sigma}^2\\).\nCheck the residuals from your full model for possible violations of the assumption that the \\(e_i \\sim \\text{N}(0, \\sigma^2)\\).\nDoes this seem like a reasonable model for these data? Why or why not?\nNow fit various models using all possible combinations of the 4 predictors, including an intercept-only model (ie, there should be a total of 16 models). Compute the AIC, AICc, and BIC for each of your models and compare the relative rankings of the different models.\nConduct a leave-one-out cross-validation for all of the models in part (d), using the root mean squared prediction error (RMSPE) as your scale-dependent measure of fit. Report your results alongside your results from part (d). Do all of the methods agree on which of these models is the best?\nGiven some uncertainty that one of these models is the true data-generating model, compute the weights of evidence for each of the models in your set. Which model has the greatest support from the data? What are the odds against the intercept-only model compared to the best model?\nCalculate the model-averaged parameters across all models in your set. Use these parameters to predict what the soil nitrogen concentration would be on the nearby island of Moloka’i if the average precipitation was 150 cm, the average temperature was 22 \\(^\\circ\\)C, and the hillside faced south with a slope of 11 degrees.\nCompare your prediction from part (g) to a prediction from the model identified as the best in part (e), using the same inputs. How much do they differ from one another?\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#constant-variance",
    "href": "Notes/Inference_Diagnostics.html#constant-variance",
    "title": "Inference & Diagnostics",
    "section": "Constant Variance",
    "text": "Constant Variance\nIs the variance in the residuals related to some other quantity?\nThe most useful diagnostic is a plot of \\(\\hat{\\varepsilon}\\) against \\(\\hat{y}\\). If all is well, you should see constant symmetrical variation (homoscedasticity) in the vertical (\\(\\hat{\\varepsilon}\\)) direction. Nonlinearity in the structural part of the model can also be detected in this plot.\n\n6.1.2 - Normality\n\nlmod &lt;- lm(sr ~ pop15+pop75+dpi+ddpi,savings)\nqqnorm(residuals(lmod),ylab=\"Residuals\",main=\"\")\nqqline(residuals(lmod))\n\n\n\nhist(residuals(lmod),xlab=\"Residuals\",main=\"\")\n\n\n\nshapiro.test(residuals(lmod))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(lmod)\nW = 0.98698, p-value = 0.8524"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#unusual-observations",
    "href": "Notes/Inference_Diagnostics.html#unusual-observations",
    "title": "Inference & Diagnostics",
    "section": "6.2 - Unusual Observations",
    "text": "6.2 - Unusual Observations\n\nOutliers: observations that do not fit the model well\nInfluential observations: observations that change the fit of the model in a substantive manner\nA leverage point is extreme in the predictor space. It has the potential to influence the fit, but does not necessarily do so.\n\n\n6.2.1 - Leverages\nLeverages \\(h_i\\) are useful diagnostics that depend only on \\(X\\) and not \\(y\\).\nWikipedia definition that I found helpful\nThe leverage score for the \\({i}^{th}\\) observation \\(\\mathbf{x}_i\\) is given as \\[h_{ii}=[H] _{ii}=x_i^\\top (X^\\top X)^{-1} x_i,\\] the \\({i}^{th}\\) diagonal element of the hat (projection) matrix, \\(H=X (X^\\top X)^{-1} X^\\top\\)\nThus the \\({i}^{th}\\) leverage score can be viewed as the ‘weighted’ distance between \\(x_i\\) to the mean of \\(x_i\\)’s. Leverage is closely related to the squared Mahalanobis distance: \\[D^2(x_i)=(x_i-\\bar{x})^\\top S^{-1}(x_i-\\bar{x})\\] where \\(S=X^\\top X\\) is the estimated covariance matrix of \\(x_i\\)’s.\nIt can also be interpreted as the degree by which the \\({i}^{th}\\) measured (dependent) value (i.e., \\(y_i\\) influences the \\({i}^{th}\\) fitted (predicted) value (i.e., \\(\\hat{y_i}\\): mathematically,\n\\[h_{ii}=\\frac{\\partial\\hat{y_i}}{\\partial y_i} \\]\nProperties of $h_{ii} - \\(0 \\leq h_{ii} \\leq 1\\) - Fixed \\(i: \\sum^n_{j=1} h_{ij}=1\\) - \\(\\bar{h}=\\sum^n_{i=1}h_{ii}/n=p/n\\) - Fixed \\(i: h_{ii}=\\sum^n_{j=1} h_{ij}^2\\)\n\\[\\text{Var}(\\hat{e}_i)=\\sigma^2(1-h_i) \\]\nLarge \\(h_i\\) lead to small variances of \\(e_i\\) and hence \\(\\hat{y_i}\\) tends to \\(y_i\\).\nTraditionally, a leverage point is one that satisfies \\(h_{ii} \\geq 2\\bar{h} = \\frac{2p}{n}\\)\n\nlmod &lt;- lm(sr ~ pop15 + pop75 + dpi + ddpi, savings)\nhatv &lt;- hatvalues(lmod)\nhead(hatv)\n\n Australia    Austria    Belgium    Bolivia     Brazil     Canada \n0.06771343 0.12038393 0.08748248 0.08947114 0.06955944 0.15840239 \n\nsum(hatv)\n\n[1] 5\n\n## threshold value for h_i ~= 0.2\nthv &lt;- 2 * (length(coefficients(lmod)) / length(hatv))\n\n## are any h_i &gt; Eh?\nhatv &gt; thv\n\n     Australia        Austria        Belgium        Bolivia         Brazil \n         FALSE          FALSE          FALSE          FALSE          FALSE \n        Canada          Chile          China       Colombia     Costa Rica \n         FALSE          FALSE          FALSE          FALSE          FALSE \n       Denmark        Ecuador        Finland         France        Germany \n         FALSE          FALSE          FALSE          FALSE          FALSE \n        Greece      Guatamala       Honduras        Iceland          India \n         FALSE          FALSE          FALSE          FALSE          FALSE \n       Ireland          Italy          Japan          Korea     Luxembourg \n          TRUE          FALSE           TRUE          FALSE          FALSE \n         Malta         Norway    Netherlands    New Zealand      Nicaragua \n         FALSE          FALSE          FALSE          FALSE          FALSE \n        Panama       Paraguay           Peru    Philippines       Portugal \n         FALSE          FALSE          FALSE          FALSE          FALSE \n  South Africa South Rhodesia          Spain         Sweden    Switzerland \n         FALSE          FALSE          FALSE          FALSE          FALSE \n        Turkey        Tunisia United Kingdom  United States      Venezuela \n         FALSE          FALSE          FALSE           TRUE          FALSE \n        Zambia        Jamaica        Uruguay          Libya       Malaysia \n         FALSE          FALSE          FALSE           TRUE          FALSE \n\nhatv[hatv&gt;thv]\n\n      Ireland         Japan United States         Libya \n    0.2122363     0.2233099     0.3336880     0.5314568 \n\n\nWe verify that the sum of the leverages is indeed 5 — the number of parameters in the model.\nWe can also identify high leverages using half-normal plots, where the data are plotted against positive normal quantiles. We do not usually expect a straight line relationship since we do not necessarily expect a positive normal distribution for quantities like leverages. We are looking for outliers, which will be apparent as points that diverge substantially from the rest of the data\n\ncountries &lt;- row.names(savings)\nhalfnorm(hatv,labs=countries,ylab=\"Leverages\")"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#standardized-residuals",
    "href": "Notes/Inference_Diagnostics.html#standardized-residuals",
    "title": "Inference & Diagnostics",
    "section": "Standardized residuals",
    "text": "Standardized residuals\nWe can use the leverages to scale the residuals so their variance is 1 \\[r_i=\\frac{\\hat{e_i}}{\\sigma \\sqrt{1-h_i}} \\]\nDoing so allows for easy examination via \\(Q-Q\\) plots, as values should lie on the 1:1 line."
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#studentized-residuals",
    "href": "Notes/Inference_Diagnostics.html#studentized-residuals",
    "title": "Inference & Diagnostics",
    "section": "Studentized residuals",
    "text": "Studentized residuals\nOne way to detect outliers is to estimate \\(n\\) different models where we exclude one data point from each model. \\[\\hat{y}_{(i)}=X_{(i)}\\hat{\\beta}_{(i)}\\] where \\((i)\\) indicates that the \\(i^{th}\\) datum has been omitted. If \\(y_i-\\hat{y}_{(i)}\\) is large, then observation \\(i\\) is an outlier. To judge the size of a potential outlier, we need an appropriate scaling. We find \\[\\widehat{\\text{var}}(y_i-\\hat{y}_{(i)})=\\hat{\\sigma}_{(i)}^2 (1+X_{i} (X_{(i)}^\\top X_{(i)})^{-1}X_i) \\]\nSo we can compute the studentized residuals as \\[t_i = \\frac{y_i-\\hat{y}_{(i)}}{\\hat{\\sigma}_{(i)}\\sqrt{1+X_{i} (X_{(i)}^\\top X_{(i)})^{-1}X_i}} \\]\nwhich are distributed as a \\(t\\)-distribution with \\(n-p-1\\) df. Fortunately, there is an easier way to compute \\(t_i\\) that avoids doing \\(n\\) regressions: \\[t_i=\\frac{\\hat{e_i}}{\\hat{\\sigma}_{(i)} \\sqrt{1-h_i}}= r_i \\left(\\frac{n-p-1}{n-p-r_i^2} \\right)^{1/2} \\]\n\nstud &lt;- rstudent(lmod)\nstud[which.max(abs(stud))]\n\n  Zambia \n2.853558 \n\ndf &lt;- df.residual(lmod)-1\n\nqt(p=.05/(length(countries)*2), df=df) #find test statistic, which is greater than the absolute value of the studentized residual\n\n[1] -3.525801\n\n2*pt((unname(-stud[which.max(abs(stud))])), df=df, lower.tail = TRUE) #p-value\n\n[1] 0.006566663\n\n2*pt((unname(-stud[which.max(abs(stud))])), df=df, lower.tail = TRUE)*nrow(savings) #corrected\n\n[1] 0.3283332\n\ncar::outlierTest(lmod) #easier\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n       rstudent unadjusted p-value Bonferroni p\nZambia 2.853558          0.0065667      0.32833"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#influential-observations",
    "href": "Notes/Inference_Diagnostics.html#influential-observations",
    "title": "Inference & Diagnostics",
    "section": "6.2.3 Influential Observations",
    "text": "6.2.3 Influential Observations\nAn influential point is one whose removal from the dataset would cause a large change in the fit. An influential point may or may not be an outlier and may or may not have large leverage, but it will tend to have at least one of these two properties.\nCook’s D: \\[D_i=\\frac{e_i^2}{p} \\left(\\frac{h_i}{1-h_i} \\right) \\]\nThis metric combines the residual effect and leverage to quantify the influence. A half-normal plot of \\(D_i\\) can be used to identify influential observations.\n\nlmod &lt;- lm(sr ~ pop15+pop75+dpi+ddpi,savings)\ncook &lt;- cooks.distance(lmod)\nhalfnorm(cook,3,labs=countries,ylab=\"Cook’s distances\")"
  },
  {
    "objectID": "Notes/model_errors.html",
    "href": "Notes/model_errors.html",
    "title": "Problems with Model Errors",
    "section": "",
    "text": "# List of packages required:\npackages &lt;- c(\"tidyverse\", \"PNWColors\", \"janitor\", \"faraway\", \"broom\", \"DHARMa\", \"emmeans\", \"performance\", \"nlme\")\n\n# Load packages into session\nlapply(packages, require, character.only = TRUE)\nrm(packages)\n\n# Ensure functions with duplicate names are from the correct package\nselect &lt;- dplyr::select\nmap &lt;- purrr::map\nsummarize &lt;- dplyr::summarize\nclean_names &lt;- janitor::clean_names\nmargin &lt;- ggplot2::margin\n\nset.seed(123) #Set seed for pseudo-random number generator, for reproducibility\n\nmytheme &lt;- theme_light()+ #define custom theme for ggplots\n  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, l = 0)),\n        axis.title.x = element_text(margin = margin(t = 10, l = 0)),\n        text=element_text(size=15))"
  },
  {
    "objectID": "Notes/model_errors.html#generalized-least-squares",
    "href": "Notes/model_errors.html#generalized-least-squares",
    "title": "Problems with Model Errors",
    "section": "8.1 Generalized Least Squares",
    "text": "8.1 Generalized Least Squares\nWe have previously assumed \\(\\text{var}(e)=\\sigma^2 I\\), but suppose that the errors have non-constant variance, \\(\\text{var}(e)=\\sigma^2 \\Sigma\\), where \\(\\sigma^2\\) is unknown and \\(\\Sigma\\) is known: we know the correlation and relative variance between the errors, but not the absolute scale of the variation.\nWe can write \\(\\Sigma = SS^\\top\\) , where S is a triangular matrix using the Choleski decomposition, which be can be viewed as a square root for a matrix. We can transform the regression model as follows: \\[\\begin{align*}\ny &= X\\beta +e\nS^{-1}y &=S^{-1}X \\beta+S^{-1}e\ny' &= X'\\beta+e'\n\\end{align*}\\] Then, \\(\\text{var}(e')=\\text{var}(S^{-1}e)=S^{-1}(\\text{var }e)S^{-\\top}=S^{-1}\\sigma^2 SS^\\top S^{-\\top}=\\sigma^2 I\\)\nSo we can reduce GLS to OLS by a regression of \\(y′ = S^{-1}y\\) on \\(X′ = S^{-1}X\\) which has error \\(e'=S^{-1}e\\) that is i.i.d. We have transformed the problem to the standard case.\nErrors between successive observations \\(\\rightarrow\\) autoregressive model, \\(e_{i+1}=\\phi e_i+\\delta_i\\) where \\(\\delta \\sim N(0, \\tau^2)\\)\n\nlmod &lt;- lm(nhtemp ~ wusa + jasper + westgreen + chesapeake + tornetrask + urals + mongolia + tasman, globwarm)\n\ncor(residuals(lmod)[-1],residuals(lmod)[-length(residuals(lmod))])\n\n[1] 0.583339\n\nlmtest::dwtest(lmod)\n\n\n    Durbin-Watson test\n\ndata:  lmod\nDW = 0.81661, p-value = 1.402e-15\nalternative hypothesis: true autocorrelation is greater than 0\n\ncar::durbinWatsonTest(lmod) #tidy(car::durbinWatsonTest(lmod))\n\n lag Autocorrelation D-W Statistic p-value\n   1       0.5710535     0.8166064       0\n Alternative hypothesis: rho != 0\n\nperformance::check_autocorrelation(lmod)\n\nWarning: Autocorrelated residuals detected (p &lt; .001).\n\nglmod &lt;- gls(nhtemp ~ wusa + jasper + westgreen + chesapeake + tornetrask + urals + mongolia + tasman, correlation=corAR1(form= ~year), data=na.omit(globwarm))\n\nsummary(glmod)\n\nGeneralized least squares fit by REML\n  Model: nhtemp ~ wusa + jasper + westgreen + chesapeake + tornetrask +      urals + mongolia + tasman \n  Data: na.omit(globwarm) \n        AIC       BIC   logLik\n  -108.2074 -76.16822 65.10371\n\nCorrelation Structure: AR(1)\n Formula: ~year \n Parameter estimate(s):\n      Phi \n0.7109922 \n\nCoefficients:\n                  Value  Std.Error   t-value p-value\n(Intercept) -0.23010624 0.06702406 -3.433188  0.0008\nwusa         0.06673819 0.09877211  0.675678  0.5004\njasper      -0.20244335 0.18802773 -1.076668  0.2835\nwestgreen   -0.00440299 0.08985321 -0.049002  0.9610\nchesapeake  -0.00735289 0.07349791 -0.100042  0.9205\ntornetrask   0.03835169 0.09482515  0.404446  0.6865\nurals        0.24142199 0.22871028  1.055580  0.2930\nmongolia     0.05694978 0.10489786  0.542907  0.5881\ntasman       0.12034918 0.07456983  1.613913  0.1089\n\n Correlation: \n           (Intr) wusa   jasper wstgrn chespk trntrs urals  mongol\nwusa       -0.517                                                 \njasper     -0.058 -0.299                                          \nwestgreen   0.330 -0.533  0.121                                   \nchesapeake  0.090 -0.314  0.230  0.147                            \ntornetrask -0.430  0.499 -0.197 -0.328 -0.441                     \nurals      -0.110 -0.142 -0.265  0.075 -0.064 -0.346              \nmongolia    0.459 -0.437 -0.205  0.217  0.449 -0.343 -0.371       \ntasman      0.037 -0.322  0.065  0.134  0.116 -0.434  0.416 -0.017\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-2.31122523 -0.53484054  0.02342908  0.50015642  2.97224724 \n\nResidual standard error: 0.204572 \nDegrees of freedom: 145 total; 136 residual\n\nintervals(glmod,which=\"var-cov\")\n\nApproximate 95% confidence intervals\n\n Correlation structure:\n       lower      est.     upper\nPhi 0.509981 0.7109922 0.8383726\n\n Residual standard error:\n    lower      est.     upper \n0.1540719 0.2045720 0.2716246 \n\n\nAlso arises in spatial data & blocked designs, for example:\n\nglmod &lt;- gls(yield ~ variety, oatvar, correlation = corCompSymm(form\n= ~1 | block))\n\nintervals(glmod)\n\nApproximate 95% confidence intervals\n\n Coefficients:\n                 lower  est.       upper\n(Intercept) 291.542999 334.4 377.2570009\nvariety2     -4.903898  42.2  89.3038984\nvariety3    -18.903898  28.2  75.3038984\nvariety4    -94.703898 -47.6  -0.4961016\nvariety5     57.896102 105.0 152.1038984\nvariety6    -50.903898  -3.8  43.3038984\nvariety7    -63.103898 -16.0  31.1038984\nvariety8      2.696102  49.8  96.9038984\n\n Correlation structure:\n         lower      est.     upper\nRho 0.05100649 0.3959955 0.7665766\n\n Residual standard error:\n   lower     est.    upper \n33.02434 47.04679 67.02329 \n\n\nThere is evidence of a non-zero correlation of ~0.4 between the errors within the blocks."
  },
  {
    "objectID": "Notes/model_errors.html#weighted-least-squares-wls",
    "href": "Notes/model_errors.html#weighted-least-squares-wls",
    "title": "Problems with Model Errors",
    "section": "8.2 Weighted Least Squares (WLS)",
    "text": "8.2 Weighted Least Squares (WLS)\n\nA special case of GLS\nErrors are uncorrelated, but have unequal variance where the form of the inequality is known.\n\\(\\Sigma\\) is diagonal but the entries are not equal\nSet \\(\\Sigma = \\text{diag}(1/w_1,\\ldots,1/w_n)\\), where the \\(w_i\\) are the weights so \\(S=\\text{diag}(\\sqrt{1/w_1},\\ldots,\\sqrt{1/w_n})\\)\nThen regress \\(\\sqrt{w_i}y_i\\) on \\(\\sqrt{w_i}x_i\\) (although the column of ones in the X-matrix needs to be replaced with \\(\\sqrt{w_i}\\))\nResiduals are modified to use \\(\\sqrt{w_i}\\hat{e}_i\\)\nCases with low variability get a high weight and those with high variability a low weight.\n\n\nExamples\n\nErrors proportional to a predictor: \\(\\text{var}(e) \\proportional CHECK x_i\\) suggests \\(w_i = x^{−1}_i\\). One might choose this option after observing a positive relationship in a plot of \\(\\lvert \\hat{e}_i \\rvert\\) against \\(x_i\\).\nWhen the \\(Y_i\\) are the averages of \\(n_i\\) observations, then \\(\\text{var}(y_i)= \\text{var}(e_i)= \\sigma^2/n_i\\), which suggests \\(w_i=n_i\\)\nWhen the observed responses are known to be of varying quality, weights may be assigned \\(w_i = 1/\\text{sd}(y_i)\\)."
  },
  {
    "objectID": "Notes/model_errors.html#robust-regression",
    "href": "Notes/model_errors.html#robust-regression",
    "title": "Problems with Model Errors",
    "section": "8.4 Robust Regression",
    "text": "8.4 Robust Regression\nWhen the errors are normally distributed, OLS is best, but long-tailed error distributions can cause difficulties because a few extreme cases can have a large effect on the fitted model. Robust regression is designed to estimate the mean relationship between the predictors and response, \\(EY = X\\beta\\).\n\nM-Estimation\nM-estimates modify the least squares idea to choose \\(\\beta\\) to minimize: \\[\\sum_{i=1}^n \\rho(y_i - x_i^\\top \\beta) \\]\nPossible choices for \\(\\rho\\) include: 1. \\(\\rho(x)=x^2\\) is just OLS 2. \\(\\rho(x)=\\lvert x \\rvert\\) is called least absolute deviation (LAD) regression or \\(L_1\\) regression. 3. \\[\\begin{equation*}\n\\rho(x)= \\begin{cases}\nx^2/2 & if \\lvert x \\rvert \\leq c \\\\\nc\\lvert x \\rvert \\leq -c^2/2 \\text{ otherwise}\n\\end{cases}\n\\end{equation*}\\]"
  },
  {
    "objectID": "Notes/lab1.html",
    "href": "Notes/lab1.html",
    "title": "Lecture & Lab 1: Fitting Linear Models",
    "section": "",
    "text": "Faraway, Ch. 1 & 2\nMatrix math cheat sheet\n\ndata(\"gala\")\nhead(gala) # or glimpse(gala)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nEndemics\nArea\nElevation\nNearest\nScruz\nAdjacent\n\n\n\n\nBaltra\n58\n23\n25.09\n346\n0.6\n0.6\n1.84\n\n\nBartolome\n31\n21\n1.24\n109\n0.6\n26.3\n572.33\n\n\nCaldwell\n3\n3\n0.21\n114\n2.8\n58.7\n0.78\n\n\nChampion\n25\n9\n0.10\n46\n1.9\n47.4\n0.18\n\n\nCoamano\n2\n1\n0.05\n77\n1.9\n1.9\n903.82\n\n\nDaphne.Major\n18\n11\n0.34\n119\n8.0\n8.0\n1.84\n\n\n\n\n\ngala &lt;- gala %&gt;% select(-Endemics) %&gt;% clean_names()\n\nlmod &lt;- lm(species ~ area + elevation + nearest + scruz + adjacent, data = gala)\n\nsummary(lmod) # or faraway::sumary(lmod) or broom::tidy(lmod)\n\n\nCall:\nlm(formula = species ~ area + elevation + nearest + scruz + adjacent, \n    data = gala)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-111.679  -34.898   -7.862   33.460  182.584 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.068221  19.154198   0.369 0.715351    \narea        -0.023938   0.022422  -1.068 0.296318    \nelevation    0.319465   0.053663   5.953 3.82e-06 ***\nnearest      0.009144   1.054136   0.009 0.993151    \nscruz       -0.240524   0.215402  -1.117 0.275208    \nadjacent    -0.074805   0.017700  -4.226 0.000297 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.98 on 24 degrees of freedom\nMultiple R-squared:  0.7658,    Adjusted R-squared:  0.7171 \nF-statistic:  15.7 on 5 and 24 DF,  p-value: 6.838e-07\n\nx &lt;- model.matrix(~ area + elevation + nearest + scruz + adjacent, data = gala)\ny &lt;- gala$species\n  \nxtxi &lt;- solve(t(x) %*% x)\nxtxi %*% t(x) %*% y\n\n                    [,1]\n(Intercept)  7.068220709\narea        -0.023938338\nelevation    0.319464761\nnearest      0.009143961\nscruz       -0.240524230\nadjacent    -0.074804832\n\nsolve(crossprod(x,x),crossprod(x,y))\n\n                    [,1]\n(Intercept)  7.068220709\narea        -0.023938338\nelevation    0.319464761\nnearest      0.009143961\nscruz       -0.240524230\nadjacent    -0.074804832\n\ndeviance(lmod) #RSS\n\n[1] 89231.37\n\nsqrt(deviance(lmod)/df.residual(lmod)) #sigma\n\n[1] 60.97519\n\nsigma &lt;- summary(lmod)$sigma\n\nxtxi &lt;- summary(lmod)$cov.unscaled\n\n#standard errors of the coefficients\nsqrt(diag(xtxi))*sigma #OR\n\n(Intercept)        area   elevation     nearest       scruz    adjacent \n19.15419782  0.02242235  0.05366280  1.05413595  0.21540225  0.01770019 \n\nsummary(lmod)$coef[,2]\n\n(Intercept)        area   elevation     nearest       scruz    adjacent \n19.15419782  0.02242235  0.05366280  1.05413595  0.21540225  0.01770019 \n\n\n\n\n\n\n\ndata(\"teengamb\")\n\ngambmod &lt;- lm(gamble ~ sex + status + income + verbal, data = teengamb)\nmodsum&lt;- summary(gambmod)\nmodsum\n\n\nCall:\nlm(formula = gamble ~ sex + status + income + verbal, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.082 -11.320  -1.451   9.452  94.252 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22.55565   17.19680   1.312   0.1968    \nsex         -22.11833    8.21111  -2.694   0.0101 *  \nstatus        0.05223    0.28111   0.186   0.8535    \nincome        4.96198    1.02539   4.839 1.79e-05 ***\nverbal       -2.95949    2.17215  -1.362   0.1803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.69 on 42 degrees of freedom\nMultiple R-squared:  0.5267,    Adjusted R-squared:  0.4816 \nF-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06\n\n#A\nmodsum$r.squared\n\n[1] 0.5267234\n\n#B\nunname(which.max(modsum$residuals))\n\n[1] 24\n\n#C\nmean(modsum$residuals)\n\n[1] -2.26769e-16\n\nmedian(modsum$residuals)\n\n[1] -1.451392\n\n#D\ncor(gambmod$fitted.values, modsum$residuals)\n\n[1] -5.79346e-17\n\nplot(gambmod, which = 1) # or DHARMa::plotConventionalResiduals(gambmod)\n\n\n\nDHARMa::plotQQunif(gambmod)\n\n\n\n#E\ncor(modsum$residuals, teengamb$gamble)\n\n[1] 0.687951\n\n#F \nabs(modsum$coefficients[\"sex\", \"Estimate\"])\n\n[1] 22.11833\n\nemmeans::emmeans(gambmod, \"sex\")\n\n sex emmean   SE df lower.CL upper.CL\n   0  28.24 4.69 42     18.8     37.7\n   1   6.12 5.91 42     -5.8     18.0\n\nConfidence level used: 0.95 \n\nemmplot &lt;- plot(emmeans(gambmod, \"sex\"), colors=c(\"#003087\")) # or marginal_effects::plot_predictions(gambmod, condition = \"sex\")\nemmplot+\n  mytheme+\n  labs(y=\"Sex\", x=\"Estimated marginal mean\")\n\n\n\n\n\n\n\n\ndata(\"uswages\")\n\nwagemod &lt;- lm(wage ~ educ + exper, data=uswages)\ntidy(wagemod)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-242.799412\n50.6815927\n-4.790682\n1.8e-06\n\n\neduc\n51.175268\n3.3419257\n15.313108\n0.0e+00\n\n\nexper\n9.774767\n0.7505955\n13.022683\n0.0e+00\n\n\n\n\n\nwagemod$coefficients[\"educ\"]\n\n    educ \n51.17527 \n\n# Each additional year of education increases the predicted weekly wage by around $51\n\nlog_wagemod &lt;- lm(log(wage) ~ educ + exper, data=uswages)\nsummary(log_wagemod)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper, data = uswages)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7533 -0.3495  0.1068  0.4381  3.5699 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.650319   0.078354   59.35   &lt;2e-16 ***\neduc        0.090506   0.005167   17.52   &lt;2e-16 ***\nexper       0.018079   0.001160   15.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6615 on 1997 degrees of freedom\nMultiple R-squared:  0.1749,    Adjusted R-squared:  0.174 \nF-statistic: 211.6 on 2 and 1997 DF,  p-value: &lt; 2.2e-16\n\ncompare_performance(wagemod, log_wagemod)\n\nSome of the nested models seem to be identical\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC\nAIC_wt\nAICc\nAICc_wt\nBIC\nBIC_wt\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\nwagemod\nlm\n29915.88\n0\n29915.9\n0\n29938.28\n0\n0.1351186\n0.1342524\n427.532832\n427.8538424\n\n\nlog_wagemod\nlm\n28706.68\n1\n28706.7\n1\n28729.08\n1\n0.1748605\n0.1740342\n0.660967\n0.6614633\n\n\n\n\n\ncheck_model(wagemod)\n\n\n\ncheck_model(log_wagemod)\n\n\n\n\nAlthough interpretation of the coefficients is less straightforward for the log-transformed model, it has much lower AIC/AICc and RMSE values and a higher R-squared, indicating it is a better fit for the data. The posterior predictive checks also look much better for the log model."
  },
  {
    "objectID": "Notes/lab1.html#exercises",
    "href": "Notes/lab1.html#exercises",
    "title": "Lecture & Lab 1: Fitting Linear Models",
    "section": "",
    "text": "data(\"teengamb\")\n\ngambmod &lt;- lm(gamble ~ sex + status + income + verbal, data = teengamb)\nmodsum&lt;- summary(gambmod)\nmodsum\n\n\nCall:\nlm(formula = gamble ~ sex + status + income + verbal, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.082 -11.320  -1.451   9.452  94.252 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22.55565   17.19680   1.312   0.1968    \nsex         -22.11833    8.21111  -2.694   0.0101 *  \nstatus        0.05223    0.28111   0.186   0.8535    \nincome        4.96198    1.02539   4.839 1.79e-05 ***\nverbal       -2.95949    2.17215  -1.362   0.1803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.69 on 42 degrees of freedom\nMultiple R-squared:  0.5267,    Adjusted R-squared:  0.4816 \nF-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06\n\n#A\nmodsum$r.squared\n\n[1] 0.5267234\n\n#B\nunname(which.max(modsum$residuals))\n\n[1] 24\n\n#C\nmean(modsum$residuals)\n\n[1] -2.26769e-16\n\nmedian(modsum$residuals)\n\n[1] -1.451392\n\n#D\ncor(gambmod$fitted.values, modsum$residuals)\n\n[1] -5.79346e-17\n\nplot(gambmod, which = 1) # or DHARMa::plotConventionalResiduals(gambmod)\n\n\n\nDHARMa::plotQQunif(gambmod)\n\n\n\n#E\ncor(modsum$residuals, teengamb$gamble)\n\n[1] 0.687951\n\n#F \nabs(modsum$coefficients[\"sex\", \"Estimate\"])\n\n[1] 22.11833\n\nemmeans::emmeans(gambmod, \"sex\")\n\n sex emmean   SE df lower.CL upper.CL\n   0  28.24 4.69 42     18.8     37.7\n   1   6.12 5.91 42     -5.8     18.0\n\nConfidence level used: 0.95 \n\nemmplot &lt;- plot(emmeans(gambmod, \"sex\"), colors=c(\"#003087\")) # or marginal_effects::plot_predictions(gambmod, condition = \"sex\")\nemmplot+\n  mytheme+\n  labs(y=\"Sex\", x=\"Estimated marginal mean\")\n\n\n\n\n\n\n\n\ndata(\"uswages\")\n\nwagemod &lt;- lm(wage ~ educ + exper, data=uswages)\ntidy(wagemod)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-242.799412\n50.6815927\n-4.790682\n1.8e-06\n\n\neduc\n51.175268\n3.3419257\n15.313108\n0.0e+00\n\n\nexper\n9.774767\n0.7505955\n13.022683\n0.0e+00\n\n\n\n\n\nwagemod$coefficients[\"educ\"]\n\n    educ \n51.17527 \n\n# Each additional year of education increases the predicted weekly wage by around $51\n\nlog_wagemod &lt;- lm(log(wage) ~ educ + exper, data=uswages)\nsummary(log_wagemod)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper, data = uswages)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7533 -0.3495  0.1068  0.4381  3.5699 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.650319   0.078354   59.35   &lt;2e-16 ***\neduc        0.090506   0.005167   17.52   &lt;2e-16 ***\nexper       0.018079   0.001160   15.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6615 on 1997 degrees of freedom\nMultiple R-squared:  0.1749,    Adjusted R-squared:  0.174 \nF-statistic: 211.6 on 2 and 1997 DF,  p-value: &lt; 2.2e-16\n\ncompare_performance(wagemod, log_wagemod)\n\nSome of the nested models seem to be identical\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC\nAIC_wt\nAICc\nAICc_wt\nBIC\nBIC_wt\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\nwagemod\nlm\n29915.88\n0\n29915.9\n0\n29938.28\n0\n0.1351186\n0.1342524\n427.532832\n427.8538424\n\n\nlog_wagemod\nlm\n28706.68\n1\n28706.7\n1\n28729.08\n1\n0.1748605\n0.1740342\n0.660967\n0.6614633\n\n\n\n\n\ncheck_model(wagemod)\n\n\n\ncheck_model(log_wagemod)\n\n\n\n\nAlthough interpretation of the coefficients is less straightforward for the log-transformed model, it has much lower AIC/AICc and RMSE values and a higher R-squared, indicating it is a better fit for the data. The posterior predictive checks also look much better for the log model."
  },
  {
    "objectID": "Notes/lab1.html#simple-model",
    "href": "Notes/lab1.html#simple-model",
    "title": "Lecture & Lab 1: Fitting Linear Models",
    "section": "2.1 Simple model",
    "text": "2.1 Simple model\n\n2.1.1 Estimating beta-hat and y-hat\n\nnn &lt;- nrow(gala)\nyy &lt;- matrix(data=gala$species, nrow = nn, ncol = 1)\nhead(yy)\n\n     [,1]\n[1,]   58\n[2,]   31\n[3,]    3\n[4,]   25\n[5,]    2\n[6,]   18\n\nintercept &lt;- rep(1, nn)\narea &lt;- gala$area\nXX &lt;- cbind(intercept, area)\nhead(XX)\n\n     intercept  area\n[1,]         1 25.09\n[2,]         1  1.24\n[3,]         1  0.21\n[4,]         1  0.10\n[5,]         1  0.05\n[6,]         1  0.34\n\nbeta_hat &lt;- solve(t(XX) %*% XX) %*% t(XX) %*% yy\nbeta_hat\n\n                 [,1]\nintercept 63.78286147\narea       0.08196317\n\nHH &lt;- XX %*% solve(t(XX) %*% XX) %*% t(XX)\ny_hat &lt;- HH %*% yy\nhead(y_hat)\n\n         [,1]\n[1,] 65.83932\n[2,] 63.88450\n[3,] 63.80007\n[4,] 63.79106\n[5,] 63.78696\n[6,] 63.81073\n\nhead(XX %*% beta_hat)\n\n         [,1]\n[1,] 65.83932\n[2,] 63.88450\n[3,] 63.80007\n[4,] 63.79106\n[5,] 63.78696\n[6,] 63.81073\n\n\n\nggplot(gala, aes(x=area, y=species))+\n  geom_point()+\n  mytheme+\n  labs(x=expression(\"Area of island (\"*km^2*\")\"), y=\"Number of species\")+\n  geom_smooth(method=\"lm\", se=FALSE, formula = y~x, color=\"black\", linewidth=0.5)\n\n\n\n\n\n\n2.1.2 Automated model fitting\n\nsimple_model &lt;- lm(species ~ area, gala)\nsimple_model\n\n\nCall:\nlm(formula = species ~ area, data = gala)\n\nCoefficients:\n(Intercept)         area  \n   63.78286      0.08196  \n\n## via fitted\ny_hat_f &lt;- fitted(simple_model)\n## via predict\ny_hat_p &lt;- predict(simple_model, type = \"response\")\n## compare these to each other\nall.equal(y_hat_f, y_hat_p)\n\n[1] TRUE\n\n\n\n\n2.1.3 Goodness-of-fit\n\n## SSE\nresids &lt;- yy - y_hat\nSSE &lt;- sum(resids^2) # = t(resids) %*% resids\n\n## SSTO\nSSTO &lt;- sum((yy - mean(yy))^2)\n\n## R^2\n1 - SSE / SSTO\n\n[1] 0.3817301\n\nsummary(simple_model)\n\n\nCall:\nlm(formula = species ~ area, data = gala)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-99.495 -53.431 -29.045   3.423 306.137 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 63.78286   17.52442   3.640 0.001094 ** \narea         0.08196    0.01971   4.158 0.000275 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 91.73 on 28 degrees of freedom\nMultiple R-squared:  0.3817,    Adjusted R-squared:  0.3596 \nF-statistic: 17.29 on 1 and 28 DF,  p-value: 0.0002748\n\n\n\n\n2.1.4 Adjusted R-squared\n\nset.seed(514)\n## generate a vector of Gaussian white noise\nWN &lt;- rnorm(nn)\n## add this to our Galapagos data frame\ngala$WN &lt;- WN \n## fit a model with Area & WN\nsummary(lm(species ~ area + WN, gala))\n\n\nCall:\nlm(formula = species ~ area + WN, data = gala)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-93.86 -44.67 -21.73  15.93 308.84 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  70.68115   17.82480   3.965 0.000485 ***\narea          0.07892    0.01944   4.059 0.000378 ***\nWN          -20.27151   13.91996  -1.456 0.156843    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89.95 on 27 degrees of freedom\nMultiple R-squared:  0.4268,    Adjusted R-squared:  0.3843 \nF-statistic: 10.05 on 2 and 27 DF,  p-value: 0.0005465\n\n1 - (SSE / (nn - 2)) / (SSTO / (nn - 1)) #adj r2\n\n[1] 0.359649"
  },
  {
    "objectID": "Notes/lab1.html#better-model-lab-example",
    "href": "Notes/lab1.html#better-model-lab-example",
    "title": "Lecture & Lab 1: Fitting Linear Models",
    "section": "2.2 Better model (lab example)",
    "text": "2.2 Better model (lab example)\n\n## larger model\nfull_mod &lt;- lm(species ~ area + elevation + nearest, gala)\nfull_mod\n\n\nCall:\nlm(formula = species ~ area + elevation + nearest, data = gala)\n\nCoefficients:\n(Intercept)         area    elevation      nearest  \n   16.46471      0.01908      0.17134      0.07123  \n\n\n\n## get matrix of predictors\nXX &lt;- model.matrix(full_mod)\n## estimate beta\nbeta_hat &lt;- solve(t(XX) %*% XX) %*% t(XX) %*% yy\n## total sum of squares\nSSE &lt;- t(yy - XX %*% beta_hat) %*% (yy - XX %*% beta_hat)\n## error sum of squares\nSSTO &lt;- t(yy - mean(yy)) %*% (yy - mean(yy))\n## F statistic\nF_stat &lt;- ((SSTO - SSE) / (4 - 1)) / (SSE / (nn - 4))\npf(F_stat, 4-1, nn-4, lower.tail = F) #F-test\n\n             [,1]\n[1,] 8.816845e-05\n\n\n\n## null model; the '1' indicates an intercept-only model\nnull_mod &lt;- lm(species ~ 1, gala)\n## use `anova('simple', 'complex')` to get the F-test results\nanova(null_mod, full_mod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n29\n381081.4\nNA\nNA\nNA\nNA\n\n\n26\n169917.6\n3\n211163.8\n10.77043\n8.82e-05\n\n\n\n\n\nfull_mod_sum&lt;-summary(full_mod)\n\npf(full_mod_sum$fstatistic[1], full_mod_sum$fstatistic[2], full_mod_sum$fstatistic[3], lower.tail = F)\n\n       value \n8.816845e-05 \n\n\n\n## reduced model without `nearest`\nreduced_mod &lt;- lm(species ~ area + elevation, gala)\n## use `anova('reduced', 'full')` to get the F-test results\nanova(reduced_mod, full_mod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n27\n169946.8\nNA\nNA\nNA\nNA\n\n\n26\n169917.6\n1\n29.24263\n0.0044746\n0.9471792\n\n\n\n\n\n\n\n2.2.1 Testing a subspace\n\n## full model (with adjacent this time)\nfull_mod2 &lt;- lm(species ~ area + adjacent + elevation + nearest, gala)\n## reduced model without `elevation + nearest`\ncomb_mod &lt;- lm(species ~ I(area + adjacent) + elevation + nearest, gala)\n## use `anova('combined', 'full')` to get the F-test results\nanova(comb_mod, full_mod2)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n26\n116865.40\nNA\nNA\nNA\nNA\n\n\n25\n93867.15\n1\n22998.25\n6.125212\n0.0204613\n\n\n\n\n\n\n\n## model with effect of `elevation` = 1\nfixed_mod &lt;- lm(species ~ area + offset(1 * elevation) + nearest, gala)\n## use `anova('comb', 'full')` to get the F-test results\nanova(fixed_mod, full_mod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n27\n1679730.7\nNA\nNA\nNA\nNA\n\n\n26\n169917.6\n1\n1509813\n231.0246\n0\n\n\n\n\n\n\n\nsumary(full_mod)\n\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 16.464711  23.388841  0.7040 0.487718\narea         0.019085   0.026764  0.7131 0.482158\nelevation    0.171336   0.054519  3.1427 0.004151\nnearest      0.071227   1.064806  0.0669 0.947179\n\nn = 30, p = 4, Residual SE = 80.84116, R-Squared = 0.55\n\n## t statistic\n(t_value &lt;- (0.171336 - 1) / 0.054519)\n\n[1] -15.19955\n\n## p-value = t_alpha * Pr(t_value, df); `pt()` is the pdf for a t-dist\n(p_value &lt;- 1.96 * pt(t_value, 26))\n\n[1] 1.853024e-14\n\n## verify t^2 = F\nall.equal(t_value^2, anova(fixed_mod, full_mod)$F[2], tolerance = 0.0001)\n\n[1] TRUE\n\n\n\n\n2.2.2 CIs for beta-hat\n\n## critical value for the t-dist\n## `qt()` is the quantile function for the t-dist; `p` is the (1-alpha/2) value \nt_crit &lt;- qt(p = 0.975, df = 30-4)\n## 95% CI\nCI95_beta &lt;- 0.019085 + c(-1,1) * t_crit * 0.026764\nround(CI95_beta, 3)\n\n[1] -0.036  0.074\n\n\n\n## all of the 95% CI's\nconfint(full_mod)\n\n                   2.5 %      97.5 %\n(Intercept) -31.61174063 64.54116288\narea         -0.03593020  0.07409948\nelevation     0.05927051  0.28340204\nnearest      -2.11751249  2.25996697\n\n\n\n\n2.2.3 Bootstrap confidence intervals\n\n## residuals from our full model\nresids &lt;- residuals(full_mod)\n\n## number of bootstrap samples\nnb &lt;- 1000\n## empty matrix for beta estimates\nbeta_est &lt;- matrix(NA, nb, 4)\n## fitted values from our full model = X*beta\nXbeta &lt;- fitted(full_mod)\n## sample many times\nfor(i in 1:nb) {\n  ## 3a: sample w/ replacement from e\n  e_star &lt;- sample(resids, rep = TRUE)\n  ## 3b: calculate y_star\n  y_star &lt;- Xbeta + e_star\n  ## 3c: re-estimate beta_star from X & y_star\n  beta_star &lt;- update(full_mod, y_star ~ .)\n  ## save estimated betas\n  beta_est[i,] &lt;- coef(beta_star)\n}\n\n## extract 2.5% and 97.5% values\nCI95 &lt;- apply(beta_est, 2, quantile, c(0.025, 0.975))\ncolnames(CI95) &lt;- c(\"Intercept\", \"area\", \"elevation\" , \"nearest\")\nt(round(CI95, 3))\n\n             2.5%  97.5%\nIntercept -22.402 61.470\narea       -0.027  0.084\nelevation   0.073  0.277\nnearest    -1.918  2.150\n\n\n\n\n2.2.4 Boostrap coefficients and CIs using the rsample package\n\nset.seed(462)\nlibrary(rsample)\n\n# Will be used to fit the models to different bootstrap data sets:\nfit_fun &lt;- function(split, ...) {\n  # We could check for convergence, make new parameters, etc.\n  lm(species ~ area + elevation + nearest, data = analysis(split), ...) %&gt;%\n    tidy()\n}\n\nbt &lt;-\n  bootstraps(gala, times = 1000, apparent = TRUE) %&gt;%\n  mutate(models = map(splits, fit_fun))\n\nint_pctl(bt, models)\n\n\n\n\n\nterm\n.lower\n.estimate\n.upper\n.alpha\n.method\n\n\n\n\n(Intercept)\n-33.7640985\n16.8546860\n57.8416355\n0.05\npercentile\n\n\narea\n-0.0911716\n0.1138601\n0.5206100\n0.05\npercentile\n\n\nelevation\n-0.1372787\n0.1516931\n0.4687342\n0.05\npercentile\n\n\nnearest\n-2.4452623\n-0.1349482\n1.8394215\n0.05\npercentile\n\n\n\n\n\n\n\n\n2.2.5 Confidence interval for new predictions\n\n2.2.5.1 By hand\n\n## matrix of predictors\nXX &lt;- model.matrix(simple_model)\n## new X; vector for now\nX_star &lt;- c(intercept = 1, area = 2000)\n## inside sqrt\ninner_X &lt;- t(X_star) %*% solve(t(XX) %*% XX) %*% X_star\n## critical t-value\nt_crit &lt;- qt(0.975, df = nn-2)\n## estimated SD\nsigma &lt;- summary(simple_model)$sigma\n## predicted y\ny_star &lt;- sum(X_star * coef(simple_model))\n## 95% CI\nc(y_star) + c(-1,1) * c(t_crit) * c(sigma) * c(sqrt(inner_X))\n\n[1] 149.5818 305.8366\n\n\n\n\n2.2.5.2 Using predict\n\npredict(simple_model, new = data.frame(t(X_star)),\n        level = 0.95, interval = \"confidence\")\n\n       fit      lwr      upr\n1 227.7092 149.5818 305.8366\n\n\n\n\n\n2.2.6 Prediction interval for new response\n\n## new X_star\nX_star &lt;- c(intercept = 1, area = 2000)\n## inside sqrt\ninner_X &lt;- 1 + t(X_star) %*% solve(t(XX) %*% XX) %*% X_star\n## 95% CI\ny_star + c(-1,1) * c(t_crit) * c(sigma) * c(sqrt(inner_X))\n\n[1]  24.21065 431.20776\n\npredict(simple_model, new = data.frame(t(X_star)),\n        level = 0.95, interval = \"prediction\")\n\n       fit      lwr      upr\n1 227.7092 24.21065 431.2078"
  },
  {
    "objectID": "Notes/ch8+9.html",
    "href": "Notes/ch8+9.html",
    "title": "Problems with Model Errors",
    "section": "",
    "text": "# List of packages required:\npackages &lt;- c(\"tidyverse\", \"PNWColors\", \"janitor\", \"faraway\", \"broom\", \"DHARMa\", \"emmeans\", \"performance\", \"nlme\", \"MASS\", \"broom.helpers\")\n\n# Load packages into session\nlapply(packages, require, character.only = TRUE)\nrm(packages)\n\n# Ensure functions with duplicate names are from the correct package\nselect &lt;- dplyr::select\nmap &lt;- purrr::map\nsummarize &lt;- dplyr::summarize\nclean_names &lt;- janitor::clean_names\nmargin &lt;- ggplot2::margin\n\nset.seed(123) #Set seed for pseudo-random number generator, for reproducibility\n\nmytheme &lt;- theme_light()+ #define custom theme for ggplots\n  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, l = 0)),\n        axis.title.x = element_text(margin = margin(t = 10, l = 0)),\n        text=element_text(size=15))"
  },
  {
    "objectID": "Notes/ch8+9.html#generalized-least-squares",
    "href": "Notes/ch8+9.html#generalized-least-squares",
    "title": "Problems with Model Errors",
    "section": "8.1 Generalized Least Squares",
    "text": "8.1 Generalized Least Squares\nWe have previously assumed \\(\\text{var}(e)=\\sigma^2 I\\), but suppose that the errors have non-constant variance, \\(\\text{var}(e)=\\sigma^2 \\Sigma\\), where \\(\\sigma^2\\) is unknown and \\(\\Sigma\\) is known: we know the correlation and relative variance between the errors, but not the absolute scale of the variation.\nWe can write \\(\\Sigma = SS^\\top\\) , where S is a triangular matrix using the Choleski decomposition, which be can be viewed as a square root for a matrix. We can transform the regression model as follows: \\[\\begin{align*}\ny &= X\\beta +e\nS^{-1}y &=S^{-1}X \\beta+S^{-1}e\ny' &= X'\\beta+e'\n\\end{align*}\\] Then, \\(\\text{var}(e')=\\text{var}(S^{-1}e)=S^{-1}(\\text{var }e)S^{-\\top}=S^{-1}\\sigma^2 SS^\\top S^{-\\top}=\\sigma^2 I\\)\nSo we can reduce GLS to OLS by a regression of \\(y′ = S^{-1}y\\) on \\(X′ = S^{-1}X\\) which has error \\(e'=S^{-1}e\\) that is i.i.d. We have transformed the problem to the standard case.\nErrors between successive observations \\(\\rightarrow\\) autoregressive model, \\(e_{i+1}=\\phi e_i+\\delta_i\\) where \\(\\delta \\sim N(0, \\tau^2)\\)\n\nlmod &lt;- lm(nhtemp ~ wusa + jasper + westgreen + chesapeake + tornetrask + urals + mongolia + tasman, globwarm)\n\ncor(residuals(lmod)[-1],residuals(lmod)[-length(residuals(lmod))])\n\n[1] 0.583339\n\nlmtest::dwtest(lmod)\n\n\n    Durbin-Watson test\n\ndata:  lmod\nDW = 0.81661, p-value = 1.402e-15\nalternative hypothesis: true autocorrelation is greater than 0\n\ncar::durbinWatsonTest(lmod) #tidy(car::durbinWatsonTest(lmod))\n\n lag Autocorrelation D-W Statistic p-value\n   1       0.5710535     0.8166064       0\n Alternative hypothesis: rho != 0\n\nperformance::check_autocorrelation(lmod)\n\nWarning: Autocorrelated residuals detected (p &lt; .001).\n\nglmod &lt;- gls(nhtemp ~ wusa + jasper + westgreen + chesapeake + tornetrask + urals + mongolia + tasman, correlation=corAR1(form= ~year), data=na.omit(globwarm))\n\nsummary(glmod)\n\nGeneralized least squares fit by REML\n  Model: nhtemp ~ wusa + jasper + westgreen + chesapeake + tornetrask +      urals + mongolia + tasman \n  Data: na.omit(globwarm) \n        AIC       BIC   logLik\n  -108.2074 -76.16822 65.10371\n\nCorrelation Structure: AR(1)\n Formula: ~year \n Parameter estimate(s):\n      Phi \n0.7109922 \n\nCoefficients:\n                  Value  Std.Error   t-value p-value\n(Intercept) -0.23010624 0.06702406 -3.433188  0.0008\nwusa         0.06673819 0.09877211  0.675678  0.5004\njasper      -0.20244335 0.18802773 -1.076668  0.2835\nwestgreen   -0.00440299 0.08985321 -0.049002  0.9610\nchesapeake  -0.00735289 0.07349791 -0.100042  0.9205\ntornetrask   0.03835169 0.09482515  0.404446  0.6865\nurals        0.24142199 0.22871028  1.055580  0.2930\nmongolia     0.05694978 0.10489786  0.542907  0.5881\ntasman       0.12034918 0.07456983  1.613913  0.1089\n\n Correlation: \n           (Intr) wusa   jasper wstgrn chespk trntrs urals  mongol\nwusa       -0.517                                                 \njasper     -0.058 -0.299                                          \nwestgreen   0.330 -0.533  0.121                                   \nchesapeake  0.090 -0.314  0.230  0.147                            \ntornetrask -0.430  0.499 -0.197 -0.328 -0.441                     \nurals      -0.110 -0.142 -0.265  0.075 -0.064 -0.346              \nmongolia    0.459 -0.437 -0.205  0.217  0.449 -0.343 -0.371       \ntasman      0.037 -0.322  0.065  0.134  0.116 -0.434  0.416 -0.017\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-2.31122523 -0.53484054  0.02342908  0.50015642  2.97224724 \n\nResidual standard error: 0.204572 \nDegrees of freedom: 145 total; 136 residual\n\nintervals(glmod,which=\"var-cov\")\n\nApproximate 95% confidence intervals\n\n Correlation structure:\n       lower      est.     upper\nPhi 0.509981 0.7109922 0.8383726\n\n Residual standard error:\n    lower      est.     upper \n0.1540719 0.2045720 0.2716246 \n\n\nAlso arises in spatial data & blocked designs, for example:\n\nglmod &lt;- gls(yield ~ variety, oatvar, correlation = corCompSymm(form\n= ~1 | block))\n\nintervals(glmod)\n\nApproximate 95% confidence intervals\n\n Coefficients:\n                 lower  est.       upper\n(Intercept) 291.542999 334.4 377.2570009\nvariety2     -4.903898  42.2  89.3038984\nvariety3    -18.903898  28.2  75.3038984\nvariety4    -94.703898 -47.6  -0.4961016\nvariety5     57.896102 105.0 152.1038984\nvariety6    -50.903898  -3.8  43.3038984\nvariety7    -63.103898 -16.0  31.1038984\nvariety8      2.696102  49.8  96.9038984\n\n Correlation structure:\n         lower      est.     upper\nRho 0.05100649 0.3959955 0.7665766\n\n Residual standard error:\n   lower     est.    upper \n33.02434 47.04679 67.02329 \n\n\nThere is evidence of a non-zero correlation of ~0.4 between the errors within the blocks."
  },
  {
    "objectID": "Notes/ch8+9.html#weighted-least-squares-wls",
    "href": "Notes/ch8+9.html#weighted-least-squares-wls",
    "title": "Problems with Model Errors",
    "section": "8.2 Weighted Least Squares (WLS)",
    "text": "8.2 Weighted Least Squares (WLS)\n\nA special case of GLS\nErrors are uncorrelated, but have unequal variance where the form of the inequality is known.\n\\(\\Sigma\\) is diagonal but the entries are not equal\nSet \\(\\Sigma = \\text{diag}(1/w_1,\\ldots,1/w_n)\\), where the \\(w_i\\) are the weights so \\(S=\\text{diag}(\\sqrt{1/w_1},\\ldots,\\sqrt{1/w_n})\\)\nThen regress \\(\\sqrt{w_i}y_i\\) on \\(\\sqrt{w_i}x_i\\) (although the column of ones in the X-matrix needs to be replaced with \\(\\sqrt{w_i}\\))\nResiduals are modified to use \\(\\sqrt{w_i}\\hat{e}_i\\)\nCases with low variability get a high weight and those with high variability a low weight.\n\n\nExamples\n\nErrors proportional to a predictor: \\(\\text{var}(e) \\propto x_i\\) suggests \\(w_i = x^{−1}_i\\). One might choose this option after observing a positive relationship in a plot of \\(\\lvert \\hat{e}_i \\rvert\\) against \\(x_i\\).\nWhen the \\(Y_i\\) are the averages of \\(n_i\\) observations, then \\(\\text{var}(y_i)= \\text{var}(e_i)= \\sigma^2/n_i\\), which suggests \\(w_i=n_i\\)\nWhen the observed responses are known to be of varying quality, weights may be assigned \\(w_i = 1/\\text{sd}(y_i)\\)."
  },
  {
    "objectID": "Notes/ch8+9.html#robust-regression",
    "href": "Notes/ch8+9.html#robust-regression",
    "title": "Problems with Model Errors",
    "section": "8.4 Robust Regression",
    "text": "8.4 Robust Regression\nWhen the errors are normally distributed, OLS is best, but long-tailed error distributions can cause difficulties because a few extreme cases can have a large effect on the fitted model. Robust regression is designed to estimate the mean relationship between the predictors and response, \\(EY = X\\beta\\).\n\nM-Estimation\nM-estimates modify the least squares idea to choose \\(\\beta\\) to minimize: \\[\\sum_{i=1}^n \\rho(y_i - x_i^\\top \\beta) \\]\nPossible choices for \\(\\rho\\) include:\n\n\\(\\rho(x)=x^2\\) is just OLS\n\\(\\rho(x)=\\lvert x \\rvert\\) is called least absolute deviation (LAD) regression or \\(L_1\\) regression.\n\\[\n\\rho(x)= \\begin{cases}\nx^2/2 & \\text{if } \\lvert x \\rvert \\leq c \\\\\nc\\lvert x \\rvert \\leq -c^2/2 & \\text{ otherwise}\n\\end{cases}\n\\tag{1}\\]\n\nEquation 1 is called Huber’s method and is a compromise between least squares and LAD regression. \\(c\\) should be a robust estimate of \\(\\sigma\\); a value proportional to the median of \\(\\lvert \\hat{e} \\rvert\\)$ is suitable.\nM-estimation is related to weighted least squares, where \\(w(u) = \\rho′\\;(u)/u\\).\nWe find for our choices of ρ above that: 1. LS: \\(w(u)\\) is constant and the estimator is simply OLS. - \\(\\rho'(x) = 2x\\), so \\(\\rho′\\;(x)/x=2x/x=2\\) 2. LAD: \\(w(u) = 1/|u|\\). We see how the weight goes down as \\(u\\) moves away from 0, so that more extreme observations get downweighted. However, the asymptote at 0 makes a weighting approach to fitting an LAD regression infeasible without some modification. 3. Huber: \\[\\begin{equation*}\nw(u)= \\begin{cases}\n1 & \\text{if } \\lvert u \\rvert \\leq c \\\\\nc/\\lvert u \\rvert & \\text{ otherwise}\n\\end{cases}\n\\end{equation*}\\]\nThis sensibly combines the downweighting of extreme cases with equal weighting for the middle cases.\nThe Huber method is the default for the rlm function:\n\nrlmod&lt;-rlm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent,gala)\nsummary(rlmod)\n\n\nCall: rlm(formula = Species ~ Area + Elevation + Nearest + Scruz + \n    Adjacent, data = gala)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-74.389 -18.353  -6.364  21.187 229.082 \n\nCoefficients:\n            Value   Std. Error t value\n(Intercept)  6.3611 12.3897     0.5134\nArea        -0.0061  0.0145    -0.4214\nElevation    0.2476  0.0347     7.1320\nNearest      0.3592  0.6819     0.5267\nScruz       -0.1952  0.1393    -1.4013\nAdjacent    -0.0546  0.0114    -4.7648\n\nResidual standard error: 29.73 on 24 degrees of freedom\n\n\nIt is worth looking at the weights assigned by the final fit. We extract and name the smallest 10 weights. The remaining weights are all ones.\n\nwts &lt;- rlmod$w\nnames(wts) &lt;- row.names(gala)\nhead(sort(wts),10)\n\n   SantaCruz   SantaMaria SanCristobal        Pinta     Gardner1     Espanola \n   0.1745816    0.3078288    0.4142330    0.5375752    0.6613011    0.6795050 \n    Gardner2       Baltra    Bartolome     Caldwell \n   0.8500380    1.0000000    1.0000000    1.0000000 \n\n\nWe can see that a few islands are substantially discounted in the calculation of the robust fit. Provided we do not believe there are mistakes in the data for these cases, we should think carefully about what might be unusual about these islands. The main purpose in analyzing these data is likely to explain the relationship between the predictors and the response. Although the robust fit gives numerically different output, the overall impression of what predictors are significant in explaining the response is unchanged. Thus the robust regression has provided some measure of confirmation. Furthermore, it has identified a few islands which are not fit so well by the model.\nWe can also do LAD regression using the quantreg package. The default option does LAD while other options allow for quantile regression:\n\nl1mod &lt;- quantreg::rq(Species ~Area+Elevation+Nearest+Scruz+Adjacent, data=gala)\nsummary(l1mod)\n\n\nCall: quantreg::rq(formula = Species ~ Area + Elevation + Nearest + \n    Scruz + Adjacent, data = gala)\n\ntau: [1] 0.5\n\nCoefficients:\n            coefficients lower bd  upper bd \n(Intercept)   1.31445    -19.87777  24.37411\nArea         -0.00306     -0.03185   0.52800\nElevation     0.23211      0.12453   0.50196\nNearest       0.16366     -3.16339   2.98896\nScruz        -0.12314     -0.47987   0.13476\nAdjacent     -0.05185     -0.10458   0.01739\n\n\n\n\nLeast Trimmed Squares (LTS)\nThe Huber and \\(L_1\\) methods will still fail if the large errors are sufficiently numerous and extreme in value. For example, very bad data entry errors might be made or measurement equipment might malfunction in a serious way. We need methods that still fit the correct data well even in the presence of such problems. LTS is an example of a resistant regression method. Resistant methods are good for dealing with data where we expect a certain number of bad observations that we want to have no weight in the analysis.\nLTS minimizes the sum of squares of the \\(q\\) smallest residuals, \\(\\sum^q_{i=1} \\hat{e}_{(i)}^2\\), where \\(q\\) is some number less than \\(n\\) and \\((i)\\) indicates sorting. This method has a high breakdown point because it can tolerate a large number of outliers depending on how \\(q\\) is chosen. For the Galapagos data:\n\nset.seed(123)\n\nltsmod &lt;- MASS::ltsreg(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala)\ncoef(ltsmod)\n\n(Intercept)        Area   Elevation     Nearest       Scruz    Adjacent \n12.50668352  1.54535820  0.01672532  0.52348693 -0.09407229 -0.14259212 \n\n# ltsmod_exact &lt;- ltsreg(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala,\n#                        nsamp=\"exact\")\n# coef(ltsmod_exact)\n\nHowever, we do not have standard errors for the LTS regression coefficients. We can use bootstrapping, as follows: 1. Generate \\(e^∗\\) by sampling with replacement from \\(\\hat{e}_1, \\ldots, \\hat{e}_n\\). 2. Form \\(y^∗ = X\\hat{\\beta}+e^∗\\). 3. Compute \\(\\hat{\\beta}^*\\) from \\((X, y^∗)\\).\n\nbcoef &lt;- matrix(0,1000,6)\n\nfor(i in 1:1000){\nnewy &lt;- predict(ltsmod) + residuals(ltsmod)[sample(30,rep=T)]\nbrg &lt;- ltsreg(newy ~ Area + Elevation + Nearest + Scruz + Adjacent\n, gala , nsamp=\"best\")\nbcoef[i,] &lt;- brg$coef\n}\n\n\ncolnames(bcoef) &lt;- names(coef(ltsmod))\n(quants &lt;- apply(bcoef,2,function(x) quantile(x, c(0.025,0.975))))\n\n      (Intercept)     Area   Elevation    Nearest      Scruz    Adjacent\n2.5%     1.412877 1.437543 -0.03415286 -0.4172014 -0.3159858 -0.19703956\n97.5%   31.382253 1.666588  0.09547331  2.2739460  0.2219613 -0.06381921\n\nbcoef &lt;- data.frame(bcoef)\np1 &lt;- ggplot(bcoef, aes(x = Area)) + geom_density() + mytheme\np1 + geom_vline(xintercept=c(quants[1,2], quants[2,2]), linetype=\"dashed\")\n\n\n\np2 &lt;- ggplot(bcoef, aes(x = Adjacent)) + geom_density() + mytheme+xlim(-0.5, 0.5)\np2 + geom_vline(xintercept=c(quants[1,6], quants[2,6]), linetype=\"dashed\")"
  },
  {
    "objectID": "Notes/ch8+9.html#transforming-the-response",
    "href": "Notes/ch8+9.html#transforming-the-response",
    "title": "Problems with Model Errors",
    "section": "9.1 Transforming the response",
    "text": "9.1 Transforming the response\nSuppose that you are contemplating a logged response in a simple regression situation: \\(\\log{y} = \\beta_0 +\\beta_1x+e\\)\nIn the original scale of the response, this model becomes: \\[y = \\exp(\\beta_0 +\\beta_1x)· \\exp(e) \\quad(9.1)\\]\nIn this model, the errors enter multiplicatively and not additively as they usually do, so the use of standard regression methods for the logged response model requires that we believe the errors enter multiplicatively in the original scale.\n\nBox-Cox"
  },
  {
    "objectID": "Notes/design_matrices.html",
    "href": "Notes/design_matrices.html",
    "title": "Design Matrices",
    "section": "",
    "text": "# List of packages required:\npackages &lt;- c(\"tidyverse\", \"janitor\")\n\n# Load packages into session\nlapply(packages, require, character.only = TRUE)\nrm(packages)\n\n# Ensure functions with duplicate names are from the correct package\nselect &lt;- dplyr::select\nmap &lt;- purrr::map\nsummarize &lt;- dplyr::summarize\nclean_names &lt;- janitor::clean_names\n\nmytheme &lt;- theme_light()+ #define custom theme for ggplots\n  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, l = 0)),\n        axis.title.x = element_text(margin = margin(t = 10, l = 0)),\n        text=element_text(size=15))"
  },
  {
    "objectID": "Notes/design_matrices.html#one-way-anova-means-model-w-3-groups-and-7-observations",
    "href": "Notes/design_matrices.html#one-way-anova-means-model-w-3-groups-and-7-observations",
    "title": "Design Matrices",
    "section": "One-way ANOVA: means model (w/ 3 groups and 7 observations)",
    "text": "One-way ANOVA: means model (w/ 3 groups and 7 observations)\nThe given data set has the first three observations belonging to the first group, the following two observations belonging to the second group and the final two observations belonging to the third group. If the model to be fit is just the mean of each group, then the model is\n\\[ y_{ij}=\\mu_{i}+\\varepsilon_{ij} \\] which can be written\n\\[ {\\begin{bmatrix}y_{1}\\\\y_{2}\\\\y_{3}\\\\y_{4}\\\\y_{5}\\\\y_{6}\\\\y_{7}\\end{bmatrix}}={\\begin{bmatrix}1&0&0\\\\1&0&0\\\\1&0&0\\\\0&1&0\\\\0&1&0\\\\0&0&1\\\\0&0&1\\end{bmatrix}}{\\begin{bmatrix}\\mu _{1}\\\\\\mu _{2}\\\\\\mu _{3}\\end{bmatrix}}+{\\begin{bmatrix}\\varepsilon _{1}\\\\\\varepsilon _{2}\\\\\\varepsilon _{3}\\\\\\varepsilon _{4}\\\\\\varepsilon _{5}\\\\\\varepsilon _{6}\\\\\\varepsilon _{7}\\end{bmatrix}}\\] In this model \\(\\mu_{i}\\) represents the mean of the \\(i\\)th group.\n\nCode\n\n## fit ANOVA w/ `- 1` to remove intercept\nm1 &lt;- lm(yy ~ ration - 1)\ncoef(m1)\n\nration_1 ration_2 ration_3 \n19.67013 26.71379 34.58413"
  },
  {
    "objectID": "Notes/design_matrices.html#one-way-anova-offset-from-global-mean",
    "href": "Notes/design_matrices.html#one-way-anova-offset-from-global-mean",
    "title": "Design Matrices",
    "section": "One-way ANOVA: offset from global mean",
    "text": "One-way ANOVA: offset from global mean\nSuppose we wanted to reframe our model to instead include the effect of ration relative to the overall mean growth rate \\((\\mu)\\)\nIf we write the model as \\[y_i = \\mu + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 x_{3,i} + e_i\\] and calculate the groups means as\n\\[\n\\bar{y}_{j=1} = \\mu + \\beta_1 \\\\\n\\bar{y}_{j=2} = \\mu + \\beta_2 \\\\\n\\bar{y}_{j=3} = \\mu + \\beta_3,\n\\] We would then define \\(\\mathbf{X}\\) and \\(\\boldsymbol{\\beta}\\) as\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & 1 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 1 & 0 \\\\\n1 & 0 & 0 & 1 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 0 & 1\n\\end{bmatrix}\n~~~\n\\boldsymbol{\\beta} =\n\\begin{bmatrix}\n\\mu \\\\\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\beta_3\n\\end{bmatrix}\n\\]\nHowever, this produces an error because \\(\\mathbf{X}\\) is not full rank \\((\\mathbf{X}_{(\\cdot 1)} = \\mathbf{X}_{(\\cdot 2)} + \\mathbf{X}_{(\\cdot 3)} + \\mathbf{X}_{(\\cdot 4)})\\)\n\n## design matrix\nX &lt;- cbind(rep(1,nn*pp), ration)\n## fit ANOVA w/ `- 1` to remove intercept\nm2 &lt;- lm(yy ~ X - 1)\ncoef(m2)\n\n         X        X_1        X_2        X_3 \n 34.584128 -14.913996  -7.870338         NA \n\n\n\n## solve for beta by hand\nbeta &lt;- solve(t(X) %*% X) %*% t(X) %*% yy\n\n## Error in solve.default(t(X) %*% X) : \n##   system is computationally singular: reciprocal condition number\nTry considering the overall mean of \\(\\mathbf{y}\\) in terms of the group means: \\[\\bar{y} = \\frac{\\bar{y}_{j=1} + \\bar{y}_{j=2} + \\bar{y}_{j=3}}{3}\\] \\[\\mu = \\frac{(\\mu + \\beta_1) + (\\mu + \\beta_2) + (\\mu + \\beta_3)}{3}\\] \\[\\beta_1 + \\beta_2 + \\beta_3 = 0\\] Now we can rewrite our model as \\[y_i = \\mu + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + (\\text{-} \\beta_1 + \\text{-} \\beta_2) x_{3,i} + e_i\\] and calculate the group means as\n\\[\n\\begin{aligned}\n\\bar{y}_{j=1} &= \\mu + \\beta_1 \\\\\n\\bar{y}_{j=2} &= \\mu + \\beta_2 \\\\\n\\bar{y}_{j=3} &= \\mu - (\\beta_1 + \\beta_2)\n\\end{aligned}\n\\] We would then define \\(\\mathbf{X}\\) and \\(\\boldsymbol{\\beta}\\) as\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & 1 & 0 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 1 \\\\\n1 & -1 & -1 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & -1 & -1\n\\end{bmatrix}\n~~~\n\\boldsymbol{\\beta} =\n\\begin{bmatrix}\n\\mu \\\\\n\\beta_1 \\\\\n\\beta_2\n\\end{bmatrix}\n\\]\nWe could also fit our grand mean model after some simple algebra \\[\\begin{align*}\ny_i &= \\mu + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 x_{3,i} + e_i \\\\\n&\\Downarrow \\\\\ny_i - \\mu &= \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 x_{3,i} + e_i \\\\\n&\\Downarrow \\\\\ny_i - \\bar{y} &= \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 x_{3,i} + e_i\n\\end{align*}\\]\n\nCode\n\n## empty design matrix\nXX &lt;- matrix(NA, nn*pp, pp)\n\nXX[i1,] &lt;- matrix(c(1,  1,  0), nn, pp, byrow = TRUE)\n\nXX[i2,] &lt;- matrix(c(1,  0,  1), nn, pp, byrow = TRUE)\n\nXX[i3,] &lt;- matrix(c(1, -1, -1), nn, pp, byrow = TRUE)\n\n## fit model & get parameters\nBvec &lt;- coef(lm(yy ~ XX - 1))\nnames(Bvec) &lt;- c(\"mu\", \"beta_1\", \"beta_2\")\nBvec\n##         mu     beta_1     beta_2 \n## 26.9893503 -7.3192180 -0.2755598\n\n\n\n## mean of ration 1\nunname(Bvec[\"mu\"] + Bvec[\"beta_1\"])\n## [1] 19.67013\n## mean of ration 2\nunname(Bvec[\"mu\"] + Bvec[\"beta_2\"])\n## [1] 26.71379\n## mean of ration 3\nunname(Bvec[\"mu\"] - (Bvec[\"beta_1\"] + Bvec[\"beta_2\"]))\n## [1] 34.58413\n\n\n## fit anova with implicit grand mean\nm2 &lt;- lm((yy - mean(yy)) ~ ration - 1)\ncoef(m2)\n##   ration_1   ration_2   ration_3 \n## -7.3192180 -0.2755598  7.5947778\n\ncoef(m2) + mean(yy)\n## ration_1 ration_2 ration_3 \n## 19.67013 26.71379 34.58413\ncoef(m1)\n## ration_1 ration_2 ration_3 \n## 19.67013 26.71379 34.58413"
  },
  {
    "objectID": "Notes/design_matrices.html#one-way-anova-offset-from-reference-group",
    "href": "Notes/design_matrices.html#one-way-anova-offset-from-reference-group",
    "title": "Design Matrices",
    "section": "One-way ANOVA: offset from reference group",
    "text": "One-way ANOVA: offset from reference group\nWhat if we wanted to treat one group as a control or reference (eg, our low ration) and estimate the other effects relative to it?\n\\[\ny_i = \\beta_1 x_{1,i} + (\\beta_1 + \\beta_2) x_{2,i} + (\\beta_1 + \\beta_3) x_{3,i} + e_i\n\\]\nsuch that\n\\[\n\\begin{align}\n\\bar{y}_{j=1} &= \\beta_1 \\\\\n\\bar{y}_{j=2} &= \\beta_1 + \\beta_2 \\\\\n\\bar{y}_{j=3} &= \\beta_1 + \\beta_3\n\\end{align}\n\\]\nWe would define \\(\\mathbf{X}\\) and \\(\\boldsymbol{\\beta}\\) as\n\\[\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & 0 & 1\n\\end{bmatrix}\n~~~\n\\boldsymbol{\\beta} =\n\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\beta_3\n\\end{bmatrix}\n\\]\n\nCode\n\n## empty design matrix\nXX &lt;- matrix(NA, nn*pp, pp)\n## for beta_1\nXX[i1,] &lt;- matrix(c(1, 0, 0), nn, pp, byrow = TRUE)\n## for beta_1 + beta_2\nXX[i2,] &lt;- matrix(c(1, 1, 0), nn, pp, byrow = TRUE)\n## for beta_1 + beta_3\nXX[i3,] &lt;- matrix(c(1, 0, 1), nn, pp, byrow = TRUE)\n## fit anova with implicit grand mean\nBvec &lt;- coef(lm(yy ~ XX - 1))\nnames(Bvec) &lt;- c(\"beta_1\", \"beta_2\", \"beta_3\")\nBvec\n\n   beta_1    beta_2    beta_3 \n19.670132  7.043658 14.913996 \n\n\n\n## mean of ration 1\nunname(Bvec[\"beta_1\"])\n## mean of ration 2\nunname(Bvec[\"beta_1\"] + Bvec[\"beta_2\"])\n## mean of ration 3\nunname(Bvec[\"beta_1\"] + Bvec[\"beta_3\"])\n\n[1] 19.67013\n[1] 26.71379\n[1] 34.58413"
  },
  {
    "objectID": "Notes/design_matrices.html#code-2",
    "href": "Notes/design_matrices.html#code-2",
    "title": "Design Matrices",
    "section": "Code",
    "text": "Code\n\n## plot all data\npar(mai = c(0.9,0.9,0.1,0.1),\n    omi = c(0, 0, 0, 0.2),\n    cex = 1.1)\n## plot all data\nplot(x_cov[1:nn], yy[1:nn], pch = 16, col = \"red\", ylim = range(yy),\n     las = 1, xlab = \"Ration size (g)\", ylab = \"Growth (mm)\")\npoints(x_cov[1:nn+nn], yy[1:nn+nn], pch = 16, col = \"blue\")\npoints(x_cov[1:nn+nn*2], yy[1:nn+nn*2], pch = 16, col = \"orange\")\n\n\n\n\n\n\n\n\n\n## create design matrix\nXX &lt;- cbind(L1 = rep(c(1,0,0), ea = nn), # effect of lineage 1\n            L2 = rep(c(0,1,0), ea = nn), # effect of lineage 2\n            L3 = rep(c(0,0,1), ea = nn), # effect of lineage 3\n            RA = x_cov)                  # effect of ration\n## fit model\nBvec &lt;- coef(lm(yy ~ XX - 1))\nnames(Bvec) &lt;- c(\"beta_1\", \"beta_2\", \"beta_3\", \"beta_4\")\nBvec\n\n  beta_1   beta_2   beta_3   beta_4 \n10.35668 15.52218 20.71430  1.89973 \n\n\n\n## plot all data\npar(mai = c(0.9,0.9,0.1,0.1),\n    omi = c(0, 0, 0, 0.2),\n    cex = 1.1)\n## blank plot\nplot(x_cov[1:nn], yy[1:nn], type = \"n\", ylim = range(yy),\n     las = 1, xlab = \"Ration size (g)\", ylab = \"Growth (mm)\")\n## add fits\nabline(a = Bvec[1], b = Bvec[4], col = \"red\")\nabline(a = Bvec[2], b = Bvec[4], col = \"blue\")\nabline(a = Bvec[3], b = Bvec[4], col = \"orange\")\n## add intercepts\nabline(h = Bvec[1], lty = \"dashed\", col = \"red\")\nabline(h = Bvec[2], lty = \"dashed\", col = \"blue\")\nabline(h = Bvec[3], lty = \"dashed\", col = \"orange\")\n## add data\npoints(x_cov[1:nn], yy[1:nn], pch = 16, col = \"red\")\npoints(x_cov[1:nn+nn], yy[1:nn+nn], pch = 16, col = \"blue\")\npoints(x_cov[1:nn+nn*2], yy[1:nn+nn*2], pch = 16, col = \"orange\")\n## add labels\ntext(x = 1.03 * par(\"usr\")[2], y = Bvec[1],\n     expression(beta[1]), xpd = NA, col = \"red\")\ntext(x = 1.03 * par(\"usr\")[2], y = Bvec[2],\n     expression(beta[2]), xpd = NA, col = \"blue\")\ntext(x = 1.03 * par(\"usr\")[2], y = Bvec[3],\n     expression(beta[3]), xpd = NA, col = \"orange\")"
  },
  {
    "objectID": "Notes/design_matrices.html#code-3",
    "href": "Notes/design_matrices.html#code-3",
    "title": "Design Matrices",
    "section": "Code",
    "text": "Code\n\n\n\n\n\n\n\n\n\n\n## create design matrix\nXX &lt;- cbind(L1 = rep(c(1,0,0), ea = nn), # effect of lineage 1\n            L2 = rep(c(0,1,0), ea = nn), # effect of lineage 2\n            L3 = rep(c(0,0,1), ea = nn), # effect of lineage 3\n            RA = x_cov)                  # effect of ration\n## fit model\nBvec &lt;- coef(lm(yy ~ XX - 1))\nnames(Bvec) &lt;- c(\"beta_1\", \"beta_2\", \"beta_3\", \"beta_4\")\nBvec\n\n  beta_1   beta_2   beta_3   beta_4 \n10.35668 15.52218 20.71430  1.89973"
  }
]