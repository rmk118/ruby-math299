[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math 299",
    "section": "",
    "text": "Ruby Krasnow\nThis website will house my assignments as I work through the course material for QERM 514: Analysis of Ecological and Environmental Data, one of the core requirements for graduate students in the Quantitative Ecology & Resource Management (QERM) program at the University of Washington. The QERM 514 course materials developed and made publicly available online by Dr. Mark Scheuerell will serve as a foundation for a directed study I will complete during my final semester at Clark, supervised by Michael Satz (Clark University, Department of Mathematics)."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Math 299",
    "section": "",
    "text": "My name is Ruby Krasnow (she/her) and I am an aspiring quantitative marine ecologist interested in using modeling and data science to support sustainable fisheries management and marine aquaculture."
  },
  {
    "objectID": "HW/hw_01_intro_to_markdown.html",
    "href": "HW/hw_01_intro_to_markdown.html",
    "title": "HW1: Introduction",
    "section": "",
    "text": "Link to original assignment PDF\n\n1) Who am I & what do I study?\nI am an aspiring marine ecologist with a strong interest in sustainable fisheries and aquaculture. I am especially passionate about using mathematical and statistical models to understand aquatic ecosystems, support sustainable fisheries management, and advance the kelp and shellfish aquaculture industries. I am currently a senior at Clark University, where I am majoring in Biology with a minor in Mathematics. After graduating with my B.A. in December 2024, I will start my PhD in Marine Biology at the University of Maine. I am one of the team captains for the Clark cross-country team and am excited to continue my athletic as well as academic career by competing on the UMaine track and cross-country teams as a graduate student.\nA major focus of my research is enhancing the accuracy of kelp (Saccharina latissima) growth models by incorporating factors such as blade erosion, biofouling, genetic variation, and phenotypic plasticity. Additionally, I am collaborating with the NOAA Northeast Fisheries Science Center to model spatial variation in crustacean size-at-maturity, supporting the sustainable management of the emerging Jonah crab fishery in New England.\nTo learn more about me, you can visit my personal website: https://rmk118.github.io/\n\nKeywords\n\nquantitative marine ecology\nfisheries & aquaculture\n\nkelp aquaculture\nmodeling crustacean size-at-maturity\npopulation dynamics & stock assessment\n\nsynthesis research/meta-analyses\nopen & reproducible data science\n\n\n\n\n2) What do I want from this course?\n\nDevelop a more solid theoretical understanding of the concepts underlying the modeling techniques I frequently use in my research\nLearn new techniques for model selection and inference (e.g., practice cross-validation)\nBecome more familiar with distributions/models that can address common issues in ecological analyses, such as zero-inflated and hurdle models\n\n\n\n3) Plot air quality data\n\nggplot(air)+\n  geom_point(aes(x=wind, y=temp))+\n  theme_light()+ #define custom theme for ggplots\n  theme(\n    axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)),\n    axis.title.x = element_text(margin = margin(t = 10, r = 0, b = 0, l = 0)),\n    text=element_text(size=13))+labs(x=\"Wind speed (mph)\", y=\"Temperature (°F)\")+\n  xlim(0,NA)\n\n\n\n\nFigure 1: Avg. wind speed (mph) and max. daily temperature (°F) at LaGuardia Airport, NY, from May-Sept. 1973\n\n\n\n\n\n\n4) Say it with an equation\n\\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon  \\tag{1}\\] \\[ \\varepsilon \\sim N(0, \\sigma^2)\\]\nIn Equation 1, the two different predictor variables are represented by \\(X_1\\) and \\(X_2\\).\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#math-299-directed-study-environmental-data-analysis",
    "href": "index.html#math-299-directed-study-environmental-data-analysis",
    "title": "Math 299",
    "section": "",
    "text": "Ruby Krasnow\nThis website will house my assignments as I work through the course material for QERM 514: Analysis of Ecological and Environmental Data, one of the core requirements for graduate students in the Quantitative Ecology & Resource Management (QERM) program at the University of Washington. The QERM 514 course materials developed and made publicly available online by Dr. Mark Scheuerell will serve as a foundation for a directed study I will complete during my final semester at Clark, supervised by Michael Satz (Clark University, Department of Mathematics)."
  },
  {
    "objectID": "HW/hw_02_fitting_lm.html",
    "href": "HW/hw_02_fitting_lm.html",
    "title": "HW2: Linear Models",
    "section": "",
    "text": "Link to original assignment PDF\n\nBackground\nThe goal of this assignment is to familiarize yourself with fitting linear models in R. We will be working some data from nearby Lake Washington that is part of a long-term monitoring program begun in the 1960s by the late, and rather famous, Dr. W.T. Edmondson and since continued by Dr. Daniel Schindler. The accompanying data file L_Washington_plankton.csv contains information on the following four variables:\n\nDaphnia: index of the density of the cladoceran Daphnia (unitless)\nGreens: index of the density of green algae (unitless)\nCyclops: index of the density of the copepod Cyclops (unitless)\nTemp: water temperature (°C)\n\nDaphnia are an effective grazer on phytoplankton and green algae make up a large proportion of their diet. Cyclops are an inferior grazer compared to Daphnia, but a competitor nonetheless. Daphnia growth rates are also affected by water temperature.\n\n\nQuestion 1\n\nWrite out the equation for a linear regression model that expresses Daphnia abundance as a function of its preferred prey, green algae, and describe the terms in your model.\n\n\\[y_i =\\beta_0 + \\beta_1 x_i+e_i \\] Each observation of Daphnia abundance \\((y_i)\\) is a function of an intercept \\((\\beta_0)\\) and the effect \\((\\beta_1)\\) of green algae density \\((x_i)\\). The model residuals are assumed to be independent and normally distributed with mean 0 and variance \\(\\sigma^2\\), such that \\(e_i \\sim \\mathrm{N}(0, \\sigma^2)\\).\n\nProduce a scatterplot that shows the relationship between Daphnia and Greens. Make sure to label your plot accordingly and give it an informative caption. Describe the relationship between Daphnia and Greens. Does a linear model seem reasonable here?\n\n\n\n\n\n\nFigure 1: Relationship between the density of Daphnia and its preferred prey, green algae\n\n\n\n\nThere seems to be a weak positive relationship between green algae and Daphnia density that could be approximated with a linear model.\n\nProduce the step-by-step R code required to fit your model via linear algebra to generate estimates the model parameters and the data. Be sure to show the construction of the design matrix \\((\\mathbf{X})\\), the calculation of the parameter estimates \\((\\hat{\\beta_i})\\), the calculation of the hat matrix \\((\\mathbf{H})\\), and the calculation of the model predictions \\((\\hat{y_i})\\).\n\nFirst, we construct the design matrix:\n\nnn &lt;- nrow(daph)\nyy &lt;- matrix(data=daph$Daphnia, nrow = nn, ncol = 1)\nhead(yy)\n\n      [,1]\n[1,] -1.15\n[2,] -1.73\n[3,] -1.89\n[4,] -0.94\n[5,] -0.05\n[6,]  0.99\n\nintercept &lt;- rep(1, nn)\ngreens &lt;- daph$Greens\nXX &lt;- cbind(intercept, greens)\nhead(XX)\n\n     intercept greens\n[1,]         1  -1.32\n[2,]         1  -1.51\n[3,]         1  -2.48\n[4,]         1  -0.69\n[5,]         1   0.02\n[6,]         1   0.82\n\n\nUsing the design matrix, we can find the parameter estimates \\((\\hat{\\beta_i})\\) using the following formula: \\[\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{y}.\n\\]\n\nbeta_hat &lt;- solve(t(XX) %*% XX) %*% t(XX) %*% yy # or solve(crossprod(XX,XX),crossprod(XX,yy))\nbeta_hat\n\n                 [,1]\nintercept -0.04914691\ngreens     0.42112528\n\n#check answer\ndaph_mod &lt;- lm(data=daph, Daphnia~Greens)\ntidy(daph_mod) %&gt;% select(c(term, estimate))\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n-0.0491469\n\n\nGreens\n0.4211253\n\n\n\n\n\n\nNow we find the hat matrix \\((\\mathbf{H})\\):\n\n## hat matrix\nHH &lt;- XX %*% solve(t(XX) %*% XX) %*% t(XX)\ndim(HH)\n\n[1] 36 36\n\n# Check answer\nhat_auto &lt;- optR::hatMatrix(XX)\nall.equal(HH, hat_auto)\n\n[1] TRUE\n\n\nFinally, we find the model predictions \\((\\hat{y_i})\\):\n\ny_hat &lt;- HH %*% yy\n#or\ny_hat &lt;- XX %*% beta_hat\n\n# alternative built-in methods\npredict(daph_mod)\n\n           1            2            3            4            5            6 \n-0.605032288 -0.685046093 -1.093537619 -0.339723359 -0.040724407  0.296175821 \n           7            8            9           10           11           12 \n 0.969976277 -0.095470694 -0.318667095  0.098246938  0.409879648  0.009810628 \n          13           14           15           16           17           18 \n-0.082836935  0.216162017  0.506738464  0.536217234  0.056134409 -0.002823131 \n          19           20           21           22           23           24 \n 0.658343567  0.047711903 -0.015456889 -0.019668142 -0.074414429 -0.466060944 \n          25           26           27           28           29           30 \n 0.152993225 -0.592398530 -0.238653290 -0.440793427  0.430935913  0.161415730 \n          31           32           33           34           35           36 \n 0.334077097  0.110880696  0.169838236 -0.099681946  0.334077097 -0.078625682 \n\nhead(broom::augment(daph_mod)) # see fitted column\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDaphnia\nGreens\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n-1.15\n-1.32\n-0.6050323\n-0.5449677\n0.0916530\n0.8569811\n0.0228327\n-0.6727384\n\n\n-1.73\n-1.51\n-0.6850461\n-1.0449539\n0.1094821\n0.8409326\n0.1043338\n-1.3027983\n\n\n-1.89\n-2.48\n-1.0935376\n-0.7964624\n0.2346629\n0.8480611\n0.1758904\n-1.0711252\n\n\n-0.94\n-0.69\n-0.3397234\n-0.6002766\n0.0482177\n0.8560678\n0.0132742\n-0.7239090\n\n\n-0.05\n0.02\n-0.0407244\n-0.0092756\n0.0281488\n0.8627408\n0.0000018\n-0.0110699\n\n\n0.99\n0.82\n0.2961758\n0.6938242\n0.0422076\n0.8538700\n0.0153292\n0.8340942\n\n\n\n\n\n\n\nCalculate and report your estimate of the residual variance \\((\\sigma^2)\\).\n\nWe can use the following formula: \\[\\sigma^2 = \\frac{\\mathrm{RSS}}{n-p}, \\quad\\text{where}\\] \\[\\mathrm{RSS}= \\mathbf{e^\\top}\\mathbf{e}=\\left(\\mathbf{y}-X\\hat{\\beta}\\right)^\\top\\left(\\mathbf{y}-X\\hat{\\beta}\\right)\\]\n\nrss &lt;- as.numeric(crossprod(yy-XX %*% beta_hat) )\nrss\n\n[1] 24.56271\n\n# check answer\nall.equal(rss, deviance(daph_mod))\n\n[1] TRUE\n\nsigma2 &lt;- rss/(nn-2)\nsigma2\n\n[1] 0.7224325\n\n# check answer\nall.equal(sigma2, sigma(daph_mod)^2)\n\n[1] TRUE\n\n\n\nGive a prediction of what you might expect the specific abundance of Daphnia to be on the next sampling occasion if the abundance of green algae is 1.5 units. Also provide an estimate of the interval around your estimate that conveys 95% confidence in your prediction. Again, do so via direct calculations rather than relying on R’s built-in functions.\n\nFirst, we use the \\(\\beta_0\\) and \\(\\beta_1\\) values we found above to estimate \\(y\\) when \\(x=1.5\\).\n\n# y = b_0 + b_1 * x\ny_star &lt;- unname(beta_hat[1,1])+unname(beta_hat[2,1])*1.5\ny_star\n\n[1] 0.582541\n\n#OR\n\n#vector of new data\nX_star &lt;- c(intercept = 1, greens = 1.5)\nsum(X_star * beta_hat)\n\n[1] 0.582541\n\n\nNow we need to find the standard error of our estimate\n\n## inside sqrt\ninner_X &lt;- t(X_star) %*% solve(t(XX) %*% XX) %*% X_star\n\n## critical t-value\nt_crit &lt;- qt(0.975, df = nn-2)\n\n## estimated SD\nsigma &lt;- sqrt(sigma2)\n\n## 95% CI\ny_star + c(-1,1) * t_crit * sigma * c(sqrt(inner_X))\n\n[1] 0.07980506 1.08527698\n\n# for prediction interval, \ninner_X_pred &lt;- 1+ t(X_star) %*% solve(t(XX) %*% XX) %*% X_star\ny_star + c(-1,1) * t_crit * sigma * c(sqrt(inner_X_pred))\n\n[1] -1.216459  2.381541\n\n\nNow let’s check our answer using the built-in functions in R\n\npredict(daph_mod, new = data.frame(Greens=1.5),\n        level = 0.95, interval = \"confidence\")\n\n       fit        lwr      upr\n1 0.582541 0.07980506 1.085277\n\npredict(daph_mod, new = data.frame(Greens=1.5),\n        level = 0.95, interval = \"pred\")\n\n       fit       lwr      upr\n1 0.582541 -1.216459 2.381541\n\n\n\n\nQuestion 2\n\nExpand upon your model from Question 1 to include the additional effects of Cyclops and water temperature on Daphnia. Write out your equation and describe the terms in the model.\n\n\\[y_i =\\beta_0 + \\beta_1 x_{1,i}+\\beta_2 x_{2,i}+\\beta_3 x_{3,i}+e_i \\] Each observation of Daphnia abundance \\((y_i)\\) is a function of an intercept \\((\\beta_0)\\), the effect \\((\\beta_1)\\) of green algae density \\((x_{1,i})\\), the effect \\((\\beta_2)\\) of Cyclops density \\((x_{2,i})\\), and the effect \\((\\beta_3)\\) of water temperature \\((x_{3,i})\\). The model residuals are again assumed to be independent and normally distributed with mean 0 and variance \\(\\sigma^2\\), such that \\(e_i \\sim \\mathrm{N}(0, \\sigma^2)\\).\n\nUsing R’s built-in functions, fit the model from (a) and show the resulting table of results. For each of the \\(p\\)-values shown in the table, describe the null hypothesis being tested.\n\n\ndaphmod_full &lt;- lm(Daphnia ~ Greens + Cyclops + Temp, data=daph)\nsumary(daphmod_full)\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept) -4.143339   0.650218 -6.3722 3.711e-07\nGreens       0.046216   0.099249  0.4657 0.6446140\nCyclops      0.278715   0.075458  3.6936 0.0008212\nTemp         0.291675   0.045371  6.4286 3.157e-07\n\nn = 36, p = 4, Residual SE = 0.50245, R-Squared = 0.73\n\n\nThe first \\(p\\)-value corresponds to a t-test that the intercept is 0, \\(H_0: \\beta_0=0\\). The remaining three \\(p\\)-values shown in the table correspond to null hypothesis tests that the given predictor can be dropped from the model.\nFor example, can Cyclops be dropped from this model? We fit a reduced model and compare to it to the full model via an \\(F\\)-test with \\(H_0: \\beta_\\text{cyclops} = 0\\)\n\\[\n\\begin{aligned}\n\\Theta: \\text{Daphnia}_i &= \\beta_0 + \\beta_1 \\text{Greens}_i + \\beta_2 \\text{Cyclops}_i + \\beta_3 \\text{Temp}_i + e_i \\\\\n~ \\\\\n\\theta: \\text{Daphnia}_i &= \\beta_0 + \\beta_1 \\text{Greens}_i + \\beta_2 \\text{Temp}_i + e_i\n\\end{aligned}\n\\]\n\n## reduced model without `Cyclops`\nreduced_mod &lt;- lm(Daphnia ~ Greens + Temp, data=daph)\n## use `anova('reduced', 'full')` to get the F-test results\nanova(reduced_mod, daphmod_full)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n33\n11.522716\nNA\nNA\nNA\nNA\n\n\n32\n8.078507\n1\n3.444209\n13.64295\n0.0008212\n\n\n\n\n\n\nWe see that the resulting p-value is the same as the one displayed for the Cyclops estimate in the summary table for the full model.\n\nTest the hypothesis that \\(\\beta_{Greens} = \\beta_{Cyclops} = \\beta_{Temp} = 0\\). What is the \\(F\\)-statistic, the associated \\(df\\), and the \\(p\\)-value? What can you conclude from this test?\n\nWe write the null hypothesis as \\[H_0: \\beta_1 = \\beta_2 = \\beta_3 = 0\\] which corresponds to a model where we simply estimate the data based on their mean.\n\\[\\begin{align*}\n\\Theta: \\mathbf{y} &= \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{e} \\\\\n\\theta: \\mathbf{y} &= \\boldsymbol{\\mu} + \\mathbf{e} \\\\\n\\end{align*}\\]\nWe will base this test on an \\(F\\)-distribution, such that\n\\[\\begin{align*}\nSSE_{\\Theta} &= \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right)^{\\top} \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) = \\mathbf{e}^{\\top} \\mathbf{e} = SSE \\\\\nSSE_{\\theta} &= \\left( \\mathbf{y} - \\bar{y} \\right)^{\\top} \\left( \\mathbf{y} - \\bar{y} \\right) =  SSTO \\\\\n&\\Downarrow \\\\\nF &= \\frac{ \\left( SSTO - SSE \\right)  / (k - 1) } { SSE  / (n - k)}\n\\end{align*}\\]\n\n\\(F\\)-test by hand\n\n## get matrix of predictors\nXX &lt;- daph %&gt;% select(-Daphnia) %&gt;% add_column(Intercept=1, .before = \"Temp\") %&gt;% as.matrix()\n## estimate beta\nbeta_hat &lt;- solve(t(XX) %*% XX) %*% t(XX) %*% yy\n## total sum of squares\nSSE &lt;- t(yy - XX %*% beta_hat) %*% (yy - XX %*% beta_hat)\n## error sum of squares\nSSTO &lt;- t(yy - mean(yy)) %*% (yy - mean(yy))\n## F statistic\n(F_stat &lt;- ((SSTO - SSE) / (4 - 1)) / (SSE / (nn - 4)))\n\n         [,1]\n[1,] 29.47894\n\n## F test\npf(F_stat, 4-1, nn-4, lower.tail = F)\n\n             [,1]\n[1,] 2.467335e-09\n\n\nThis \\(F\\)-statistic is quite large and the \\(p\\)-value is very small, so we would reject the null hypothesis that we would be justified in dropping the 3 predictors from this model in favor of a mean-only model.\nChecking using the built-in summary function, we see that the F-statistic and p-value reported at the bottom of the output are the same as our manually calculated values.\n\nsummary(daphmod_full)\n\n\nCall:\nlm(formula = Daphnia ~ Greens + Cyclops + Temp, data = daph)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.74029 -0.41712 -0.04736  0.27781  1.16894 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -4.14334    0.65022  -6.372 3.71e-07 ***\nGreens       0.04622    0.09925   0.466 0.644614    \nCyclops      0.27871    0.07546   3.694 0.000821 ***\nTemp         0.29168    0.04537   6.429 3.16e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5024 on 32 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7094 \nF-statistic: 29.48 on 3 and 32 DF,  p-value: 2.467e-09\n\n\n\nIt has come to your attention that someone has done lab experiments suggesting the effect of temperature on Daphnia abundance is 0.4 per degree Celsius after controlling for the effects of prey (green algae) and competitors (Cyclops). Create a null hypothesis test to evaluate the evidence for this finding from the data collected in the field. Specify \\(H_0\\) and report the results of your test. What do you conclude?\n\n\\[\n\\text{Daphnia}_i = \\beta_0 + \\beta_1 \\text{Greens}_i + \\beta_2 \\text{Cyclops}_i + 0.4* \\text{Temp}_i + e_i\n\\]\nWe write the null hypothesis as \\[H_0: \\beta_{\\text{temp}} = 0.4\\]\n\n## model with effect of `elevation` = 1\nfixed_mod &lt;- lm(Daphnia ~ Greens + Cyclops + offset(0.4 * Temp), data=daph)\n## use `anova('comb', 'full')` to get the F-test results\nanova(fixed_mod, daphmod_full)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n33\n9.517552\nNA\nNA\nNA\nNA\n\n\n32\n8.078507\n1\n1.439045\n5.700241\n0.0230387\n\n\n\n\n\n\nWe have found significant evidence to reject \\(H_0\\), suggesting that it may be worthwhile to re-run the lab experiments demonstrating the effect of temperature on Daphnia abundance is 0.4/°C after controlling for green algae and Cyclops density and determine if the findings can be replicated.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Labs/lab1.html",
    "href": "Labs/lab1.html",
    "title": "Lecture & Lab 1: Fitting Linear Models",
    "section": "",
    "text": "Faraway, Ch. 1 & 2\nMatrix math cheat sheet\n\ndata(\"gala\")\nhead(gala) # or glimpse(gala)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\nEndemics\nArea\nElevation\nNearest\nScruz\nAdjacent\n\n\n\n\nBaltra\n58\n23\n25.09\n346\n0.6\n0.6\n1.84\n\n\nBartolome\n31\n21\n1.24\n109\n0.6\n26.3\n572.33\n\n\nCaldwell\n3\n3\n0.21\n114\n2.8\n58.7\n0.78\n\n\nChampion\n25\n9\n0.10\n46\n1.9\n47.4\n0.18\n\n\nCoamano\n2\n1\n0.05\n77\n1.9\n1.9\n903.82\n\n\nDaphne.Major\n18\n11\n0.34\n119\n8.0\n8.0\n1.84\n\n\n\n\n\ngala &lt;- gala %&gt;% select(-Endemics) %&gt;% clean_names()\n\nlmod &lt;- lm(species ~ area + elevation + nearest + scruz + adjacent, data = gala)\n\nsummary(lmod) # or faraway::sumary(lmod) or broom::tidy(lmod)\n\n\nCall:\nlm(formula = species ~ area + elevation + nearest + scruz + adjacent, \n    data = gala)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-111.679  -34.898   -7.862   33.460  182.584 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.068221  19.154198   0.369 0.715351    \narea        -0.023938   0.022422  -1.068 0.296318    \nelevation    0.319465   0.053663   5.953 3.82e-06 ***\nnearest      0.009144   1.054136   0.009 0.993151    \nscruz       -0.240524   0.215402  -1.117 0.275208    \nadjacent    -0.074805   0.017700  -4.226 0.000297 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 60.98 on 24 degrees of freedom\nMultiple R-squared:  0.7658,    Adjusted R-squared:  0.7171 \nF-statistic:  15.7 on 5 and 24 DF,  p-value: 6.838e-07\n\nx &lt;- model.matrix(~ area + elevation + nearest + scruz + adjacent, data = gala)\ny &lt;- gala$species\n  \nxtxi &lt;- solve(t(x) %*% x)\nxtxi %*% t(x) %*% y\n\n                    [,1]\n(Intercept)  7.068220709\narea        -0.023938338\nelevation    0.319464761\nnearest      0.009143961\nscruz       -0.240524230\nadjacent    -0.074804832\n\nsolve(crossprod(x,x),crossprod(x,y))\n\n                    [,1]\n(Intercept)  7.068220709\narea        -0.023938338\nelevation    0.319464761\nnearest      0.009143961\nscruz       -0.240524230\nadjacent    -0.074804832\n\ndeviance(lmod) #RSS\n\n[1] 89231.37\n\nsqrt(deviance(lmod)/df.residual(lmod)) #sigma\n\n[1] 60.97519\n\nsigma &lt;- summary(lmod)$sigma\n\nxtxi &lt;- summary(lmod)$cov.unscaled\n\n#standard errors of the coefficients\nsqrt(diag(xtxi))*sigma #OR\n\n(Intercept)        area   elevation     nearest       scruz    adjacent \n19.15419782  0.02242235  0.05366280  1.05413595  0.21540225  0.01770019 \n\nsummary(lmod)$coef[,2]\n\n(Intercept)        area   elevation     nearest       scruz    adjacent \n19.15419782  0.02242235  0.05366280  1.05413595  0.21540225  0.01770019 \n\n\n\n\n\n\n\ndata(\"teengamb\")\n\ngambmod &lt;- lm(gamble ~ sex + status + income + verbal, data = teengamb)\nmodsum&lt;- summary(gambmod)\nmodsum\n\n\nCall:\nlm(formula = gamble ~ sex + status + income + verbal, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.082 -11.320  -1.451   9.452  94.252 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22.55565   17.19680   1.312   0.1968    \nsex         -22.11833    8.21111  -2.694   0.0101 *  \nstatus        0.05223    0.28111   0.186   0.8535    \nincome        4.96198    1.02539   4.839 1.79e-05 ***\nverbal       -2.95949    2.17215  -1.362   0.1803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.69 on 42 degrees of freedom\nMultiple R-squared:  0.5267,    Adjusted R-squared:  0.4816 \nF-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06\n\n#A\nmodsum$r.squared\n\n[1] 0.5267234\n\n#B\nunname(which.max(modsum$residuals))\n\n[1] 24\n\n#C\nmean(modsum$residuals)\n\n[1] -2.26769e-16\n\nmedian(modsum$residuals)\n\n[1] -1.451392\n\n#D\ncor(gambmod$fitted.values, modsum$residuals)\n\n[1] -5.79346e-17\n\nplot(gambmod, which = 1) # or DHARMa::plotConventionalResiduals(gambmod)\n\n\n\nDHARMa::plotQQunif(gambmod)\n\n\n\n#E\ncor(modsum$residuals, teengamb$gamble)\n\n[1] 0.687951\n\n#F \nabs(modsum$coefficients[\"sex\", \"Estimate\"])\n\n[1] 22.11833\n\nemmeans::emmeans(gambmod, \"sex\")\n\n sex emmean   SE df lower.CL upper.CL\n   0  28.24 4.69 42     18.8     37.7\n   1   6.12 5.91 42     -5.8     18.0\n\nConfidence level used: 0.95 \n\nemmplot &lt;- plot(emmeans(gambmod, \"sex\"), colors=c(\"#003087\")) # or marginal_effects::plot_predictions(gambmod, condition = \"sex\")\nemmplot+\n  mytheme+\n  labs(y=\"Sex\", x=\"Estimated marginal mean\")\n\n\n\n\n\n\n\n\ndata(\"uswages\")\n\nwagemod &lt;- lm(wage ~ educ + exper, data=uswages)\ntidy(wagemod)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-242.799412\n50.6815927\n-4.790682\n1.8e-06\n\n\neduc\n51.175268\n3.3419257\n15.313108\n0.0e+00\n\n\nexper\n9.774767\n0.7505955\n13.022683\n0.0e+00\n\n\n\n\n\nwagemod$coefficients[\"educ\"]\n\n    educ \n51.17527 \n\n# Each additional year of education increases the predicted weekly wage by around $51\n\nlog_wagemod &lt;- lm(log(wage) ~ educ + exper, data=uswages)\nsummary(log_wagemod)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper, data = uswages)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7533 -0.3495  0.1068  0.4381  3.5699 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.650319   0.078354   59.35   &lt;2e-16 ***\neduc        0.090506   0.005167   17.52   &lt;2e-16 ***\nexper       0.018079   0.001160   15.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6615 on 1997 degrees of freedom\nMultiple R-squared:  0.1749,    Adjusted R-squared:  0.174 \nF-statistic: 211.6 on 2 and 1997 DF,  p-value: &lt; 2.2e-16\n\ncompare_performance(wagemod, log_wagemod)\n\nSome of the nested models seem to be identical\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC\nAIC_wt\nAICc\nAICc_wt\nBIC\nBIC_wt\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\nwagemod\nlm\n29915.88\n0\n29915.9\n0\n29938.28\n0\n0.1351186\n0.1342524\n427.532832\n427.8538424\n\n\nlog_wagemod\nlm\n28706.68\n1\n28706.7\n1\n28729.08\n1\n0.1748605\n0.1740342\n0.660967\n0.6614633\n\n\n\n\n\ncheck_model(wagemod)\n\n\n\ncheck_model(log_wagemod)\n\n\n\n\nAlthough interpretation of the coefficients is less straightforward for the log-transformed model, it has much lower AIC/AICc and RMSE values and a higher R-squared, indicating it is a better fit for the data. The posterior predictive checks also look much better for the log model."
  },
  {
    "objectID": "Labs/lab1.html#exercises",
    "href": "Labs/lab1.html#exercises",
    "title": "Lecture & Lab 1: Fitting Linear Models",
    "section": "",
    "text": "data(\"teengamb\")\n\ngambmod &lt;- lm(gamble ~ sex + status + income + verbal, data = teengamb)\nmodsum&lt;- summary(gambmod)\nmodsum\n\n\nCall:\nlm(formula = gamble ~ sex + status + income + verbal, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.082 -11.320  -1.451   9.452  94.252 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  22.55565   17.19680   1.312   0.1968    \nsex         -22.11833    8.21111  -2.694   0.0101 *  \nstatus        0.05223    0.28111   0.186   0.8535    \nincome        4.96198    1.02539   4.839 1.79e-05 ***\nverbal       -2.95949    2.17215  -1.362   0.1803    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 22.69 on 42 degrees of freedom\nMultiple R-squared:  0.5267,    Adjusted R-squared:  0.4816 \nF-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06\n\n#A\nmodsum$r.squared\n\n[1] 0.5267234\n\n#B\nunname(which.max(modsum$residuals))\n\n[1] 24\n\n#C\nmean(modsum$residuals)\n\n[1] -2.26769e-16\n\nmedian(modsum$residuals)\n\n[1] -1.451392\n\n#D\ncor(gambmod$fitted.values, modsum$residuals)\n\n[1] -5.79346e-17\n\nplot(gambmod, which = 1) # or DHARMa::plotConventionalResiduals(gambmod)\n\n\n\nDHARMa::plotQQunif(gambmod)\n\n\n\n#E\ncor(modsum$residuals, teengamb$gamble)\n\n[1] 0.687951\n\n#F \nabs(modsum$coefficients[\"sex\", \"Estimate\"])\n\n[1] 22.11833\n\nemmeans::emmeans(gambmod, \"sex\")\n\n sex emmean   SE df lower.CL upper.CL\n   0  28.24 4.69 42     18.8     37.7\n   1   6.12 5.91 42     -5.8     18.0\n\nConfidence level used: 0.95 \n\nemmplot &lt;- plot(emmeans(gambmod, \"sex\"), colors=c(\"#003087\")) # or marginal_effects::plot_predictions(gambmod, condition = \"sex\")\nemmplot+\n  mytheme+\n  labs(y=\"Sex\", x=\"Estimated marginal mean\")\n\n\n\n\n\n\n\n\ndata(\"uswages\")\n\nwagemod &lt;- lm(wage ~ educ + exper, data=uswages)\ntidy(wagemod)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-242.799412\n50.6815927\n-4.790682\n1.8e-06\n\n\neduc\n51.175268\n3.3419257\n15.313108\n0.0e+00\n\n\nexper\n9.774767\n0.7505955\n13.022683\n0.0e+00\n\n\n\n\n\nwagemod$coefficients[\"educ\"]\n\n    educ \n51.17527 \n\n# Each additional year of education increases the predicted weekly wage by around $51\n\nlog_wagemod &lt;- lm(log(wage) ~ educ + exper, data=uswages)\nsummary(log_wagemod)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper, data = uswages)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.7533 -0.3495  0.1068  0.4381  3.5699 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4.650319   0.078354   59.35   &lt;2e-16 ***\neduc        0.090506   0.005167   17.52   &lt;2e-16 ***\nexper       0.018079   0.001160   15.58   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6615 on 1997 degrees of freedom\nMultiple R-squared:  0.1749,    Adjusted R-squared:  0.174 \nF-statistic: 211.6 on 2 and 1997 DF,  p-value: &lt; 2.2e-16\n\ncompare_performance(wagemod, log_wagemod)\n\nSome of the nested models seem to be identical\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC\nAIC_wt\nAICc\nAICc_wt\nBIC\nBIC_wt\nR2\nR2_adjusted\nRMSE\nSigma\n\n\n\n\nwagemod\nlm\n29915.88\n0\n29915.9\n0\n29938.28\n0\n0.1351186\n0.1342524\n427.532832\n427.8538424\n\n\nlog_wagemod\nlm\n28706.68\n1\n28706.7\n1\n28729.08\n1\n0.1748605\n0.1740342\n0.660967\n0.6614633\n\n\n\n\n\ncheck_model(wagemod)\n\n\n\ncheck_model(log_wagemod)\n\n\n\n\nAlthough interpretation of the coefficients is less straightforward for the log-transformed model, it has much lower AIC/AICc and RMSE values and a higher R-squared, indicating it is a better fit for the data. The posterior predictive checks also look much better for the log model."
  },
  {
    "objectID": "Labs/lab1.html#simple-model",
    "href": "Labs/lab1.html#simple-model",
    "title": "Lecture & Lab 1: Fitting Linear Models",
    "section": "2.1 Simple model",
    "text": "2.1 Simple model\n\n2.1.1 Estimating beta-hat and y-hat\n\nnn &lt;- nrow(gala)\nyy &lt;- matrix(data=gala$species, nrow = nn, ncol = 1)\nhead(yy)\n\n     [,1]\n[1,]   58\n[2,]   31\n[3,]    3\n[4,]   25\n[5,]    2\n[6,]   18\n\nintercept &lt;- rep(1, nn)\narea &lt;- gala$area\nXX &lt;- cbind(intercept, area)\nhead(XX)\n\n     intercept  area\n[1,]         1 25.09\n[2,]         1  1.24\n[3,]         1  0.21\n[4,]         1  0.10\n[5,]         1  0.05\n[6,]         1  0.34\n\nbeta_hat &lt;- solve(t(XX) %*% XX) %*% t(XX) %*% yy\nbeta_hat\n\n                 [,1]\nintercept 63.78286147\narea       0.08196317\n\nHH &lt;- XX %*% solve(t(XX) %*% XX) %*% t(XX)\ny_hat &lt;- HH %*% yy\nhead(y_hat)\n\n         [,1]\n[1,] 65.83932\n[2,] 63.88450\n[3,] 63.80007\n[4,] 63.79106\n[5,] 63.78696\n[6,] 63.81073\n\nhead(XX %*% beta_hat)\n\n         [,1]\n[1,] 65.83932\n[2,] 63.88450\n[3,] 63.80007\n[4,] 63.79106\n[5,] 63.78696\n[6,] 63.81073\n\n\n\nggplot(gala, aes(x=area, y=species))+\n  geom_point()+\n  mytheme+\n  labs(x=expression(\"Area of island (\"*km^2*\")\"), y=\"Number of species\")+\n  geom_smooth(method=\"lm\", se=FALSE, formula = y~x, color=\"black\", linewidth=0.5)\n\n\n\n\n\n\n2.1.2 Automated model fitting\n\nsimple_model &lt;- lm(species ~ area, gala)\nsimple_model\n\n\nCall:\nlm(formula = species ~ area, data = gala)\n\nCoefficients:\n(Intercept)         area  \n   63.78286      0.08196  \n\n## via fitted\ny_hat_f &lt;- fitted(simple_model)\n## via predict\ny_hat_p &lt;- predict(simple_model, type = \"response\")\n## compare these to each other\nall.equal(y_hat_f, y_hat_p)\n\n[1] TRUE\n\n\n\n\n2.1.3 Goodness-of-fit\n\n## SSE\nresids &lt;- yy - y_hat\nSSE &lt;- sum(resids^2) # = t(resids) %*% resids\n\n## SSTO\nSSTO &lt;- sum((yy - mean(yy))^2)\n\n## R^2\n1 - SSE / SSTO\n\n[1] 0.3817301\n\nsummary(simple_model)\n\n\nCall:\nlm(formula = species ~ area, data = gala)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-99.495 -53.431 -29.045   3.423 306.137 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 63.78286   17.52442   3.640 0.001094 ** \narea         0.08196    0.01971   4.158 0.000275 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 91.73 on 28 degrees of freedom\nMultiple R-squared:  0.3817,    Adjusted R-squared:  0.3596 \nF-statistic: 17.29 on 1 and 28 DF,  p-value: 0.0002748\n\n\n\n\n2.1.4 Adjusted R-squared\n\nset.seed(514)\n## generate a vector of Gaussian white noise\nWN &lt;- rnorm(nn)\n## add this to our Galapagos data frame\ngala$WN &lt;- WN \n## fit a model with Area & WN\nsummary(lm(species ~ area + WN, gala))\n\n\nCall:\nlm(formula = species ~ area + WN, data = gala)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-93.86 -44.67 -21.73  15.93 308.84 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  70.68115   17.82480   3.965 0.000485 ***\narea          0.07892    0.01944   4.059 0.000378 ***\nWN          -20.27151   13.91996  -1.456 0.156843    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 89.95 on 27 degrees of freedom\nMultiple R-squared:  0.4268,    Adjusted R-squared:  0.3843 \nF-statistic: 10.05 on 2 and 27 DF,  p-value: 0.0005465\n\n1 - (SSE / (nn - 2)) / (SSTO / (nn - 1)) #adj r2\n\n[1] 0.359649"
  },
  {
    "objectID": "Labs/lab1.html#better-model-lab-example",
    "href": "Labs/lab1.html#better-model-lab-example",
    "title": "Lecture & Lab 1: Fitting Linear Models",
    "section": "2.2 Better model (lab example)",
    "text": "2.2 Better model (lab example)\n\n## larger model\nfull_mod &lt;- lm(species ~ area + elevation + nearest, gala)\nfull_mod\n\n\nCall:\nlm(formula = species ~ area + elevation + nearest, data = gala)\n\nCoefficients:\n(Intercept)         area    elevation      nearest  \n   16.46471      0.01908      0.17134      0.07123  \n\n\n\n## get matrix of predictors\nXX &lt;- model.matrix(full_mod)\n## estimate beta\nbeta_hat &lt;- solve(t(XX) %*% XX) %*% t(XX) %*% yy\n## total sum of squares\nSSE &lt;- t(yy - XX %*% beta_hat) %*% (yy - XX %*% beta_hat)\n## error sum of squares\nSSTO &lt;- t(yy - mean(yy)) %*% (yy - mean(yy))\n## F statistic\nF_stat &lt;- ((SSTO - SSE) / (4 - 1)) / (SSE / (nn - 4))\npf(F_stat, 4-1, nn-4, lower.tail = F) #F-test\n\n             [,1]\n[1,] 8.816845e-05\n\n\n\n## null model; the '1' indicates an intercept-only model\nnull_mod &lt;- lm(species ~ 1, gala)\n## use `anova('simple', 'complex')` to get the F-test results\nanova(null_mod, full_mod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n29\n381081.4\nNA\nNA\nNA\nNA\n\n\n26\n169917.6\n3\n211163.8\n10.77043\n8.82e-05\n\n\n\n\n\nfull_mod_sum&lt;-summary(full_mod)\n\npf(full_mod_sum$fstatistic[1], full_mod_sum$fstatistic[2], full_mod_sum$fstatistic[3], lower.tail = F)\n\n       value \n8.816845e-05 \n\n\n\n## reduced model without `nearest`\nreduced_mod &lt;- lm(species ~ area + elevation, gala)\n## use `anova('reduced', 'full')` to get the F-test results\nanova(reduced_mod, full_mod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n27\n169946.8\nNA\nNA\nNA\nNA\n\n\n26\n169917.6\n1\n29.24263\n0.0044746\n0.9471792\n\n\n\n\n\n\n\n2.2.1 Testing a subspace\n\n## full model (with adjacent this time)\nfull_mod2 &lt;- lm(species ~ area + adjacent + elevation + nearest, gala)\n## reduced model without `elevation + nearest`\ncomb_mod &lt;- lm(species ~ I(area + adjacent) + elevation + nearest, gala)\n## use `anova('combined', 'full')` to get the F-test results\nanova(comb_mod, full_mod2)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n26\n116865.40\nNA\nNA\nNA\nNA\n\n\n25\n93867.15\n1\n22998.25\n6.125212\n0.0204613\n\n\n\n\n\n\n\n## model with effect of `elevation` = 1\nfixed_mod &lt;- lm(species ~ area + offset(1 * elevation) + nearest, gala)\n## use `anova('comb', 'full')` to get the F-test results\nanova(fixed_mod, full_mod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n27\n1679730.7\nNA\nNA\nNA\nNA\n\n\n26\n169917.6\n1\n1509813\n231.0246\n0\n\n\n\n\n\n\n\nsumary(full_mod)\n\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 16.464711  23.388841  0.7040 0.487718\narea         0.019085   0.026764  0.7131 0.482158\nelevation    0.171336   0.054519  3.1427 0.004151\nnearest      0.071227   1.064806  0.0669 0.947179\n\nn = 30, p = 4, Residual SE = 80.84116, R-Squared = 0.55\n\n## t statistic\n(t_value &lt;- (0.171336 - 1) / 0.054519)\n\n[1] -15.19955\n\n## p-value = t_alpha * Pr(t_value, df); `pt()` is the pdf for a t-dist\n(p_value &lt;- 1.96 * pt(t_value, 26))\n\n[1] 1.853024e-14\n\n## verify t^2 = F\nall.equal(t_value^2, anova(fixed_mod, full_mod)$F[2], tolerance = 0.0001)\n\n[1] TRUE\n\n\n\n\n2.2.2 CIs for beta-hat\n\n## critical value for the t-dist\n## `qt()` is the quantile function for the t-dist; `p` is the (1-alpha/2) value \nt_crit &lt;- qt(p = 0.975, df = 30-4)\n## 95% CI\nCI95_beta &lt;- 0.019085 + c(-1,1) * t_crit * 0.026764\nround(CI95_beta, 3)\n\n[1] -0.036  0.074\n\n\n\n## all of the 95% CI's\nconfint(full_mod)\n\n                   2.5 %      97.5 %\n(Intercept) -31.61174063 64.54116288\narea         -0.03593020  0.07409948\nelevation     0.05927051  0.28340204\nnearest      -2.11751249  2.25996697\n\n\n\n\n2.2.3 Bootstrap confidence intervals\n\n## residuals from our full model\nresids &lt;- residuals(full_mod)\n\n## number of bootstrap samples\nnb &lt;- 1000\n## empty matrix for beta estimates\nbeta_est &lt;- matrix(NA, nb, 4)\n## fitted values from our full model = X*beta\nXbeta &lt;- fitted(full_mod)\n## sample many times\nfor(i in 1:nb) {\n  ## 3a: sample w/ replacement from e\n  e_star &lt;- sample(resids, rep = TRUE)\n  ## 3b: calculate y_star\n  y_star &lt;- Xbeta + e_star\n  ## 3c: re-estimate beta_star from X & y_star\n  beta_star &lt;- update(full_mod, y_star ~ .)\n  ## save estimated betas\n  beta_est[i,] &lt;- coef(beta_star)\n}\n\n## extract 2.5% and 97.5% values\nCI95 &lt;- apply(beta_est, 2, quantile, c(0.025, 0.975))\ncolnames(CI95) &lt;- c(\"Intercept\", \"area\", \"elevation\" , \"nearest\")\nt(round(CI95, 3))\n\n             2.5%  97.5%\nIntercept -22.402 61.470\narea       -0.027  0.084\nelevation   0.073  0.277\nnearest    -1.918  2.150\n\n\n\n\n2.2.4 Boostrap coefficients and CIs using the rsample package\n\nset.seed(462)\nlibrary(rsample)\n\n# Will be used to fit the models to different bootstrap data sets:\nfit_fun &lt;- function(split, ...) {\n  # We could check for convergence, make new parameters, etc.\n  lm(species ~ area + elevation + nearest, data = analysis(split), ...) %&gt;%\n    tidy()\n}\n\nbt &lt;-\n  bootstraps(gala, times = 1000, apparent = TRUE) %&gt;%\n  mutate(models = map(splits, fit_fun))\n\nint_pctl(bt, models)\n\n\n\n\n\nterm\n.lower\n.estimate\n.upper\n.alpha\n.method\n\n\n\n\n(Intercept)\n-33.7640985\n16.8546860\n57.8416355\n0.05\npercentile\n\n\narea\n-0.0911716\n0.1138601\n0.5206100\n0.05\npercentile\n\n\nelevation\n-0.1372787\n0.1516931\n0.4687342\n0.05\npercentile\n\n\nnearest\n-2.4452623\n-0.1349482\n1.8394215\n0.05\npercentile\n\n\n\n\n\n\n\n\n2.2.5 Confidence interval for new predictions\n\n2.2.5.1 By hand\n\n## matrix of predictors\nXX &lt;- model.matrix(simple_model)\n## new X; vector for now\nX_star &lt;- c(intercept = 1, area = 2000)\n## inside sqrt\ninner_X &lt;- t(X_star) %*% solve(t(XX) %*% XX) %*% X_star\n## critical t-value\nt_crit &lt;- qt(0.975, df = nn-2)\n## estimated SD\nsigma &lt;- summary(simple_model)$sigma\n## predicted y\ny_star &lt;- sum(X_star * coef(simple_model))\n## 95% CI\nc(y_star) + c(-1,1) * c(t_crit) * c(sigma) * c(sqrt(inner_X))\n\n[1] 149.5818 305.8366\n\n\n\n\n2.2.5.2 Using predict\n\npredict(simple_model, new = data.frame(t(X_star)),\n        level = 0.95, interval = \"confidence\")\n\n       fit      lwr      upr\n1 227.7092 149.5818 305.8366\n\n\n\n\n\n2.2.6 Prediction interval for new response\n\n## new X_star\nX_star &lt;- c(intercept = 1, area = 2000)\n## inside sqrt\ninner_X &lt;- 1 + t(X_star) %*% solve(t(XX) %*% XX) %*% X_star\n## 95% CI\ny_star + c(-1,1) * c(t_crit) * c(sigma) * c(sqrt(inner_X))\n\n[1]  24.21065 431.20776\n\npredict(simple_model, new = data.frame(t(X_star)),\n        level = 0.95, interval = \"prediction\")\n\n       fit      lwr      upr\n1 227.7092 24.21065 431.2078"
  },
  {
    "objectID": "Notes/Inference.html",
    "href": "Notes/Inference.html",
    "title": "Inference, Diagnostics, & Errors",
    "section": "",
    "text": "# List of packages required:\npackages &lt;- c(\"tidyverse\", \"PNWColors\", \"janitor\", \"faraway\", \"broom\", \"DHARMa\", \"emmeans\", \"performance\", \"ellipse\")\n\n# Load packages into session\nlapply(packages, require, character.only = TRUE)\nrm(packages)\n\n# Ensure functions with duplicate names are from the correct package\nselect &lt;- dplyr::select\nmap &lt;- purrr::map\nsummarize &lt;- dplyr::summarize\nclean_names &lt;- janitor::clean_names\nmargin &lt;- ggplot2::margin\n\nset.seed(123) #Set seed for pseudo-random number generator, for reproducibility\n\nmytheme &lt;- theme_light()+ #define custom theme for ggplots\n  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, l = 0)),\n        axis.title.x = element_text(margin = margin(t = 10, l = 0)),\n        text=element_text(size=15))\n\ndata(\"gala\")"
  },
  {
    "objectID": "HW/hw_03_diagnostics.html",
    "href": "HW/hw_03_diagnostics.html",
    "title": "HW 3: Model Diagnostics",
    "section": "",
    "text": "Background\nSection 7 of the U.S. Endangered Species Act (ESA) regulates situations in which a federal agency funds, permits, or otherwise has a “federal nexus” on any project that may influence a protected species. Federal agencies must seek a “consultation” on the project with either the U.S. Fish and Wildlife Service (USFWS) or the National Marine Fisheries Service (NMFS), depending on the species, and USFWS or NMFS must assure that any project does not cause “jeopardy” (a relatively high legal standard) for a protected species. A major conservation value of Section 7 consultation is the opportunity for USFWS and NMFS biologists to negotiate changes to projects that could minimize any negative impacts on species (or maximize any positive benefits).\nThe USFWS office in Lacey, Washington wanted to identify the characteristics of projects that would make them worthwhile for focused consultation time, with an emphasis on projects potentially impacting ESA-listed bull trout (Salvelinus confluentus). Experts developed assessments of the potential improvement(s) in a project that could be realized from negotiating changes to projects such as nearshore construction, culvert improvements, and riparian restoration. These assessment generated a unitless score of the potential value for 38 projects.\nAt this point the USFWS would like your assistance in evaluating a statistical model they hope to use for prioritizing project consultations. The accompanying data file usfws_bull_trout.csv contains 9 columns of information. They are\n\nscore: a project’s potential value (numerical score on a scale of 0-15)\nstage: 1 of 3 life history stage(s) occurring in the project area\n\nadults (A)\njuveniles/adults (JA)\neggs/juveniles (EJ)\n\nform: 1 of 2 life history form(s) occurring in the project area\n\nanadromous (An)\nfluvial/anadromous (FlAn)\n\ncond: 1 of 3 habitat conditions in the project area\n\npristine (P)\ndegraded (D)\nhighly degraded (H)\n\nrisk: 1 of 4 levels of extinction risk of the core population occurring in the project area\n\noutside core area (OC)\nlow (L)\nmedium (M)\nhigh (H)\n\nunit: 1 of 4 habitat unit types in the project area\n\ninside a core area (IC)\noutside a core area in freshwater (OF)\nmarine (M)\nother (OT)\n\nprog: whether or not the set of detailed management guidelines for projects of that type have been established\n\nYes\nNo\n\nBMP: whether or not established best management practices will be followed in the project\n\nYes\nNo\n\ndegflex: the degree of flexibility in project design, timing, and location\n\nlow (L)\nmedium (M)\n\n\nAs you work through the following problems, make sure to explain your thought process and show all of your R code, so Mark can give you partial credit, if necessary.\n\n\nProblems\n\nFit a linear model to the dataset that includes all 8 predictor variables. What is the \\(R^2\\) for the model? Does is seem like a promising model?\n\n\nfws&lt;- read.csv(\"../data/usfws_bull_trout.csv\")\n\nfish &lt;- lm(score ~ stage+form+cond+risk+unit+prog+BMP+degflex, data=fws)\n\nsummary(fish)\n\n\nCall:\nlm(formula = score ~ stage + form + cond + risk + unit + prog + \n    BMP + degflex, data = fws)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8393 -0.4525  0.0465  0.6257  5.0838 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.27852    3.12978   1.048 0.305742    \nstageEJ      6.54996    1.67547   3.909 0.000704 ***\nstageJA      5.66124    1.48850   3.803 0.000916 ***\nformFlAn    -0.63393    2.38959  -0.265 0.793151    \ncondH        0.62407    1.10026   0.567 0.576072    \ncondP       -0.96800    1.55225  -0.624 0.539018    \nriskL        1.48544    2.70158   0.550 0.587728    \nriskM        0.64192    1.79215   0.358 0.723471    \nriskOC      -1.94297    4.11150  -0.473 0.640974    \nunitM        2.29973    3.26551   0.704 0.488348    \nunitOF       1.49315    2.84772   0.524 0.605065    \nunitOT       0.36718    3.17158   0.116 0.908839    \nprogYes     -1.98726    1.05228  -1.889 0.071633 .  \nBMPYes       0.06842    1.34844   0.051 0.959970    \ndegflexM     2.68433    0.91894   2.921 0.007685 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.152 on 23 degrees of freedom\nMultiple R-squared:  0.7549,    Adjusted R-squared:  0.6058 \nF-statistic: 5.061 on 14 and 23 DF,  p-value: 0.000314\n\nanova(fish)\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nstage\n2\n181.5597719\n90.7798860\n19.6083127\n0.0000107\n\n\nform\n1\n11.6307302\n11.6307302\n2.5122194\n0.1266212\n\n\ncond\n2\n3.6778367\n1.8389184\n0.3972035\n0.6767199\n\n\nrisk\n3\n36.6117530\n12.2039177\n2.6360270\n0.0738824\n\n\nunit\n3\n21.5314509\n7.1771503\n1.5502532\n0.2284705\n\n\nprog\n1\n33.4703391\n33.4703391\n7.2295406\n0.0131082\n\n\nBMP\n1\n0.0287882\n0.0287882\n0.0062182\n0.9378296\n\n\ndegflex\n1\n39.5041792\n39.5041792\n8.5328406\n0.0076851\n\n\nResiduals\n23\n106.4822561\n4.6296633\nNA\nNA\n\n\n\n\n\nglance(fish)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.7549299\n0.6057568\n2.151665\n5.060764\n0.000314\n14\n-73.49712\n178.9942\n205.1956\n106.4823\n23\n38\n\n\n\n\n\n\nThe \\(R^2\\) for the model is around 0.755. While this is relatively high, the anova() table suggests that some of the predictors may not be necessary.\n\nMake a plot of the residuals against the model predictions. Name at least two things you should be looking for in a plot like this. What do you see?\n\n\n# base R\nplot(fish, which=1)\n\n\n\n# ggplot\nres_fit &lt;- data.frame(res = fish$residuals, fit = fish$fitted.values)\n\nggplot(res_fit)+\n  geom_point(aes(x=fit, y=res))+\n  labs(x=\"Model predictions\", y=\"Residuals\")+\n  geom_hline(yintercept = 0, lty=2)+mytheme\n\n\n\n\nThings to look for:\n\nNo obvious nonlinear patterns\nConstant symmetrical variation (homoscedasticity) in the vertical (\\(\\hat{\\varepsilon}\\)) direction (i.e., vertical spread of the points does not increase or decrease with the x-values)\n\nAlthough there are no clear nonlinear patterns suggesting nonlinearity in the structural part of the model, the magnitude of the residuals appears to be greater for points with larger predicted values, suggesting possible heteroscedasticity.\n\nAdditional content\nSince the diagnostic plot has indicated potential heteroscedasticity, we can confirm our suspicion with a formal diagnostic test like the Breusch-Pagan test.\nThe check_heteroscedasticity function from the performance package computes the BP-test non-studentized and uses the fitted values for the potential explanatory variables.\n\ncheck_heteroscedasticity(fish)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.002).\n\n\nThis function has the additional benefit of a built-in plot method:\n\ncheck_heteroscedasticity(fish) %&gt;% plot()\n\n\n\n\nAs a default, the bptest function from the lmtest package uses the same explanatory variables as in the regression model you specify (and studentizes as well).\n\nlmtest::bptest(fish, studentize = TRUE) #studentized Breusch-Pagan test\n\n\n    studentized Breusch-Pagan test\n\ndata:  fish\nBP = 17.194, df = 14, p-value = 0.246\n\nlmtest::bptest(fish, studentize = FALSE) #Breusch-Pagan test\n\n\n    Breusch-Pagan test\n\ndata:  fish\nBP = 39.311, df = 14, p-value = 0.0003265\n\n\nWe can obtain matching results to the check_heteroscedasticity function by explicitly using the fitted values of the model:\n\nlmtest::bptest(fish, studentize = F, varformula = ~fitted.values(fish))\n\n\n    Breusch-Pagan test\n\ndata:  fish\nBP = 9.1824, df = 1, p-value = 0.002444\n\n\nThe same result can also be obtained using the ncvTest (Non-constant Variance Score Test) function from the package car:\n\ncar::ncvTest(fish)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 9.182395, Df = 1, p = 0.0024435\n\n\nFor additional functionality related to checking for heteroscedasticity, see this vignette from the olsrr package.\n\n\nMake a plot of the residuals against the predictor variable stage. Do you find this plot useful? Why or why not?\n\n\nres_fit &lt;- res_fit %&gt;% mutate(stage = fws$stage)\n\nggplot(res_fit)+\n  geom_point(aes(x=stage, y=res))+\n  labs(x=\"Life history stage\", y=\"Residuals\")+\n  geom_hline(yintercept = 0, lty=2)+mytheme\n\n\n\n\n\nProduce a \\(Q\\)-\\(Q\\) plot of the model residuals and include a \\(Q\\)-\\(Q\\) line. Describe what you would hope to see here. Do you?\n\n\n# base R\nqqnorm(residuals(fish),ylab=\"Residuals\",main=\"\")\nqqline(residuals(fish))\n\n\n\n# ggplot\nggplot() + \n  stat_qq(aes(sample = residuals(fish))) + \n  stat_qq_line(aes(sample = residuals(fish))) +\n  mytheme+labs(x=\"Theoretical Quantiles\", y=\"Residuals\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Notes/Inference.html#examples",
    "href": "Notes/Inference.html#examples",
    "title": "Inference, Diagnostics, & Errors",
    "section": "Examples",
    "text": "Examples\n\ndata(\"gala\")\nlmod &lt;- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala)\nnullmod &lt;- lm(Species ~ 1, gala)\nanova(lmod, nullmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n24\n89231.37\nNA\nNA\nNA\nNA\n\n\n29\n381081.37\n-5\n-291850\n15.69941\n7e-07\n\n\n\n\n\n\nWe can compute this by hand:\n\nrss0 &lt;- deviance(nullmod)\nrss &lt;- deviance(lmod)\ndf0 &lt;- df.residual(nullmod)\ndf &lt;- df.residual(lmod)\nfstat &lt;- ((rss-rss0)/(df-df0))/(rss/df)\n\n1-pf(fstat, df0-df, df)\n\n[1] 6.837893e-07\n\n\n\nTesting 1 predictor\n\nlmods &lt;- lm(Species ~ Elevation + Nearest + Scruz + Adjacent, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n25\n93469.08\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n1\n4237.718\n1.139792\n0.296318\n\n\n\n\n\nsumary(lmod)\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)  7.068221  19.154198  0.3690 0.7153508\nArea        -0.023938   0.022422 -1.0676 0.2963180\nElevation    0.319465   0.053663  5.9532 3.823e-06\nNearest      0.009144   1.054136  0.0087 0.9931506\nScruz       -0.240524   0.215402 -1.1166 0.2752082\nAdjacent    -0.074805   0.017700 -4.2262 0.0002971\n\nn = 30, p = 6, Residual SE = 60.97519, R-Squared = 0.77\n\n\n\nsumary(lm(Species ~ Area, gala))\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept) 63.782861  17.524416  3.6397 0.0010943\nArea         0.081963   0.019713  4.1578 0.0002748\n\nn = 30, p = 2, Residual SE = 91.73159, R-Squared = 0.38\n\n\n\n\nTesting a pair of predictors\n\nlmods &lt;- lm(Species ~ Elevation + Nearest + Scruz, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n26\n158291.63\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n2\n69060.26\n9.287352\n0.0010297\n\n\n\n\n\n\n\n\nTesting a subspace\n\\[H_0:\\beta_\\text{Area}= \\beta_\\text{Adjacent}\\]\n\nlmods &lt;- lm(Species ~ I(Area+Adjacent) + Elevation + Nearest + Scruz, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n25\n109591.12\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n1\n20359.75\n5.476035\n0.0279256\n\n\n\n\n\n\n\\[H_0:\\beta_\\text{Elevation}= 0.5\\]\n\nlmods &lt;- lm(Species ~ Area+ offset(0.5*Elevation) + Nearest + Scruz + Adjacent, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n25\n131312.12\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n1\n42080.76\n11.3182\n0.0025738\n\n\n\n\n\n\nA simpler way to test such point hypotheses is to use a t-statistic: \\[t = (\\hat{\\beta}−c)/\\text{se}(\\hat{\\beta})\\] where \\(c\\) is the point hypothesis. In this example, the statistic and corresponding p-value are:\n\nb_hat &lt;- unname(lmod$coefficients[\"Elevation\"])\nse_b_hat &lt;- summary(lmod)$coef[\"Elevation\", \"Std. Error\"]\n(tstat &lt;- (b_hat-0.5)/se_b_hat)\n\n[1] -3.364253\n\n2*pt(tstat, 24)\n\n[1] 0.002573836\n\ntstat^2\n\n[1] 11.3182"
  },
  {
    "objectID": "Notes/Inference.html#permutation-tests",
    "href": "Notes/Inference.html#permutation-tests",
    "title": "Inference, Diagnostics, & Errors",
    "section": "Permutation Tests",
    "text": "Permutation Tests\n\nOverall model\n\nlmod &lt;- lm(Species ~ Nearest + Scruz, gala)\nlms &lt;- summary(lmod)\nlms$fstatistic\n\n     value      numdf      dendf \n 0.6019558  2.0000000 27.0000000 \n\npf(lms$fstatistic[1],lms$fstatistic[2],lms$fstatistic[3],lower.tail=F)\n\n    value \n0.5549255 \n\n\n\nset.seed(123)\n\nnreps &lt;- 4000\nfstats &lt;- numeric(nreps)\n\nfor(i in 1:nreps){\n     lmods &lt;- lm(sample(Species) ~ Nearest+Scruz , gala)\n     fstats[i] &lt;- summary(lmods)$fstat[1]\n}\n\nmean(fstats &gt; lms$fstat[1])\n\n[1] 0.55825\n\n\n\n\nSingle predictor\n\nsummary(lmod)$coef[3,]\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n-0.4406401  0.4025312 -1.0946731  0.2833295 \n\ntstats &lt;- numeric(nreps)\n\nset.seed(123)\n\nfor(i in 1:nreps){\n  lmods &lt;- lm(Species ~ Nearest+sample(Scruz), gala)\n  tstats[i] &lt;- summary(lmods)$coef[3,3]\n}\n\nmean(abs(tstats) &gt; abs(lms$coef[3,3]))\n\n[1] 0.26825"
  },
  {
    "objectID": "Notes/Inference.html#confidence-intervals",
    "href": "Notes/Inference.html#confidence-intervals",
    "title": "Inference, Diagnostics, & Errors",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nSingle parameter\n\\[\\hat{\\beta_i}\\pm t^{(\\alpha/2)}_{n-p}\\text{se}(\\hat{\\beta}) \\]\n\nlmod &lt;- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala)\nsumary(lmod)\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)  7.068221  19.154198  0.3690 0.7153508\nArea        -0.023938   0.022422 -1.0676 0.2963180\nElevation    0.319465   0.053663  5.9532 3.823e-06\nNearest      0.009144   1.054136  0.0087 0.9931506\nScruz       -0.240524   0.215402 -1.1166 0.2752082\nAdjacent    -0.074805   0.017700 -4.2262 0.0002971\n\nn = 30, p = 6, Residual SE = 60.97519, R-Squared = 0.77\n\n\nWe can construct individual 95% CIs for \\(\\beta_\\text{Area}\\) for which we need the 2.5% and 97.5% percentiles of the t-distribution with 30−6 = 24 degrees of freedom.\n\n(t_stat&lt;-qt(0.975, 30-6))\n\n[1] 2.063899\n\nb_hat &lt;- summary(lmod)$coefficients[\"Area\", \"Estimate\"]\nse_b_hat &lt;- summary(lmod)$coefficients[\"Area\", \"Std. Error\"]\n\nb_hat + c(-1, 1)*t_stat*se_b_hat\n\n[1] -0.07021580  0.02233912\n\nconfint(lmod, \"Area\")\n\n          2.5 %     97.5 %\nArea -0.0702158 0.02233912\n\n\n\n\nMultiple parameters\nIf you are interested in more than one parameter, you can construct a \\(100(1−\\alpha)\\)% confidence region for \\(\\beta\\) using: \\[\\left(\\hat{\\beta}-\\beta \\right)^TX^TX\\left(\\hat{\\beta}-\\beta \\right) \\leq p \\hat{\\sigma}^2 F^{(\\alpha)}_{p, n-p} \\] These regions are ellipsoidally shaped. Because these ellipsoids lie in higher dimensions, they cannot easily be visualized except for the two-dimensional case. Let’s see how these compare to the univariate confidence intervals. For example, we can construct the joint 95% confidence region for \\(\\beta_\\text{Area}\\) and \\(\\beta_\\text{Adjacent}\\). We have added the point of the least squares estimates which lies at the center of the ellipse and the univariate confidence intervals for both dimensions as dotted lines:\nUsing base R:\n\nplot(ellipse(lmod,c(2,6)),type=\"l\",ylim=c(-0.13,0), xlim = c(-0.09, 0.04))\npoints(coef(lmod)[2], coef(lmod)[6], pch=19)\nabline(v=confint(lmod)[2,],lty=2)\nabline(h=confint(lmod)[6,],lty=2)\n\n\n\n\nA ggplot version:\n\n# ggplot version\n\ndata.frame(ellipse(lmod,c(2,6))) %&gt;% \n  ggplot(aes(x=Area, y=Adjacent))+\n # geom_point()+\n  geom_density_2d(stat=\"identity\", color=\"black\")+\n  lims(y=c(-0.13,0), x=c(-0.09, 0.04))+\n  geom_vline(xintercept = confint(lmod)[2,],lty=2)+\n  geom_hline(yintercept = confint(lmod)[6,],lty=2)+\n  annotate(geom=\"point\", x=coef(lmod)[2], y=coef(lmod)[6])+\n  mytheme"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html",
    "href": "Notes/Inference_Diagnostics.html",
    "title": "Inference, Diagnostics, & Errors",
    "section": "",
    "text": "# List of packages required:\npackages &lt;- c(\"tidyverse\", \"PNWColors\", \"janitor\", \"faraway\", \"broom\", \"DHARMa\", \"emmeans\", \"performance\", \"ellipse\")\n\n# Load packages into session\nlapply(packages, require, character.only = TRUE)\nrm(packages)\n\n# Ensure functions with duplicate names are from the correct package\nselect &lt;- dplyr::select\nmap &lt;- purrr::map\nsummarize &lt;- dplyr::summarize\nclean_names &lt;- janitor::clean_names\nmargin &lt;- ggplot2::margin\n\nset.seed(123) #Set seed for pseudo-random number generator, for reproducibility\n\nmytheme &lt;- theme_light()+ #define custom theme for ggplots\n  theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, l = 0)),\n        axis.title.x = element_text(margin = margin(t = 10, l = 0)),\n        text=element_text(size=15))\n\ndata(\"gala\")"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#examples",
    "href": "Notes/Inference_Diagnostics.html#examples",
    "title": "Inference, Diagnostics, & Errors",
    "section": "Examples",
    "text": "Examples\n\ndata(\"gala\")\nlmod &lt;- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala)\nnullmod &lt;- lm(Species ~ 1, gala)\nanova(lmod, nullmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n24\n89231.37\nNA\nNA\nNA\nNA\n\n\n29\n381081.37\n-5\n-291850\n15.69941\n7e-07\n\n\n\n\n\n\nWe can compute this by hand:\n\nrss0 &lt;- deviance(nullmod)\nrss &lt;- deviance(lmod)\ndf0 &lt;- df.residual(nullmod)\ndf &lt;- df.residual(lmod)\nfstat &lt;- ((rss-rss0)/(df-df0))/(rss/df)\n\n1-pf(fstat, df0-df, df)\n\n[1] 6.837893e-07\n\n\n\nTesting 1 predictor\n\nlmods &lt;- lm(Species ~ Elevation + Nearest + Scruz + Adjacent, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n25\n93469.08\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n1\n4237.718\n1.139792\n0.296318\n\n\n\n\n\nsumary(lmod)\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)  7.068221  19.154198  0.3690 0.7153508\nArea        -0.023938   0.022422 -1.0676 0.2963180\nElevation    0.319465   0.053663  5.9532 3.823e-06\nNearest      0.009144   1.054136  0.0087 0.9931506\nScruz       -0.240524   0.215402 -1.1166 0.2752082\nAdjacent    -0.074805   0.017700 -4.2262 0.0002971\n\nn = 30, p = 6, Residual SE = 60.97519, R-Squared = 0.77\n\n\n\nsumary(lm(Species ~ Area, gala))\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept) 63.782861  17.524416  3.6397 0.0010943\nArea         0.081963   0.019713  4.1578 0.0002748\n\nn = 30, p = 2, Residual SE = 91.73159, R-Squared = 0.38\n\n\n\n\nTesting a pair of predictors\n\nlmods &lt;- lm(Species ~ Elevation + Nearest + Scruz, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n26\n158291.63\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n2\n69060.26\n9.287352\n0.0010297\n\n\n\n\n\n\n\n\nTesting a subspace\n\\[H_0:\\beta_\\text{Area}= \\beta_\\text{Adjacent}\\]\n\nlmods &lt;- lm(Species ~ I(Area+Adjacent) + Elevation + Nearest + Scruz, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n25\n109591.12\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n1\n20359.75\n5.476035\n0.0279256\n\n\n\n\n\n\n\\[H_0:\\beta_\\text{Elevation}= 0.5\\]\n\nlmods &lt;- lm(Species ~ Area+ offset(0.5*Elevation) + Nearest + Scruz + Adjacent, gala)\nanova(lmods, lmod)\n\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n25\n131312.12\nNA\nNA\nNA\nNA\n\n\n24\n89231.37\n1\n42080.76\n11.3182\n0.0025738\n\n\n\n\n\n\nA simpler way to test such point hypotheses is to use a t-statistic: \\[t = (\\hat{\\beta}−c)/\\text{se}(\\hat{\\beta})\\] where \\(c\\) is the point hypothesis. In this example, the statistic and corresponding p-value are:\n\nb_hat &lt;- unname(lmod$coefficients[\"Elevation\"])\nse_b_hat &lt;- summary(lmod)$coef[\"Elevation\", \"Std. Error\"]\n(tstat &lt;- (b_hat-0.5)/se_b_hat)\n\n[1] -3.364253\n\n2*pt(tstat, 24)\n\n[1] 0.002573836\n\ntstat^2\n\n[1] 11.3182"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#permutation-tests",
    "href": "Notes/Inference_Diagnostics.html#permutation-tests",
    "title": "Inference, Diagnostics, & Errors",
    "section": "Permutation Tests",
    "text": "Permutation Tests\n\nOverall model\n\nlmod &lt;- lm(Species ~ Nearest + Scruz, gala)\nlms &lt;- summary(lmod)\nlms$fstatistic\n\n     value      numdf      dendf \n 0.6019558  2.0000000 27.0000000 \n\npf(lms$fstatistic[1],lms$fstatistic[2],lms$fstatistic[3],lower.tail=F)\n\n    value \n0.5549255 \n\n\n\nset.seed(123)\n\nnreps &lt;- 4000\nfstats &lt;- numeric(nreps)\n\nfor(i in 1:nreps){\n     lmods &lt;- lm(sample(Species) ~ Nearest+Scruz , gala)\n     fstats[i] &lt;- summary(lmods)$fstat[1]\n}\n\nmean(fstats &gt; lms$fstat[1])\n\n[1] 0.55825\n\n\n\n\nSingle predictor\n\nsummary(lmod)$coef[3,]\n\n  Estimate Std. Error    t value   Pr(&gt;|t|) \n-0.4406401  0.4025312 -1.0946731  0.2833295 \n\ntstats &lt;- numeric(nreps)\n\nset.seed(123)\n\nfor(i in 1:nreps){\n  lmods &lt;- lm(Species ~ Nearest+sample(Scruz), gala)\n  tstats[i] &lt;- summary(lmods)$coef[3,3]\n}\n\nmean(abs(tstats) &gt; abs(lms$coef[3,3]))\n\n[1] 0.26825"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#confidence-intervals",
    "href": "Notes/Inference_Diagnostics.html#confidence-intervals",
    "title": "Inference, Diagnostics, & Errors",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nSingle parameter\n\\[\\hat{\\beta_i}\\pm t^{(\\alpha/2)}_{n-p}\\text{se}(\\hat{\\beta}) \\]\n\nlmod &lt;- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, gala)\nsumary(lmod)\n\n             Estimate Std. Error t value  Pr(&gt;|t|)\n(Intercept)  7.068221  19.154198  0.3690 0.7153508\nArea        -0.023938   0.022422 -1.0676 0.2963180\nElevation    0.319465   0.053663  5.9532 3.823e-06\nNearest      0.009144   1.054136  0.0087 0.9931506\nScruz       -0.240524   0.215402 -1.1166 0.2752082\nAdjacent    -0.074805   0.017700 -4.2262 0.0002971\n\nn = 30, p = 6, Residual SE = 60.97519, R-Squared = 0.77\n\n\nWe can construct individual 95% CIs for \\(\\beta_\\text{Area}\\) for which we need the 2.5% and 97.5% percentiles of the t-distribution with 30−6 = 24 degrees of freedom.\n\n(t_stat&lt;-qt(0.975, 30-6))\n\n[1] 2.063899\n\nb_hat &lt;- summary(lmod)$coefficients[\"Area\", \"Estimate\"]\nse_b_hat &lt;- summary(lmod)$coefficients[\"Area\", \"Std. Error\"]\n\nb_hat + c(-1, 1)*t_stat*se_b_hat\n\n[1] -0.07021580  0.02233912\n\nconfint(lmod, \"Area\")\n\n          2.5 %     97.5 %\nArea -0.0702158 0.02233912\n\n\n\n\nMultiple parameters\nIf you are interested in more than one parameter, you can construct a \\(100(1−\\alpha)\\)% confidence region for \\(\\beta\\) using: \\[\\left(\\hat{\\beta}-\\beta \\right)^TX^TX\\left(\\hat{\\beta}-\\beta \\right) \\leq p \\hat{\\sigma}^2 F^{(\\alpha)}_{p, n-p} \\] These regions are ellipsoidally shaped. Because these ellipsoids lie in higher dimensions, they cannot easily be visualized except for the two-dimensional case. Let’s see how these compare to the univariate confidence intervals. For example, we can construct the joint 95% confidence region for \\(\\beta_\\text{Area}\\) and \\(\\beta_\\text{Adjacent}\\). We have added the point of the least squares estimates which lies at the center of the ellipse and the univariate confidence intervals for both dimensions as dotted lines:\nUsing base R:\n\nplot(ellipse(lmod,c(2,6)),type=\"l\",ylim=c(-0.13,0), xlim = c(-0.09, 0.04))\npoints(coef(lmod)[2], coef(lmod)[6], pch=19)\nabline(v=confint(lmod)[2,],lty=2)\nabline(h=confint(lmod)[6,],lty=2)\n\n\n\n\nA ggplot version:\n\n# ggplot version\n\ndata.frame(ellipse(lmod,c(2,6))) %&gt;% \n  ggplot(aes(x=Area, y=Adjacent))+\n # geom_point()+\n  geom_density_2d(stat=\"identity\", color=\"black\")+\n  lims(y=c(-0.13,0), x=c(-0.09, 0.04))+\n  geom_vline(xintercept = confint(lmod)[2,],lty=2)+\n  geom_hline(yintercept = confint(lmod)[6,],lty=2)+\n  annotate(geom=\"point\", x=coef(lmod)[2], y=coef(lmod)[6])+\n  mytheme+\n  theme(panel.grid = element_blank())\n\n\n\n\nWe can determine the outcome of various hypotheses from the plot. The joint hypothesis \\(H_0 : \\beta_\\text{Area} = \\beta_\\text{Adjacent} = 0\\) is rejected because the origin does not lie inside the ellipse. The hypothesis \\(H_0 : \\beta_\\text{Area} = 0\\) is not rejected because zero does lie within the vertical dashed lines, whereas the horizontal dashed lines do not encompass zero and so \\(H_0: \\beta_\\text{Adjacent} = 0\\) is rejected."
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#bootstrap-confidence-intervals",
    "href": "Notes/Inference_Diagnostics.html#bootstrap-confidence-intervals",
    "title": "Inference, Diagnostics, & Errors",
    "section": "Bootstrap Confidence Intervals",
    "text": "Bootstrap Confidence Intervals\n\nset.seed(123)\nnb &lt;- 4000\ncoefmat &lt;- matrix(NA,nb,6)\nresids &lt;- residuals(lmod)\npreds &lt;- fitted(lmod)\n \nfor(i in 1:nb){\nboot &lt;- preds + sample(resids , rep=TRUE)\nbmod &lt;- update(lmod , boot ~ .)\ncoefmat[i,] &lt;- coef(bmod)\n}\n\ncolnames(coefmat) &lt;- c(\"Intercept\",colnames(gala[,3:7]))\ncoefmat &lt;- data.frame(coefmat)\napply(coefmat,2,function(x) quantile(x,c(0.025,0.975)))\n\n      Intercept        Area Elevation   Nearest      Scruz    Adjacent\n2.5%  -25.31406 -0.06236506 0.2310989 -1.716588 -0.6061978 -0.10545278\n97.5%  42.69309  0.01807403 0.4207570  2.122722  0.1677720 -0.03979658"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#cis-for-predictions",
    "href": "Notes/Inference_Diagnostics.html#cis-for-predictions",
    "title": "Inference, Diagnostics, & Errors",
    "section": "CIs for predictions",
    "text": "CIs for predictions\nThere are two kinds of predictions made from regression models. One is a predicted mean response and the other is a prediction of a future observation. For example, suppose we have built a regression model that predicts the rental price of houses in a given area based on predictors such as the number of bedrooms and closeness to a major highway. There are two kinds of predictions that can be made for a given \\(x_0\\):\n\nSuppose a specific house comes on the market with characteristics \\(x_0\\). Its rental price will be \\(x_0^T \\beta +\\varepsilon\\). Since \\(E(\\varepsilon)=0\\), the predicted price is \\(x_0^T \\hat{\\beta}\\), but in assessing the variance of this prediction, we must include the variance of \\(\\varepsilon\\).\nSuppose we ask the question — “What would a house with characteristics \\(x_0\\) rent for on average?” This selling price is \\(x_0^T \\beta\\) and is again predicted by \\(x_0^T \\hat{\\beta}\\), but now only the variance in \\(\\hat{\\beta}\\) needs to be taken into account.\n\nMost times, we will want the first case, which is called “prediction of a future value,” while the second case, called “prediction of the mean response” is less commonly required. We have:\n\\[\\text{var}(x_0^T \\hat{\\beta})= x_0^T (X^TX)^{-1}x_0\\sigma^2\\]\nWe assume that the future \\(\\varepsilon\\) is independent of \\(\\hat{\\beta}\\). So a \\(100(1-\\alpha)\\)% CI for a single future response is\n\\[\\hat{y_0}\\pm t^{(\\alpha/2)}_{n-p}\\hat{\\sigma}\\sqrt{1+x_0^T(X^TX)^{-1}x_0}\\]\nThere is a conceptual difference here because previous confidence intervals have been for parameters. Parameters are considered to be fixed but unknown — they are not random under the Frequentist approach we are using here. However, a future observation is a random variable. For this reason, it is better to call this a prediction interval. We are saying there is a 95% chance that the future value falls within this interval whereas it would be incorrect to say that for a parameter."
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#example",
    "href": "Notes/Inference_Diagnostics.html#example",
    "title": "Inference, Diagnostics, & Errors",
    "section": "Example",
    "text": "Example\n\ndata(fat,package=\"faraway\")\nlmod &lt;- lm(brozek ~ age + weight + height + neck + chest + abdom + hip + thigh + knee + ankle + biceps + forearm + wrist, data=fat)\n\n x &lt;- model.matrix(lmod)\n(x0 &lt;- apply(x,2,median))\n\n(Intercept)         age      weight      height        neck       chest \n       1.00       43.00      176.50       70.00       38.00       99.65 \n      abdom         hip       thigh        knee       ankle      biceps \n      90.95       99.30       59.00       38.50       22.80       32.05 \n    forearm       wrist \n      28.70       18.30 \n\n(y0 &lt;- sum(x0*coef(lmod)))\n\n[1] 17.49322\n\npredict(lmod,new=data.frame(t(x0)))\n\n       1 \n17.49322 \n\npredict(lmod,new=data.frame(t(x0)),interval=\"prediction\")\n\n       fit     lwr      upr\n1 17.49322 9.61783 25.36861\n\npredict(lmod,new=data.frame(t(x0)),interval=\"confidence\")\n\n       fit      lwr      upr\n1 17.49322 16.94426 18.04219\n\n\nSkipping autoregression section b/c would use explicit ARIMA model or other method of time-series analysis where appropriate"
  },
  {
    "objectID": "Notes/Inference_Diagnostics.html#checking-error-assumptions",
    "href": "Notes/Inference_Diagnostics.html#checking-error-assumptions",
    "title": "Inference, Diagnostics, & Errors",
    "section": "Checking Error Assumptions",
    "text": "Checking Error Assumptions\nWe wish to check the independence, constant variance and normality of the errors, \\(\\varepsilon\\). The errors are not observable, but we can examine the residuals, \\(\\hat{\\varepsilon}\\). These are not interchangeable with the error, as they have somewhat different properties. Recall that \\(\\hat{y}=X(X^TX)^{-1}Xy=Hy\\), where \\(H\\) is the hat matrix, so that \\[\\hat{\\varepsilon}=y-\\hat{y}=(I-H)y=(I-H)X\\beta+(I-H)\\varepsilon=(I-H)\\varepsilon \\]\nTherefore, \\(\\text{var}\\hat{\\varepsilon} = \\text{var} (I-H)\\varepsilon = (I-H)\\sigma^2\\) assuming that \\(\\text{var}\\varepsilon= \\sigma^2I\\). We see that although the errors may have equal variance and be uncorrelated, the residuals do not."
  }
]